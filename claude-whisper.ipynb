{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 108875,
     "status": "ok",
     "timestamp": 1753160353411,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "FZBDvCkpryD5",
    "outputId": "f22ad22a-59dc-45b9-d3d3-8b1e51645660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-12wfptm7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-12wfptm7\n",
      "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=ae87e7d26afe84481c53768255c8cc85794711e5b17c0c64a3dda3b563887b6e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cj3fe8s4/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-packages git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24942,
     "status": "ok",
     "timestamp": 1753160378370,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "QKWUqxj0r1UO",
    "outputId": "d7bd5011-da8c-446a-8728-db71c0681b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.audio\n",
      "  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.33.4)\n",
      "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
      "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n",
      "Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
      "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
      "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
      "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n",
      "Collecting semver>=3.0.0 (from pyannote.audio)\n",
      "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n",
      "Collecting speechbrain>=1.0.0 (from pyannote.audio)\n",
      "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tensorboardX>=2.6 (from pyannote.audio)\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
      "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
      "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
      "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
      "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.15.3)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
      "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n",
      "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n",
      "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
      "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n",
      "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
      "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
      "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n",
      "Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
      "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt, julius\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8f1c968cdf81b4212c4f5b1e58f35485868a8b8303db8fbaa9b79e925b6f3693\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=a0a3cdbe0b082cb977727c605b0a57c08b0a53dc0918a1d50af27d2e06877128\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n",
      "Successfully built docopt julius\n",
      "Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n",
      "Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.14.3 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-packages pyannote.audio torchaudio # pydub not strictly needed if only ffmpeg is used for audio proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HEgpdmCrri8"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj1jJgHxr3FR"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_AUDIO_PATH = \"/content/001_t2.wav\"\n",
    "CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n",
    "HUGGING_FACE_ACCESS_TOKEN = \"hf__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116945,
     "status": "ok",
     "timestamp": 1753160525295,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "Sikg3pI54rdI",
    "outputId": "57695d46-7f4f-49cb-937f-df9d143ce875"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [01:17<00:00, 40.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"large\")  # or \"large-v3\" if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1753164394800,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "DvOxTwr-NeXZ",
    "outputId": "0645103e-8872-410e-a5fb-789242f86caf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753168637252,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "CRz3sdMGr80J"
   },
   "outputs": [],
   "source": [
    "def get_audio_duration(audio_path):\n",
    "    \"\"\"Get audio duration using ffprobe\"\"\"\n",
    "    try:\n",
    "        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
    "               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        return float(result.stdout.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get duration: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753168638646,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "xzMTzixIsAmG"
   },
   "outputs": [],
   "source": [
    "def audio_preprocessing_v1(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Version 1: Advanced filters (your original working approach)\n",
    "    \"\"\"\n",
    "    print(\"--- Trying Advanced Audio Preprocessing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-af\", \"loudnorm=I=-16:TP=-1.5:LRA=11, highpass=f=200, lowpass=f=3000\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Advanced preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Advanced preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v2(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Version 2: Simplified but effective\n",
    "    \"\"\"\n",
    "    print(\"--- Trying Simplified Audio Preprocessing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-af\", \"loudnorm,highpass=f=100,lowpass=f=7000\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Simplified preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Simplified preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v3(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Version 3: Basic but reliable\n",
    "    \"\"\"\n",
    "    print(\"--- Trying Basic Audio Preprocessing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-af\", \"loudnorm\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Basic preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Basic preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v4(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Version 4: Minimal - just format conversion\n",
    "    \"\"\"\n",
    "    print(\"--- Trying Minimal Audio Processing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Minimal processing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Minimal processing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def smart_audio_preprocessing(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Try different preprocessing methods in order of preference\n",
    "    \"\"\"\n",
    "    original_duration = get_audio_duration(input_path)\n",
    "    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n",
    "\n",
    "    # Try methods in order of sophistication\n",
    "    methods = [\n",
    "        audio_preprocessing_v1,  # Your original working method\n",
    "        audio_preprocessing_v2,  # Simplified\n",
    "        audio_preprocessing_v3,  # Basic\n",
    "        audio_preprocessing_v4   # Minimal\n",
    "    ]\n",
    "\n",
    "    for i, method in enumerate(methods, 1):\n",
    "        if method(input_path, output_path):\n",
    "            # Verify the output\n",
    "            if os.path.exists(output_path):\n",
    "                processed_duration = get_audio_duration(output_path)\n",
    "                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n",
    "\n",
    "                if abs(original_duration - processed_duration) < 1.0:  # Allow 1 second difference\n",
    "                    print(f\"✅ Audio preprocessing successful with method {i}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n",
    "                    continue\n",
    "\n",
    "    print(\"❌ All preprocessing methods failed!\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753168632030,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "jPgB-uU34udg",
    "outputId": "e6bd7a35-8e5b-4110-cb8b-d495907361c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1753168642194,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "6IDZ-zTJ7ALa"
   },
   "outputs": [],
   "source": [
    "def enhanced_whisper_transcription(audio_path):\n",
    "    \"\"\"\n",
    "    Enhanced Whisper transcription with better parameters\n",
    "    \"\"\"\n",
    "    print(\"--- Enhanced Whisper Transcription ---\")\n",
    "\n",
    "    # Load model\n",
    "    # model\n",
    "    # Enhanced initial prompt for insurance domain\n",
    "    initial_prompt = (\n",
    "        \"Axis Maxlife Insurance, Policy number, fund value, risk term, Premium Due, Due date, \"\n",
    "        \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking, \"\n",
    "        \"risk coverage, policy benefits, health declaration form, renewal, premium payment, \"\n",
    "        \"claim settlement, maturity benefit, surrender value, loan against policy, \"\n",
    "        \"nominee details, KYC documents, medical examination, insurance advisor, customer service\"\n",
    "    )\n",
    "\n",
    "    # Transcribe with enhanced parameters\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        language=\"ta\",           # Tamil\n",
    "        task=\"translate\",        # Translate to English\n",
    "        verbose=True,\n",
    "\n",
    "        # Enhanced parameters for better accuracy\n",
    "        # temperature=(0.0, 0.2, 0.4),  # Try multiple temperatures\n",
    "        temperature=0.0,\n",
    "        # beam_size=5,                  # Use beam search\n",
    "        # best_of=5,                    # Generate multiple candidates\n",
    "\n",
    "        initial_prompt=initial_prompt,\n",
    "        word_timestamps=True,         # Get word-level timestamps\n",
    "\n",
    "        # Thresholds for better quality\n",
    "        compression_ratio_threshold=2.4,\n",
    "        logprob_threshold=-1.0,\n",
    "        no_speech_threshold=0.6,\n",
    "        condition_on_previous_text=True,\n",
    "        # condition_on_previous_text=False,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753168644014,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "XXAWPm-zsGxK"
   },
   "outputs": [],
   "source": [
    "def post_process_text(text):\n",
    "    \"\"\"\n",
    "    Clean up transcribed text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Common corrections for Indian insurance context\n",
    "    corrections = {\n",
    "        'access max life': 'Axis Max Life',\n",
    "        'axis max life': 'Axis Max Life',\n",
    "        'g pay': 'GPay',\n",
    "        'google pay': 'Google Pay',\n",
    "        'phone pay': 'PhonePe',\n",
    "        'phone pe': 'PhonePe',\n",
    "        'pay tm': 'Paytm',\n",
    "        'net banking': 'netbanking',\n",
    "        'some assured': 'sum assured',\n",
    "        'premium do': 'premium due',\n",
    "        'do date': 'due date',\n",
    "    }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    for wrong, correct in corrections.items():\n",
    "        text_lower = text_lower.replace(wrong, correct)\n",
    "\n",
    "    # Capitalize first letter of sentences\n",
    "    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n",
    "                       lambda m: m.group(1) + m.group(2).upper(), text_lower)\n",
    "\n",
    "    return text_lower.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1753168760221,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "-S1-LMuVutg_"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main processing pipeline\n",
    "    \"\"\"\n",
    "    print(\"🎯 Starting Enhanced Audio Processing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Smart Audio Preprocessing\n",
    "    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n",
    "        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Enhanced Whisper Transcription\n",
    "    try:\n",
    "        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n",
    "        print(\"✅ Whisper transcription completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Whisper transcription failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Post-process transcription\n",
    "    processed_segments = []\n",
    "    for segment in whisper_result[\"segments\"]:\n",
    "        processed_text = post_process_text(segment['text'])\n",
    "        if processed_text.strip():  # Only keep non-empty segments\n",
    "            segment_copy = segment.copy()\n",
    "            segment_copy['text'] = processed_text\n",
    "            processed_segments.append(segment_copy)\n",
    "\n",
    "    whisper_result[\"segments\"] = processed_segments\n",
    "\n",
    "    # Step 4: Speaker Diarization\n",
    "    print(\"\\n--- Speaker Diarization ---\")\n",
    "    try:\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"✅ Using GPU for diarization\")\n",
    "\n",
    "        diarization = pipeline(CLEAN_AUDIO_PATH)\n",
    "        print(\"✅ Speaker diarization completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Speaker diarization failed: {e}\")\n",
    "        diarization = None\n",
    "\n",
    "    # Step 5: Generate Enhanced Dialogue\n",
    "    print(\"\\n--- Generating Dialogue ---\")\n",
    "\n",
    "    def get_dominant_speaker(start_time, end_time, diarization_result):\n",
    "        if not diarization_result:\n",
    "            return \"Speaker_Unknown\"\n",
    "\n",
    "        speakers = {}\n",
    "        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "            overlap_start = max(start_time, segment.start)\n",
    "            overlap_end = min(end_time, segment.end)\n",
    "            overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap_duration > 0:\n",
    "                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n",
    "\n",
    "        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n",
    "\n",
    "    # Combine segments by speaker\n",
    "    dialogue = []\n",
    "    current_speaker = None\n",
    "    current_texts = []\n",
    "    current_start = 0\n",
    "    current_end = 0\n",
    "\n",
    "    for segment in processed_segments:\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        text = segment['text'].strip()\n",
    "\n",
    "        speaker = get_dominant_speaker(start, end, diarization)\n",
    "\n",
    "        # Merge consecutive segments from same speaker (within 3 seconds)\n",
    "        if (speaker == current_speaker and\n",
    "            current_speaker and\n",
    "            (start - current_end) < 3.0):\n",
    "            current_texts.append(text)\n",
    "            current_end = end\n",
    "        else:\n",
    "            # Save previous speaker's dialogue\n",
    "            if current_speaker and current_texts:\n",
    "                dialogue.append({\n",
    "                    'speaker': current_speaker,\n",
    "                    'text': ' '.join(current_texts),\n",
    "                    'start_time': current_start,\n",
    "                    'end_time': current_end\n",
    "                })\n",
    "\n",
    "            # Start new speaker segment\n",
    "            current_speaker = speaker\n",
    "            current_texts = [text]\n",
    "            current_start = start\n",
    "            current_end = end\n",
    "\n",
    "    # Add final segment\n",
    "    if current_speaker and current_texts:\n",
    "        dialogue.append({\n",
    "            'speaker': current_speaker,\n",
    "            'text': ' '.join(current_texts),\n",
    "            'start_time': current_start,\n",
    "            'end_time': current_end\n",
    "        })\n",
    "\n",
    "    # Step 6: Display Results\n",
    "    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n",
    "\n",
    "    for entry in dialogue:\n",
    "        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n",
    "        print(f\"\\n{entry['speaker']} {timestamp}:\")\n",
    "        print(f\"  📝 {entry['text']}\")\n",
    "\n",
    "    # Step 7: Save Results\n",
    "    output_data = {\n",
    "        'metadata': {\n",
    "            'total_duration': whisper_result.get('duration', 0),\n",
    "            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n",
    "            'total_segments': len(dialogue),\n",
    "            'model_used': 'whisper-large',s\n",
    "            'processing_successful': True\n",
    "        },\n",
    "        'dialogue': dialogue,\n",
    "        'raw_transcription': whisper_result\n",
    "    }\n",
    "\n",
    "    with open('enhanced_transcription_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n",
    "    print(\"✅ Processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD63_Q95eNPm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDbziSsHTYPH27Lkm9meV3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
