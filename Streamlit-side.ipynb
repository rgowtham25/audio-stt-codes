{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP425XZvbR6cmJcQe0ELOeK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Ub5bqDOhGJyk","executionInfo":{"status":"ok","timestamp":1757046340697,"user_tz":-330,"elapsed":20032,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"4ab84d81-7061-41a5-9275-e8ab71763a57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-m7sxpqs6\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-m7sxpqs6\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.11.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=d4d0737090ffea034bb76c52c87dafa03268c99c754e101840eb59e8a6ea72b3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-j6rg405s/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n","Successfully built openai-whisper\n","Installing collected packages: openai-whisper\n","Successfully installed openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","source":["!pip install --break-system-packages pyannote.audio torchaudio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"7PhD0BJRGXOB","executionInfo":{"status":"ok","timestamp":1757046364248,"user_tz":-330,"elapsed":23579,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"5a926953-c5b3-4c10-cb10-9bbb2f4a57ca"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.34.4)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.4-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (2.8.0+cu126)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->pyannote.audio) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.9)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.4-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.1)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.12/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.17.3)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote.audio) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.1)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.12.15)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.16.5)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.43)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.8.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.10)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.4)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.4-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.2/825.2 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.4-py3-none-any.whl (829 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6e2121c914fae26c044d90708a6c2ee518f3ed39e0bfd489883f323e0ad54f48\n","  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=95d6a86bb33a948b2d1c7c739282c743e85c0fe2eae4635295e8e45393c8c5bc\n","  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.4 lightning-utilities-0.15.2 optuna-4.5.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.4 pytorch-metric-learning-2.9.0 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cQDGpUZSLcqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fw5Lt4QdTcPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # app_gradio.py\n","# # ---------------------------------------------------------\n","# # Whisper + (optional) Pyannote UI with \"Dialogue\" and \"Report Analysis\"\n","# # ---------------------------------------------------------\n","# # Run:\n","# #   python app_gradio.py\n","# # In Colab:\n","# #   from app_gradio import demo; demo.launch(share=True)\n","\n","# import os\n","# import re\n","# import json\n","# import subprocess\n","# from datetime import datetime\n","# from functools import lru_cache\n","# from typing import List, Dict, Any\n","\n","# import gradio as gr\n","# import torch\n","# import pandas as pd\n","# from openpyxl import Workbook\n","# from openpyxl.styles import Font, Alignment, Border, Side\n","# from openpyxl.utils import get_column_letter\n","# from html import escape\n","\n","# # ML libs\n","# import whisper\n","# from pyannote.audio import Pipeline\n","\n","# # -----------------------\n","# # Config\n","# # -----------------------\n","# DEFAULT_MODEL_NAME = os.getenv(\"WHISPER_MODEL\", \"large-v3\")\n","# SAMPLE_RATE_HZ = 16000\n","\n","# # Default HF token (falls back to your hardcoded value if env var is empty)\n","# DEFAULT_HF_TOKEN = os.getenv(\n","#     \"HUGGING_FACE_TOKEN\",\n","#     \"hf_\"\n","# )\n","\n","# # Known agents list (added from recent updates)\n","# KNOWN_AGENTS = [\n","#     \"Jaya Parkash\", \"Chandru\", \"Sneha\", \"Kowsalya\", \"Swathi\",\n","#     \"Arogya Marry\", \"Delphina\", \"Aesu Marry\"\n","# ]\n","\n","# # -----------------------\n","# # FFmpeg / audio utils\n","# # -----------------------\n","# def get_audio_duration(audio_path: str) -> float:\n","#     try:\n","#         cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","#                \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","#         out = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","#         return float(out.stdout.strip())\n","#     except Exception:\n","#         return 0.0\n","\n","# def _ffmpeg_run(cmd: List[str]) -> bool:\n","#     try:\n","#         subprocess.run(cmd, check=True, capture_output=True, text=True)\n","#         return True\n","#     except subprocess.CalledProcessError:\n","#         return False\n","\n","# def audio_preprocessing_v1(input_path, output_path):\n","#     cmd = [\n","#         \"ffmpeg\", \"-i\", input_path,\n","#         \"-acodec\", \"pcm_s16le\", \"-ac\", \"1\", \"-ar\", str(SAMPLE_RATE_HZ),\n","#         \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n","#         \"-y\", output_path\n","#     ]\n","#     return _ffmpeg_run(cmd)\n","\n","# def audio_preprocessing_v2(input_path, output_path):\n","#     cmd = [\n","#         \"ffmpeg\", \"-i\", input_path,\n","#         \"-acodec\", \"pcm_s16le\", \"-ac\", \"1\", \"-ar\", str(SAMPLE_RATE_HZ),\n","#         \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n","#         \"-y\", output_path\n","#     ]\n","#     return _ffmpeg_run(cmd)\n","\n","# def audio_preprocessing_v3(input_path, output_path):\n","#     cmd = [\n","#         \"ffmpeg\", \"-i\", input_path,\n","#         \"-acodec\", \"pcm_s16le\", \"-ac\", \"1\", \"-ar\", str(SAMPLE_RATE_HZ),\n","#         \"-af\", \"loudnorm\",\n","#         \"-y\", output_path\n","#     ]\n","#     return _ffmpeg_run(cmd)\n","\n","# def audio_preprocessing_v4(input_path, output_path):\n","#     cmd = [\n","#         \"ffmpeg\", \"-i\", input_path,\n","#         \"-acodec\", \"pcm_s16le\", \"-ac\", \"1\", \"-ar\", str(SAMPLE_RATE_HZ),\n","#         \"-y\", output_path\n","#     ]\n","#     return _ffmpeg_run(cmd)\n","\n","# def smart_audio_preprocessing(input_path, output_path, log: List[str]) -> bool:\n","#     original_dur = get_audio_duration(input_path)\n","#     log.append(f\"Original audio duration: {original_dur:.2f}s\")\n","\n","#     methods = [\n","#         (\"Advanced (afftdn)\", audio_preprocessing_v1),\n","#         (\"Simplified\", audio_preprocessing_v2),\n","#         (\"Basic\", audio_preprocessing_v3),\n","#         (\"Minimal\", audio_preprocessing_v4),\n","#     ]\n","#     for idx, (name, fn) in enumerate(methods, 1):\n","#         log.append(f\"→ Trying {name} preprocessing...\")\n","#         if fn(input_path, output_path) and os.path.exists(output_path):\n","#             processed_dur = get_audio_duration(output_path)\n","#             log.append(f\"Processed duration: {processed_dur:.2f}s\")\n","#             if abs(original_dur - processed_dur) < 1.0:\n","#                 log.append(f\"✅ Preprocessing successful with method {idx}: {name}\")\n","#                 return True\n","#             else:\n","#                 log.append(f\"⚠️ Duration mismatch on method {idx}; trying next...\")\n","#     log.append(\"❌ All preprocessing methods failed.\")\n","#     return False\n","\n","# # -----------------------\n","# # Text cleanup / repetition control\n","# # -----------------------\n","# def post_process_text(text: str) -> str:\n","#     if not text:\n","#         return \"\"\n","#     words = text.split()\n","#     cleaned_words = []\n","#     i = 0\n","#     while i < len(words):\n","#         current = words[i].lower()\n","#         count = 1\n","#         j = i + 1\n","#         while j < len(words) and words[j].lower() == current:\n","#             count += 1\n","#             j += 1\n","#         keep = min(count, 2) if count <= 3 else 1\n","#         for _ in range(keep):\n","#             cleaned_words.append(words[i])\n","#         i += count\n","#     text = \" \".join(cleaned_words)\n","\n","#     filler_sounds = [\"uh\", \"um\", \"mm\", \"hmm\", \"ah\", \"oh\", \"huh\", \"ha ha\"]\n","#     soft_fillers = [\"okay okay\", \"yes yes\", \"yes yes yes\", \"i mean\", \"you know\", \"like like\", \"ok ok\"]\n","#     for f in filler_sounds + soft_fillers:\n","#         text = re.sub(rf'\\b{re.escape(f)}\\b', '', text, flags=re.IGNORECASE)\n","\n","#     corrections = {\n","#         'access max life': 'Axis Max Life',\n","#         'axis max life': 'Axis Max Life',\n","#         'g pay': 'GPay',\n","#         'google pay': 'Google Pay',\n","#         'phone pay': 'PhonePe',\n","#         'phone pe': 'PhonePe',\n","#         'pay tm': 'Paytm',\n","#         'net banking': 'netbanking',\n","#         'some assured': 'sum assured',\n","#         'premium do': 'premium due',\n","#         'do date': 'due date',\n","#         'okay sir': 'Okay sir',\n","#     }\n","#     text_lower = text.lower()\n","#     for wrong, correct in corrections.items():\n","#         text_lower = text_lower.replace(wrong, correct)\n","\n","#     text_lower = re.sub(r'\\brs[.]?\\s*', '₹', text_lower)\n","#     text_lower = re.sub(r'\\s{2,}', ' ', text_lower)\n","#     text_lower = re.sub(r'[,]{2,}', ',', text_lower)\n","#     text_lower = re.sub(r'\\s+,', ',', text_lower)\n","#     text_lower = re.sub(r'\\s+\\.', '.', text_lower)\n","#     text_lower = re.sub(r'\\s+[!?]', lambda m: m.group(0).strip(), text_lower)\n","#     text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text_lower)\n","#     return text_lower.strip()\n","\n","# def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3, log=None):\n","#     if log is None:\n","#         log = []\n","#     log.append(\"🔍 Aggressive repetition detection started...\")\n","#     cleaned = []\n","#     for seg in segments:\n","#         text = seg[\"text\"].strip()\n","#         words = text.split()\n","#         if len(words) < 2:\n","#             continue\n","\n","#         wc = {}\n","#         for w in words:\n","#             wl = w.lower().strip('.,!?')\n","#             wc[wl] = wc.get(wl, 0) + 1\n","#         max_count = max(wc.values()) if wc else 0\n","#         dominance = max_count / len(words) if words else 0\n","#         if dominance > 0.4:\n","#             log.append(f\"🚫 Drop (dominant word): {text[:50]}...\")\n","#             continue\n","\n","#         consec = 0\n","#         max_consec = 0\n","#         for j in range(1, len(words)):\n","#             if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","#                 consec += 1\n","#                 max_consec = max(max_consec, consec + 1)\n","#             else:\n","#                 consec = 0\n","#         if max_consec > 3:\n","#             log.append(f\"🚫 Drop (consecutive repeats): {text[:50]}...\")\n","#             continue\n","\n","#         repetitive = False\n","#         for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","#             for start in range(len(words) - phrase_len * 2 + 1):\n","#                 p1 = ' '.join(words[start:start+phrase_len]).lower()\n","#                 p2 = ' '.join(words[start+phrase_len:start+phrase_len*2]).lower()\n","#                 if p1 == p2:\n","#                     coverage = (phrase_len * 2) / len(words)\n","#                     if coverage > max_repetition_ratio:\n","#                         log.append(f\"🚫 Drop (pattern repeat): {text[:50]}...\")\n","#                         repetitive = True\n","#                         break\n","#             if repetitive:\n","#                 break\n","#         if repetitive:\n","#             continue\n","\n","#         is_dup = False\n","#         for prev in cleaned[-5:]:\n","#             prev_words = prev['text'].lower().split()\n","#             curr_words = [w.lower() for w in words]\n","#             if prev_words and curr_words:\n","#                 prev_set = set(prev_words)\n","#                 curr_set = set(curr_words)\n","#                 inter = len(prev_set & curr_set)\n","#                 union = len(prev_set | curr_set)\n","#                 sim = inter / union if union else 0\n","#                 if sim > 0.7 and abs(len(prev_words) - len(curr_words)) < 5:\n","#                     log.append(f\"🚫 Drop (near-duplicate): {text[:50]}...\")\n","#                     is_dup = True\n","#                     break\n","#         if is_dup:\n","#             continue\n","\n","#         cleaned.append(seg)\n","#     log.append(f\"📊 Cleaned {len(segments)} → {len(cleaned)} segments\")\n","#     return cleaned\n","\n","# # -----------------------\n","# # Whisper + diarization\n","# # -----------------------\n","# @lru_cache(maxsize=1)\n","# def get_whisper_model(name: str):\n","#     return whisper.load_model(name)\n","\n","# def enhanced_whisper_transcription(audio_path: str, model_name: str, log) -> Dict[str, Any]:\n","#     log.append(\"--- Enhanced Whisper Transcription ---\")\n","#     # prompt = (\n","#     #     \"This is a customer support call for Axis Maxlife Insurance. \"\n","#     #     \"We will discuss policy numbers, due date, fund value, sum assured, late fee, \"\n","#     #     \"and payment methods such as Google Pay, PhonePe, Paytm and netbanking.\"\n","#     # )\n","#     agents_str = \", \".join(KNOWN_AGENTS)\n","#     prompt = (\n","#         f\"This is a customer support call from Axis Maxlife Insurance in Tamil regarding insurance renewal call. \"\n","#         f\"Agents are always one of these: {agents_str}. Use exact names like 'Jaya Parkash' or 'Swathi' \"\n","#         f\"when they introduce themselves. Discuss policy numbers, due dates, fund values, sum assured, late fees, \"\n","#         f\"and payment methods such as Google Pay, PhonePe, Paytm, netbanking.\"\n","#     )\n","#     model = get_whisper_model(model_name)\n","#     result = model.transcribe(\n","#         audio_path,\n","#         language=\"ta\",\n","#         task=\"translate\",\n","#         temperature=0.0,\n","#         beam_size=5,\n","#         patience=1.2,\n","#         condition_on_previous_text=False,\n","#         no_speech_threshold=0.8,\n","#         compression_ratio_threshold=2.0,\n","#         logprob_threshold=-0.35,\n","#         word_timestamps=False,\n","#         initial_prompt=prompt,\n","#         verbose=True,\n","#     )\n","#     log.append(\"✅ Whisper transcription done.\")\n","#     return result\n","\n","# def run_diarization(wav_path: str, hf_token: str, log):\n","#     try:\n","#         token = hf_token or os.getenv(\"HUGGING_FACE_TOKEN\", \"\")\n","#         if not token:\n","#             log.append(\"ℹ️ No HF token provided: skipping diarization (Speaker_Unknown).\")\n","#             return None\n","#         # Support both old/new arg names\n","#         try:\n","#             pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=token)\n","#         except TypeError:\n","#             pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", token=token)\n","#         if torch.cuda.is_available():\n","#             pipeline.to(torch.device(\"cuda\"))\n","#             log.append(\"✅ Using GPU for diarization.\")\n","#         diarization = pipeline(wav_path)\n","#         log.append(\"✅ Speaker diarization completed.\")\n","#         return diarization\n","#     except Exception as e:\n","#         log.append(f\"⚠️ Diarization failed: {e}\")\n","#         return None\n","\n","# # -----------------------\n","# # Dialogue building/formatting\n","# # -----------------------\n","# def get_dominant_speaker(start_time, end_time, diarization_result):\n","#     if not diarization_result:\n","#         return \"Speaker_Unknown\"\n","#     speakers = {}\n","#     for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","#         overlap_start = max(start_time, segment.start)\n","#         overlap_end = min(end_time, segment.end)\n","#         dur = max(0.0, overlap_end - overlap_start)\n","#         if dur > 0:\n","#             speakers[speaker] = speakers.get(speaker, 0.0) + dur\n","#     return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","# def build_dialogue(processed_segments, diarization):\n","#     dialogue = []\n","#     current_speaker = None\n","#     current_texts = []\n","#     current_start = 0.0\n","#     current_end = 0.0\n","#     for seg in processed_segments:\n","#         start = seg['start']\n","#         end = seg['end']\n","#         text = seg['text'].strip()\n","#         speaker = get_dominant_speaker(start, end, diarization)\n","#         if (speaker == current_speaker and current_speaker and (start - current_end) < 3.0):\n","#             current_texts.append(text)\n","#             current_end = end\n","#         else:\n","#             if current_speaker and current_texts:\n","#                 combined = ' '.join(current_texts)\n","#                 if len(combined.strip()) > 10:\n","#                     dialogue.append({\n","#                         'speaker': current_speaker,\n","#                         'text': combined,\n","#                         'start_time': current_start,\n","#                         'end_time': current_end\n","#                     })\n","#             current_speaker = speaker\n","#             current_texts = [text]\n","#             current_start = start\n","#             current_end = end\n","#     if current_speaker and current_texts:\n","#         combined = ' '.join(current_texts)\n","#         if len(combined.strip()) > 10:\n","#             dialogue.append({\n","#                 'speaker': current_speaker,\n","#                 'text': combined,\n","#                 'start_time': current_start,\n","#                 'end_time': current_end\n","#             })\n","#     return dialogue\n","\n","# def format_dialogue_markdown(dialogue: List[Dict[str, Any]]) -> str:\n","#     if not dialogue:\n","#         return \"_No substantial dialogue detected._\"\n","#     lines = [\"## 🎭 Dialogue\", \"\"]\n","#     for d in dialogue:\n","#         ts = f\"[{d['start_time']:.1f}s - {d['end_time']:.1f}s]\"\n","#         lines.append(f\"**{d['speaker']} {ts}:**\")\n","#         lines.append(f\"{d['text']}\")\n","#         lines.append(\"\")\n","#     return \"\\n\".join(lines)\n","\n","# # -----------------------\n","# # Field extraction from Dialogue\n","# # -----------------------\n","# def _norm(text: str) -> str:\n","#     return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","# def extract_fields_from_dialogue(dialogue):\n","#     full = \" \".join(_norm(d[\"text\"]) for d in dialogue).lower()\n","#     get = {}\n","\n","#     m = re.search(r\"(policy\\s*(no|number)\\s*[:\\-]?\\s*)(\\d{6,12})\", full)\n","#     get[\"Policy No\"] = m.group(3) if m else \"—\"\n","\n","#     m = re.search(r\"(this is|i am|speaking)\\s+([a-zA-Z][a-zA-Z.\\s]{2,30})\\s*(from|with|here)\", full)\n","#     get[\"Associate Name\"] = m.group(2).title().strip() if m else \"—\"\n","\n","#     m = re.search(r\"(mr|mrs|ms)\\.?\\s+([a-zA-Z][a-zA-Z.\\s]{2,30})\", full)\n","#     get[\"Customer Name\"] = m.group(2).title().strip() if m else \"—\"\n","\n","#     m = re.search(r\"(?:mobile|phone|contact)\\s*(no|number)?\\s*[:\\-]?\\s*(\\+?91[\\s\\-]?)?(\\d{10})\", full)\n","#     get[\"Mobile Number\"] = ((\"+\" + \"91 \") if m and m.group(2) else \"\") + (m.group(3) if m else \"—\")\n","\n","#     m = re.search(r\"(due\\s*date|premium\\s*due)\\s*[:\\-]?\\s*([0-3]?\\d[\\/\\-][01]?\\d(?:[\\/\\-]\\d{2,4})?|[0-3]?\\d\\s+[a-zA-Z]{3,9}\\s+\\d{4})\", full)\n","#     get[\"Due Date\"] = m.group(2) if m else \"—\"\n","\n","#     m = re.search(r\"(?:premium\\s*(amount|due)?\\s*[:\\-]?\\s*)(?:₹|rs\\.?\\s*)\\s*([0-9,]+(\\.\\d{1,2})?)\", full)\n","#     get[\"Premium Amount\"] = \"₹\" + m.group(2) if m else \"—\"\n","\n","#     m = re.search(r\"(sum\\s*assured)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+)\", full)\n","#     get[\"Sum Assured\"] = \"₹\" + m.group(2) if m else \"—\"\n","\n","#     m = re.search(r\"(fund\\s*value)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+(\\.\\d{1,2})?)\", full)\n","#     get[\"Fund Value\"] = \"₹\" + m.group(2) if m else \"—\"\n","\n","#     m = re.search(r\"(policy\\s*status)\\s*[:\\-]?\\s*(active|inactive|lapsed|in force|paid up)\", full)\n","#     get[\"Policy Status\"] = m.group(2).title() if m else \"—\"\n","\n","#     m = re.search(r\"(payment\\s*(mode|method))\\s*[:\\-]?\\s*(cash|card|netbanking|upi|google\\s*pay|gpay|phonepe|paytm)\", full)\n","#     get[\"Payment Mode\"] = (m.group(3).title().replace(\"Gpay\",\"GPay\").replace(\"Google Pay\",\"Google Pay\").replace(\"Phonepe\",\"PhonePe\")) if m else \"—\"\n","\n","#     return get\n","\n","# # -----------------------\n","# # Report UI\n","# # -----------------------\n","# def build_report_html(fields: dict, title: str = \"BRU - CQ Sheet\") -> str:\n","#     \"\"\"Render a simple HTML report (title + label/value rows) like your reference sheet.\"\"\"\n","#     style = \"\"\"\n","#     <style>\n","#       .vs-report{max-width:860px;margin:10px auto 24px;background:#111319;border:1px solid #2a2f3a;\n","#                  border-radius:14px;padding:18px 22px;font-family:ui-sans-serif,system-ui,Segoe UI,Roboto,Helvetica,Arial;\n","#                  color:#e7e9ee}\n","#       .vs-title{font-size:20px;font-weight:700;margin:4px 0 14px;color:#f2f4f8}\n","#       .vs-grid{width:100%;border-collapse:separate;border-spacing:0 8px}\n","#       .vs-grid th{width:30%;text-align:left;color:#aeb6c2;font-weight:600;padding:10px 12px;background:#0c0e13;border-radius:10px 0 0 10px;border:1px solid #272c36;border-right:none}\n","#       .vs-grid td{padding:10px 12px;background:#0c0f16;border-radius:0 10px 10px 0;border:1px solid #272c36;border-left:none}\n","#       .vs-section{margin-top:18px}\n","#       .vs-sec-h{font-size:14px;font-weight:700;margin:8px 0 6px;color:#d9dde6}\n","#       .vs-muted{color:#9aa3af}\n","#       .vs-foot{margin-top:12px;color:#9aa3af;font-size:12px}\n","#     </style>\n","#     \"\"\"\n","#     rows = \"\".join(\n","#         f\"<tr><th>{escape(k)}</th><td>{escape(str(v) if v is not None else '—')}</td></tr>\"\n","#         for k, v in fields.items()\n","#     )\n","#     html = f\"\"\"\n","#     {style}\n","#     <div class=\"vs-report\">\n","#       <div class=\"vs-title\">{escape(title)}</div>\n","#       <table class=\"vs-grid\">{rows}</table>\n","#       <div class=\"vs-section\">\n","#         <div class=\"vs-sec-h\">Call Highlights</div>\n","#         <div class=\"vs-muted\">We’ll auto-fill specific highlights later (issues, commitments, amounts, dates).</div>\n","#       </div>\n","#       <div class=\"vs-section\">\n","#         <div class=\"vs-sec-h\">Compliance / Quality</div>\n","#         <div class=\"vs-muted\">Checklist to be added per your requirements.</div>\n","#       </div>\n","#       <div class=\"vs-foot\">Generated live from the audio-derived dialogue.</div>\n","#     </div>\n","#     \"\"\"\n","#     return html\n","\n","# def generate_report_html_from_state(dialogue, fields):\n","#     \"\"\"Return HTML (no file).\"\"\"\n","#     if not dialogue:\n","#         return \"<div class='vs-report'><div class='vs-title'>BRU - CQ Sheet</div><div class='vs-muted'>⚠️ Process an audio first to build the report.</div></div>\"\n","#     if not fields or not isinstance(fields, dict):\n","#         fields = extract_fields_from_dialogue(dialogue)\n","#     return build_report_html(fields, title=\"BRU - CQ Sheet\")\n","\n","# # -----------------------\n","# # Gradio app\n","# # -----------------------\n","# def transcribe_and_diarize(audio_file, model_name, enable_diarization, hf_token):\n","#     logs = []\n","#     if audio_file is None:\n","#         return \"_Please upload an audio file._\", \"{}\", \"No audio provided.\", [], {}\n","\n","#     input_path = audio_file\n","#     cleaned_path = os.path.join(os.path.dirname(input_path), \"cleaned_audio_for_asr_and_diarization.wav\")\n","\n","#     ok = smart_audio_preprocessing(input_path, cleaned_path, logs)\n","#     if not ok:\n","#         return \"_Preprocessing failed._\", \"{}\", \"\\n\".join(logs), [], {}\n","\n","#     try:\n","#         whisper_result = enhanced_whisper_transcription(cleaned_path, model_name, logs)\n","#     except Exception as e:\n","#         logs.append(f\"❌ Whisper transcription failed: {e}\")\n","#         return \"_Transcription failed._\", \"{}\", \"\\n\".join(logs), [], {}\n","\n","#     segments = (whisper_result.get(\"segments\") or [])\n","#     cleaned_segments = detect_and_remove_repetitions(segments, log=logs)\n","\n","#     processed_segments = []\n","#     for seg in cleaned_segments:\n","#         processed_text = post_process_text(seg['text'])\n","#         if processed_text.strip() and len(processed_text.strip()) > 5:\n","#             seg_copy = dict(seg)\n","#             seg_copy['text'] = processed_text\n","#             processed_segments.append(seg_copy)\n","\n","#     diarization = None\n","#     if enable_diarization:\n","#         diarization = run_diarization(cleaned_path, hf_token, logs)\n","\n","#     dialogue = build_dialogue(processed_segments, diarization)\n","#     fields = extract_fields_from_dialogue(dialogue)\n","\n","#     md = format_dialogue_markdown(dialogue)\n","#     output_payload = {\n","#         \"metadata\": {\n","#             \"total_duration\": whisper_result.get('duration', 0),\n","#             \"total_speakers\": len(set(d['speaker'] for d in dialogue)),\n","#             \"total_segments\": len(dialogue),\n","#             \"model_used\": f\"whisper-{model_name}\",\n","#             \"processing_successful\": True,\n","#             \"anti_repetition_applied\": True,\n","#             \"diarization_enabled\": bool(enable_diarization),\n","#         },\n","#         \"dialogue\": dialogue,\n","#         \"fields\": fields,\n","#     }\n","#     return md, json.dumps(output_payload, indent=2, ensure_ascii=False), \"\\n\".join(logs), dialogue, fields\n","\n","# def _enable_build_button(dialogue):\n","#     # enable only if we actually have a dialogue from Process\n","#     return gr.update(interactive=bool(dialogue))\n","\n","# # Build UI\n","# latest_dialogue_state = gr.State([])\n","# latest_fields_state = gr.State({})\n","\n","# with gr.Blocks(title=\"Speech Analysis - QA\") as demo:\n","#     gr.Markdown(\"# 🗣️ Whisper Call Transcriber → 📑 Report\\nUpload a call recording, get a clean dialogue and a report like your reference sheet.\")\n","#     with gr.Row():\n","#         audio = gr.Audio(type=\"filepath\", label=\"Upload audio (wav/mp3/m4a...)\")\n","#     with gr.Row():\n","#         model_name = gr.Dropdown(choices=[\"large-v3\",\"large\", \"medium\", \"small\", \"base\", \"tiny\"], value=DEFAULT_MODEL_NAME, label=\"Whisper model\")\n","#         enable_diar = gr.Checkbox(value=True, label=\"Enable diarization (Pyannote)\")\n","#         hf_token = gr.Textbox(value=DEFAULT_HF_TOKEN, label=\"Hugging Face token (required if diarization enabled)\", type=\"password\", placeholder=\"hf_xxx...\")\n","#     run_btn = gr.Button(\"▶️ Process\")\n","\n","#     with gr.Tab(\"Dialogue\"):\n","#         dialogue_md = gr.Markdown()\n","\n","#     with gr.Tab(\"Report Analysis\"):\n","#         gr.Markdown(\"Build a report **from the audio-derived dialogue** and render it here.\")\n","#         build_btn = gr.Button(\"📄 Generate Report from Dialogue\")  # <- remove interactive=False\n","#         report_html = gr.HTML(value=\"<div class='vs-muted'>_No report yet._</div>\")\n","\n","#     with gr.Tab(\"JSON\"):\n","#         json_out = gr.Code(language=\"json\", label=\"Output JSON\")\n","\n","#     with gr.Tab(\"Logs\"):\n","#         logs_out = gr.Textbox(label=\"Processing Logs\", lines=18)\n","#         dialogue_inline = gr.Markdown(value=\"_Dialogue will appear here_\")\n","\n","#     # Wire processing\n","#     run_btn.click(\n","#         transcribe_and_diarize,\n","#         inputs=[audio, model_name, enable_diar, hf_token],\n","#         outputs=[dialogue_md, json_out, logs_out, latest_dialogue_state, latest_fields_state]\n","#     ).then(\n","#         # optional: keep or remove this; button is always clickable now\n","#         _enable_build_button, inputs=[latest_dialogue_state], outputs=[build_btn]\n","#     ).then(\n","#         lambda md, *_: md,  # mirror Dialogue under Logs\n","#         inputs=[dialogue_md],\n","#         outputs=[dialogue_inline]\n","#     ).then(\n","#         # NEW: generate the report HTML immediately after processing\n","#         generate_report_html_from_state,\n","#         inputs=[latest_dialogue_state, latest_fields_state],\n","#         outputs=[report_html]\n","#     )\n","\n","#     # Wire report builder (render HTML, no download)\n","#     build_btn.click(\n","#         generate_report_html_from_state,\n","#         inputs=[latest_dialogue_state, latest_fields_state],\n","#         outputs=[report_html]\n","#     )\n","\n","\n","# if __name__ == \"__main__\":\n","#     # In notebooks/Colab, letting Gradio choose a free port is easiest\n","#     gr.close_all()\n","#     demo.launch(share=True)\n"],"metadata":{"id":"-ZzgcjVjTcSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Show what's on 7860\n","# !lsof -i:7860 || true\n","# # Kill anything still bound there\n","# !fuser -k 7860/tcp || true\n","\n","# import gradio as gr\n","# gr.close_all()\n","# demo.launch(share=True)  # or pick another port like 7861\n"],"metadata":{"id":"wnKkwun8NmXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rwVjmNDcFHy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sample report structure"],"metadata":{"id":"565j0vzgFH1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # app_gradio.py\n","# # ---------------------------------------------------------\n","# # Whisper + (optional) Pyannote UI with \"Dialogue\" and \"Report Analysis\"\n","# # ---------------------------------------------------------\n","\n","# import os, re, json, subprocess\n","# from functools import lru_cache\n","# from typing import List, Dict, Any\n","# from html import escape\n","\n","# import gradio as gr\n","# import torch\n","# import whisper\n","# from pyannote.audio import Pipeline\n","\n","# # -----------------------\n","# # Config\n","# # -----------------------\n","# DEFAULT_MODEL_NAME = os.getenv(\"WHISPER_MODEL\", \"large-v3\")\n","# SAMPLE_RATE_HZ = 16000\n","# DEFAULT_HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\", \"hf_\")\n","\n","# KNOWN_AGENTS = [\n","#     \"Jaya Parkash\", \"Chandru\", \"Sneha\", \"Kowsalya\", \"Swathi\",\n","#     \"Arogya Marry\", \"Delphina\", \"Aesu Marry\"\n","# ]\n","\n","# # -----------------------\n","# # FFmpeg / audio utils\n","# # -----------------------\n","# def get_audio_duration(audio_path: str) -> float:\n","#     try:\n","#         out = subprocess.run(\n","#             [\"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\"-of\",\"default=noprint_wrappers=1:nokey=1\",audio_path],\n","#             capture_output=True, text=True, check=True\n","#         )\n","#         return float(out.stdout.strip())\n","#     except Exception:\n","#         return 0.0\n","\n","# def _ffmpeg_run(cmd: List[str]) -> bool:\n","#     try:\n","#         subprocess.run(cmd, check=True, capture_output=True, text=True)\n","#         return True\n","#     except subprocess.CalledProcessError:\n","#         return False\n","\n","# def audio_preprocessing_v1(i,o):\n","#     return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","#                         \"-af\",\"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\"-y\",o])\n","# def audio_preprocessing_v2(i,o):\n","#     return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","#                         \"-af\",\"loudnorm=I=-23:TP=-2,highpass=f=100\",\"-y\",o])\n","# def audio_preprocessing_v3(i,o):\n","#     return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","#                         \"-af\",\"loudnorm\",\"-y\",o])\n","# def audio_preprocessing_v4(i,o):\n","#     return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\"-y\",o])\n","\n","# def smart_audio_preprocessing(input_path, output_path, log: List[str]) -> bool:\n","#     original_dur = get_audio_duration(input_path)\n","#     log.append(f\"Original audio duration: {original_dur:.2f}s\")\n","#     methods = [(\"Advanced (afftdn)\",audio_preprocessing_v1),(\"Simplified\",audio_preprocessing_v2),\n","#                (\"Basic\",audio_preprocessing_v3),(\"Minimal\",audio_preprocessing_v4)]\n","#     for idx,(name,fn) in enumerate(methods,1):\n","#         log.append(f\"→ Trying {name} preprocessing...\")\n","#         if fn(input_path, output_path) and os.path.exists(output_path):\n","#             processed_dur = get_audio_duration(output_path)\n","#             log.append(f\"Processed duration: {processed_dur:.2f}s\")\n","#             if abs(original_dur - processed_dur) < 1.0:\n","#                 log.append(f\"✅ Preprocessing successful with method {idx}: {name}\")\n","#                 return True\n","#             else:\n","#                 log.append(f\"⚠️ Duration mismatch on method {idx}; trying next...\")\n","#     log.append(\"❌ All preprocessing methods failed.\")\n","#     return False\n","\n","# # -----------------------\n","# # Text cleanup / repetition control\n","# # -----------------------\n","# def post_process_text(text: str) -> str:\n","#     if not text: return \"\"\n","#     words = text.split()\n","#     cleaned_words, i = [], 0\n","#     while i < len(words):\n","#         cur = words[i].lower(); count = 1; j = i+1\n","#         while j < len(words) and words[j].lower() == cur: count += 1; j += 1\n","#         cleaned_words.extend([words[i]] * (min(count,2) if count <= 3 else 1)); i += count\n","#     text = \" \".join(cleaned_words)\n","\n","#     filler = [\"uh\",\"um\",\"mm\",\"hmm\",\"ah\",\"oh\",\"huh\",\"ha ha\",\n","#               \"okay okay\",\"yes yes\",\"yes yes yes\",\"i mean\",\"you know\",\"like like\",\"ok ok\"]\n","#     for f in filler:\n","#         text = re.sub(rf'\\b{re.escape(f)}\\b','',text,flags=re.IGNORECASE)\n","\n","#     repl = {'access max life':'Axis Max Life','axis max life':'Axis Max Life','g pay':'GPay',\n","#             'google pay':'Google Pay','phone pay':'PhonePe','phone pe':'PhonePe','pay tm':'Paytm',\n","#             'net banking':'netbanking','some assured':'sum assured','premium do':'premium due',\n","#             'do date':'due date','okay sir':'Okay sir'}\n","#     low = text.lower()\n","#     for w,c in repl.items(): low = low.replace(w,c)\n","#     low = re.sub(r'\\brs[.]?\\s*','₹',low)\n","#     low = re.sub(r'\\s{2,}',' ',low)\n","#     low = re.sub(r'[,]{2,}',',',low)\n","#     low = re.sub(r'\\s+,',',',low)\n","#     low = re.sub(r'\\s+\\.','.',low)\n","#     low = re.sub(r'\\s+[!?]', lambda m:m.group(0).strip(), low)\n","#     low = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m:m.group(1)+m.group(2).upper(), low)\n","#     return low.strip()\n","\n","# def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3, log=None):\n","#     log = log or []; log.append(\"🔍 Aggressive repetition detection started...\")\n","#     cleaned = []\n","#     for seg in segments:\n","#         text = seg[\"text\"].strip(); words = text.split()\n","#         if len(words) < 2: continue\n","#         wc = {}\n","#         for w in words:\n","#             wl = w.lower().strip('.,!?'); wc[wl] = wc.get(wl,0)+1\n","#         dominance = (max(wc.values())/len(words)) if wc else 0\n","#         if dominance > 0.4: log.append(f\"🚫 Drop (dominant word): {text[:50]}...\"); continue\n","\n","#         consec=max_consec=0\n","#         for j in range(1,len(words)):\n","#             if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","#                 consec += 1; max_consec = max(max_consec, consec+1)\n","#             else: consec = 0\n","#         if max_consec > 3: log.append(f\"🚫 Drop (consecutive repeats): {text[:50]}...\"); continue\n","\n","#         repetitive=False\n","#         for L in range(2, min(len(words)//3+1,8)):\n","#             for s in range(len(words)-L*2+1):\n","#                 if ' '.join(words[s:s+L]).lower() == ' '.join(words[s+L:s+2*L]).lower():\n","#                     if (L*2)/len(words) > max_repetition_ratio:\n","#                         log.append(f\"🚫 Drop (pattern repeat): {text[:50]}...\"); repetitive=True; break\n","#             if repetitive: break\n","#         if repetitive: continue\n","\n","#         is_dup=False\n","#         for prev in cleaned[-5:]:\n","#             a,b=set(prev['text'].lower().split()), set(w.lower() for w in words)\n","#             if a|b and (len(a&b)/len(a|b))>0.7 and abs(len(a)-len(b))<5:\n","#                 log.append(f\"🚫 Drop (near-duplicate): {text[:50]}...\"); is_dup=True; break\n","#         if is_dup: continue\n","#         cleaned.append(seg)\n","#     log.append(f\"📊 Cleaned {len(segments)} → {len(cleaned)} segments\")\n","#     return cleaned\n","\n","# # -----------------------\n","# # Whisper + diarization\n","# # -----------------------\n","# @lru_cache(maxsize=1)\n","# def get_whisper_model(name: str):\n","#     return whisper.load_model(name)\n","\n","# def enhanced_whisper_transcription(audio_path: str, model_name: str, log) -> Dict[str, Any]:\n","#     log.append(\"--- Enhanced Whisper Transcription ---\")\n","#     agents_str = \", \".join(KNOWN_AGENTS)\n","#     prompt = (\n","#         f\"This is a customer support call from Axis Maxlife Insurance in Tamil regarding insurance renewal call. \"\n","#         f\"Agents are always one of these: {agents_str}. Use exact names like 'Jaya Parkash' or 'Swathi'. \"\n","#         f\"Discuss policy numbers, due dates, fund values, sum assured, late fees, and payment methods such as \"\n","#         f\"Google Pay, PhonePe, Paytm, netbanking.\"\n","#     )\n","#     model = get_whisper_model(model_name)\n","#     result = model.transcribe(\n","#         audio_path, language=\"ta\", task=\"translate\", temperature=0.0,\n","#         beam_size=5, patience=1.2, condition_on_previous_text=False,\n","#         no_speech_threshold=0.8, compression_ratio_threshold=2.0,\n","#         logprob_threshold=-0.35, word_timestamps=False,\n","#         initial_prompt=prompt, verbose=True,\n","#     )\n","#     log.append(\"✅ Whisper transcription done.\")\n","#     return result\n","\n","# def run_diarization(wav_path: str, hf_token: str, log):\n","#     try:\n","#         token = hf_token or os.getenv(\"HUGGING_FACE_TOKEN\", \"\")\n","#         if not token:\n","#             log.append(\"ℹ️ No HF token provided: skipping diarization (Speaker_Unknown).\")\n","#             return None\n","#         try:\n","#             pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=token)\n","#         except TypeError:\n","#             pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", token=token)\n","#         if torch.cuda.is_available():\n","#             pipeline.to(torch.device(\"cuda\")); log.append(\"✅ Using GPU for diarization.\")\n","#         diarization = pipeline(wav_path); log.append(\"✅ Speaker diarization completed.\")\n","#         return diarization\n","#     except Exception as e:\n","#         log.append(f\"⚠️ Diarization failed: {e}\"); return None\n","\n","# # -----------------------\n","# # Dialogue + fields\n","# # -----------------------\n","# def get_dominant_speaker(start_time, end_time, diarization_result):\n","#     if not diarization_result: return \"Speaker_Unknown\"\n","#     speakers = {}\n","#     for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","#         overlap_start = max(start_time, segment.start)\n","#         overlap_end = min(end_time, segment.end)\n","#         dur = max(0.0, overlap_end - overlap_start)\n","#         if dur > 0: speakers[speaker] = speakers.get(speaker, 0.0) + dur\n","#     return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","# def build_dialogue(processed_segments, diarization):\n","#     out=[]; cur_spk=None; cur_texts=[]; cur_start=0.0; cur_end=0.0\n","#     for seg in processed_segments:\n","#         start,end,txt = seg['start'], seg['end'], seg['text'].strip()\n","#         spk = get_dominant_speaker(start,end,diarization)\n","#         if (spk==cur_spk and cur_spk and (start-cur_end)<3.0):\n","#             cur_texts.append(txt); cur_end=end\n","#         else:\n","#             if cur_spk and cur_texts:\n","#                 c=' '.join(cur_texts)\n","#                 if len(c.strip())>10: out.append({'speaker':cur_spk,'text':c,'start_time':cur_start,'end_time':cur_end})\n","#             cur_spk,cur_texts,cur_start,cur_end = spk,[txt],start,end\n","#     if cur_spk and cur_texts:\n","#         c=' '.join(cur_texts)\n","#         if len(c.strip())>10: out.append({'speaker':cur_spk,'text':c,'start_time':cur_start,'end_time':cur_end})\n","#     return out\n","\n","# def format_dialogue_markdown(dialogue: List[Dict[str, Any]]) -> str:\n","#     if not dialogue: return \"_No substantial dialogue detected._\"\n","#     lines=[\"## 🎭 Dialogue\",\"\"]\n","#     for d in dialogue:\n","#         lines.append(f\"**{d['speaker']} [{d['start_time']:.1f}s - {d['end_time']:.1f}s]:**\")\n","#         lines.append(d['text']); lines.append(\"\")\n","#     return \"\\n\".join(lines)\n","\n","# def _norm(t: str) -> str: return re.sub(r\"\\s+\",\" \",t).strip()\n","\n","# def extract_fields_from_dialogue(dialogue):\n","#     full = \" \".join(_norm(d[\"text\"]) for d in dialogue).lower()\n","#     g={}\n","#     m=re.search(r\"(policy\\s*(no|number)\\s*[:\\-]?\\s*)(\\d{6,12})\",full); g[\"Policy No\"]=m.group(3) if m else \"—\"\n","#     m=re.search(r\"(this is|i am|speaking)\\s+([a-zA-Z][a-zA-Z.\\s]{2,30})\\s*(from|with|here)\",full); g[\"Associate Name\"]=m.group(2).title().strip() if m else \"—\"\n","#     m=re.search(r\"(mr|mrs|ms)\\.?\\s+([a-zA-Z][a-zA-Z.\\s]{2,30})\",full); g[\"Customer Name\"]=m.group(2).title().strip() if m else \"—\"\n","#     m=re.search(r\"(?:mobile|phone|contact)\\s*(no|number)?\\s*[:\\-]?\\s*(\\+?91[\\s\\-]?)?(\\d{10})\",full); g[\"Mobile Number\"]=((\"+\"+\"91 \") if m and m.group(2) else \"\")+(m.group(3) if m else \"—\")\n","#     m=re.search(r\"(due\\s*date|premium\\s*due)\\s*[:\\-]?\\s*([0-3]?\\d[\\/\\-][01]?\\d(?:[\\/\\-]\\d{2,4})?|[0-3]?\\d\\s+[a-zA-Z]{3,9}\\s+\\d{4})\",full); g[\"Due Date\"]=m.group(2) if m else \"—\"\n","#     m=re.search(r\"(?:premium\\s*(amount|due)?\\s*[:\\-]?\\s*)(?:₹|rs\\.?\\s*)\\s*([0-9,]+(\\.\\d{1,2})?)\",full); g[\"Premium Amount\"]=\"₹\"+m.group(2) if m else \"—\"\n","#     m=re.search(r\"(sum\\s*assured)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+)\",full); g[\"Sum Assured\"]=\"₹\"+m.group(2) if m else \"—\"\n","#     m=re.search(r\"(fund\\s*value)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+(\\.\\d{1,2})?)\",full); g[\"Fund Value\"]=\"₹\"+m.group(2) if m else \"—\"\n","#     m=re.search(r\"(policy\\s*status)\\s*[:\\-]?\\s*(active|inactive|lapsed|in force|paid up)\",full); g[\"Policy Status\"]=m.group(2).title() if m else \"—\"\n","#     m=re.search(r\"(payment\\s*(mode|method))\\s*[:\\-]?\\s*(cash|card|netbanking|upi|google\\s*pay|gpay|phonepe|paytm)\",full)\n","#     g[\"Payment Mode\"]=(m.group(3).title().replace(\"Gpay\",\"GPay\").replace(\"Google Pay\",\"Google Pay\").replace(\"Phonepe\",\"PhonePe\")) if m else \"—\"\n","#     return g\n","\n","# # -----------------------\n","# # Report (HTML) — auto-rendered\n","# # -----------------------\n","# def build_report_html(fields: dict, title: str = \"BRU - CQ Sheet\") -> str:\n","#     def fv(k): return escape(str(fields.get(k,\"—\") or \"—\"))\n","#     style = \"\"\"\n","#     <style>\n","#       .vs-wrap{max-width:1100px;margin:10px auto 24px;font-family:ui-sans-serif,system-ui,Segoe UI,Roboto,Helvetica,Arial}\n","#       .vs-card{background:#111319;border:1px solid #2a2f3a;border-radius:14px;padding:16px;color:#e7e9ee}\n","#       .vs-title{font-size:20px;font-weight:800;margin:0 0 12px;color:#f2f4f8}\n","#       .meta{width:100%;border-collapse:collapse;font-size:13px}\n","#       .meta th,.meta td{border:1px solid #2a2f3a;padding:8px 10px}\n","#       .meta th{background:#0c0e13;text-align:left;width:20%;color:#b8c1ce}\n","#       .meta td{background:#0c0f16}\n","#       .sec{margin-top:16px}\n","#       .grid{width:100%;border-collapse:separate;border-spacing:0 6px;font-size:13px}\n","#       .grid th{background:#0c0e13;color:#aeb6c2;text-align:left;padding:10px;border:1px solid #2a2f3a;border-right:none;border-radius:8px 0 0 8px;width:35%}\n","#       .grid td{background:#0c0f16;padding:10px;border:1px solid #2a2f3a;border-left:none;border-radius:0 8px 8px 0}\n","#       .muted{color:#9aa3af}\n","#     </style>\n","#     \"\"\"\n","#     meta = f\"\"\"\n","#     <table class=\"meta\">\n","#       <tr><th>Policy No</th><td>{fv('Policy No')}</td><th>Associate Name</th><td>{fv('Associate Name')}</td></tr>\n","#       <tr><th>Customer Name</th><td>{fv('Customer Name')}</td><th>Mobile Number</th><td>{fv('Mobile Number')}</td></tr>\n","#       <tr><th>Due Date</th><td>{fv('Due Date')}</td><th>Premium Amount</th><td>{fv('Premium Amount')}</td></tr>\n","#       <tr><th>Sum Assured</th><td>{fv('Sum Assured')}</td><th>Fund Value</th><td>{fv('Fund Value')}</td></tr>\n","#       <tr><th>Policy Status</th><td>{fv('Policy Status')}</td><th>Payment Mode</th><td>{fv('Payment Mode')}</td></tr>\n","#     </table>\n","#     \"\"\"\n","\n","#     rows = [\n","#         (\"Opening a Call\",\"Greeting, self-introduction, branding & opening.\"),\n","#         (\"Opening a Call\",\"Ask for customer’s availability to start the conversation.\"),\n","#         (\"Customer Service & Assistance\",\"Linkage with last recorded disposition/remarks.\"),\n","#         (\"Customer Service & Assistance\",\"Identify need & provide suitable response.\"),\n","#         (\"Retention & Knowledge\",\"Explore reasons for non-payment; educate benefits/charges.\"),\n","#         (\"Compliance\",\"Inform correct premium & due date.\"),\n","#         (\"Soft Skill & Communication\",\"Active listening; quick responses.\"),\n","#         (\"Call Closing\",\"Close as per script.\"),\n","#         (\"Data Enrichment\",\"Collect email & alternate number.\"),\n","#         (\"Call Score\",\"(Template row; scoring wiring TBD)\"),\n","#     ]\n","#     grid = \"\".join(f\"<tr><th>{escape(sec)}</th><td>{escape(item)}</td></tr>\" for sec,item in rows)\n","\n","#     return f\"\"\"{style}\n","#     <div class=\"vs-wrap\">\n","#       <div class=\"vs-card\">\n","#         <div class=\"vs-title\">{escape(title)}</div>\n","#         {meta}\n","#         <div class=\"sec\"><table class=\"grid\">{grid}</table>\n","#         </div>\n","#       </div>\n","#     </div>\n","#     \"\"\"\n","\n","# def generate_report_html_from_state(dialogue, fields):\n","#     try:\n","#         if not dialogue:\n","#             return (\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div>\"\n","#                     \"<div class='muted'>⚠️ Process an audio first to build the report.</div></div></div>\")\n","#         if not fields or not isinstance(fields, dict):\n","#             fields = extract_fields_from_dialogue(dialogue)\n","#         return build_report_html(fields, title=\"BRU - CQ Sheet\")\n","#     except Exception as e:\n","#         return f\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Error while building report: {escape(str(e))}</div></div></div>\"\n","\n","# # -----------------------\n","# # Main callback — returns the report HTML directly (no .then needed)\n","# # -----------------------\n","# def transcribe_and_diarize(audio_file, model_name, enable_diarization, hf_token):\n","#     logs=[]\n","#     if audio_file is None:\n","#         html = \"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Upload an audio first.</div></div></div>\"\n","#         return \"_Please upload an audio file._\", \"{}\", \"No audio provided.\", [], {}, html\n","\n","#     input_path = audio_file\n","#     cleaned_path = os.path.join(os.path.dirname(input_path), \"cleaned_audio_for_asr_and_diarization.wav\")\n","\n","#     ok = smart_audio_preprocessing(input_path, cleaned_path, logs)\n","#     if not ok:\n","#         html = \"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Preprocessing failed.</div></div></div>\"\n","#         return \"_Preprocessing failed._\", \"{}\", \"\\n\".join(logs), [], {}, html\n","\n","#     try:\n","#         whisper_result = enhanced_whisper_transcription(cleaned_path, model_name, logs)\n","#     except Exception as e:\n","#         logs.append(f\"❌ Whisper transcription failed: {e}\")\n","#         html = f\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Transcription failed: {escape(str(e))}</div></div></div>\"\n","#         return \"_Transcription failed._\", \"{}\", \"\\n\".join(logs), [], {}, html\n","\n","#     segments = (whisper_result.get(\"segments\") or [])\n","#     cleaned_segments = detect_and_remove_repetitions(segments, log=logs)\n","\n","#     processed_segments = []\n","#     for seg in cleaned_segments:\n","#         processed_text = post_process_text(seg['text'])\n","#         if processed_text.strip() and len(processed_text.strip()) > 5:\n","#             seg_copy = dict(seg); seg_copy['text'] = processed_text\n","#             processed_segments.append(seg_copy)\n","\n","#     diarization = run_diarization(cleaned_path, hf_token, logs) if enable_diarization else None\n","\n","#     dialogue = build_dialogue(processed_segments, diarization)\n","#     fields   = extract_fields_from_dialogue(dialogue)\n","\n","#     md = format_dialogue_markdown(dialogue)\n","#     payload = {\n","#         \"metadata\": {\n","#             \"total_duration\": whisper_result.get('duration', 0),\n","#             \"total_speakers\": len(set(d['speaker'] for d in dialogue)),\n","#             \"total_segments\": len(dialogue),\n","#             \"model_used\": f\"whisper-{model_name}\",\n","#             \"processing_successful\": True,\n","#             \"anti_repetition_applied\": True,\n","#             \"diarization_enabled\": bool(enable_diarization),\n","#         },\n","#         \"dialogue\": dialogue,\n","#         \"fields\": fields,\n","#     }\n","\n","#     # Build the report HTML HERE and return it as an output\n","#     report_html = generate_report_html_from_state(dialogue, fields)\n","#     return md, json.dumps(payload, indent=2, ensure_ascii=False), \"\\n\".join(logs), dialogue, fields, report_html\n","\n","# # -----------------------\n","# # UI\n","# # -----------------------\n","# latest_dialogue_state = gr.State([])\n","# latest_fields_state  = gr.State({})\n","\n","# with gr.Blocks(title=\"Speech Analysis - QA\") as demo:\n","#     gr.Markdown(\"# 🗣️ Whisper Call Transcriber → 📑 Report\\nUpload a call recording, get a clean dialogue and a report like your reference sheet.\")\n","\n","#     with gr.Row():\n","#         audio = gr.Audio(type=\"filepath\", label=\"Upload audio (wav/mp3/m4a...)\")\n","#     with gr.Row():\n","#         model_name = gr.Dropdown(choices=[\"large-v3\",\"large\",\"medium\",\"small\",\"base\",\"tiny\"],\n","#                                  value=DEFAULT_MODEL_NAME, label=\"Whisper model\")\n","#         enable_diar = gr.Checkbox(value=True, label=\"Enable diarization (Pyannote)\")\n","#         hf_token = gr.Textbox(value=DEFAULT_HF_TOKEN,\n","#                               label=\"Hugging Face token (required if diarization enabled)\",\n","#                               type=\"password\", placeholder=\"hf_xxx...\")\n","\n","#     run_btn = gr.Button(\"▶️ Process\")\n","\n","#     with gr.Tab(\"Dialogue\"):\n","#         dialogue_md = gr.Markdown()\n","\n","#     with gr.Tab(\"Report Analysis\"):\n","#         gr.Markdown(\"Report generated automatically from the audio-derived dialogue.\")\n","#         report_html = gr.HTML(value=\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>_No report yet._</div></div></div>\")\n","\n","#     with gr.Tab(\"JSON\"):\n","#         json_out = gr.Code(language=\"json\", label=\"Output JSON\")\n","\n","#     with gr.Tab(\"Logs\"):\n","#         logs_out = gr.Textbox(label=\"Processing Logs\", lines=18)\n","#         dialogue_inline = gr.Markdown(value=\"_Dialogue will appear here_\")\n","\n","#     # Single callback returns EVERYTHING, including the report HTML (no .then chain for report)\n","#     run_btn.click(\n","#         transcribe_and_diarize,\n","#         inputs=[audio, model_name, enable_diar, hf_token],\n","#         outputs=[dialogue_md, json_out, logs_out, latest_dialogue_state, latest_fields_state, report_html]\n","#     ).then(\n","#         lambda md, *_: md, inputs=[dialogue_md], outputs=[dialogue_inline]\n","#     )\n","\n","# if __name__ == \"__main__\":\n","#     gr.close_all()\n","#     demo.launch(share=True)\n"],"metadata":{"id":"AKPuubx4FH4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2lqYvP9MBKLb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZDYTfRAgBKP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kOvempw5BKXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CELPwbNyBKmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#part - 1 for report structure"],"metadata":{"id":"AqJwZybFBK1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# app_gradio.py\n","# ---------------------------------------------------------\n","# Whisper + (optional) Pyannote UI with \"Dialogue\" and \"Report Analysis\"\n","# ---------------------------------------------------------\n","\n","import os, re, json, subprocess\n","from functools import lru_cache\n","from typing import List, Dict, Any\n","from html import escape\n","from datetime import datetime\n","\n","import gradio as gr\n","import torch\n","import whisper\n","from pyannote.audio import Pipeline\n","\n","# -----------------------\n","# Config\n","# -----------------------\n","DEFAULT_MODEL_NAME = os.getenv(\"WHISPER_MODEL\", \"large-v3\")\n","SAMPLE_RATE_HZ = 16000\n","DEFAULT_HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\", \"hf_\")\n","\n","# NEW: default owners for every audit\n","DEFAULT_TL_NAME = \"Shalini\"\n","DEFAULT_QA_NAME = \"Kalaivani\"\n","\n","#agents names list\n","KNOWN_AGENTS = [\n","    \"Jaya Parkash\", \"Chandru\", \"Sneha\", \"Kowsalya\", \"Swathi\",\n","    \"Arogya Marry\", \"Delphina\", \"Aesu Marry\"\n","]\n","\n","SCORE_CHOICES = [\"Yes\", \"No\", \"N/A\"]\n","\n","# -----------------------\n","# Date function for Audit Date\n","# -----------------------\n","def mdy_from_file(path: str) -> str:\n","    try:\n","        ts = os.path.getmtime(path)\n","        dt = datetime.fromtimestamp(ts)\n","        # M/D/YYYY (no leading zeros)\n","        return f\"{dt.month}/{dt.day}/{dt.year}\"\n","    except Exception:\n","        return \"\"\n","\n","# -----------------------\n","# FFmpeg / audio utils\n","# -----------------------\n","def get_audio_duration(audio_path: str) -> float:\n","    try:\n","        out = subprocess.run(\n","            [\"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\"-of\",\"default=noprint_wrappers=1:nokey=1\",audio_path],\n","            capture_output=True, text=True, check=True\n","        )\n","        return float(out.stdout.strip())\n","    except Exception:\n","        return 0.0\n","\n","def _ffmpeg_run(cmd: List[str]) -> bool:\n","    try:\n","        subprocess.run(cmd, check=True, capture_output=True, text=True)\n","        return True\n","    except subprocess.CalledProcessError:\n","        return False\n","\n","def audio_preprocessing_v1(i,o):\n","    return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","                        \"-af\",\"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\"-y\",o])\n","def audio_preprocessing_v2(i,o):\n","    return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","                        \"-af\",\"loudnorm=I=-23:TP=-2,highpass=f=100\",\"-y\",o])\n","def audio_preprocessing_v3(i,o):\n","    return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\n","                        \"-af\",\"loudnorm\",\"-y\",o])\n","def audio_preprocessing_v4(i,o):\n","    return _ffmpeg_run([\"ffmpeg\",\"-i\",i,\"-acodec\",\"pcm_s16le\",\"-ac\",\"1\",\"-ar\",str(SAMPLE_RATE_HZ),\"-y\",o])\n","\n","def smart_audio_preprocessing(input_path, output_path, log: List[str]) -> bool:\n","    original_dur = get_audio_duration(input_path)\n","    log.append(f\"Original audio duration: {original_dur:.2f}s\")\n","    methods = [(\"Advanced (afftdn)\",audio_preprocessing_v1),(\"Simplified\",audio_preprocessing_v2),\n","               (\"Basic\",audio_preprocessing_v3),(\"Minimal\",audio_preprocessing_v4)]\n","    for idx,(name,fn) in enumerate(methods,1):\n","        log.append(f\"→ Trying {name} preprocessing...\")\n","        if fn(input_path, output_path) and os.path.exists(output_path):\n","            processed_dur = get_audio_duration(output_path)\n","            log.append(f\"Processed duration: {processed_dur:.2f}s\")\n","            if abs(original_dur - processed_dur) < 1.0:\n","                log.append(f\"✅ Preprocessing successful with method {idx}: {name}\")\n","                return True\n","            else:\n","                log.append(f\"⚠️ Duration mismatch on method {idx}; trying next...\")\n","    log.append(\"❌ All preprocessing methods failed.\")\n","    return False\n","\n","# -----------------------\n","# Text cleanup / repetition control\n","# -----------------------\n","def post_process_text(text: str) -> str:\n","    if not text: return \"\"\n","    words = text.split()\n","    cleaned_words, i = [], 0\n","    while i < len(words):\n","        cur = words[i].lower(); count = 1; j = i+1\n","        while j < len(words) and words[j].lower() == cur: count += 1; j += 1\n","        cleaned_words.extend([words[i]] * (min(count,2) if count <= 3 else 1)); i += count\n","    text = \" \".join(cleaned_words)\n","\n","    filler = [\"uh\",\"um\",\"mm\",\"hmm\",\"ah\",\"oh\",\"huh\",\"ha ha\",\n","              \"okay okay\",\"yes yes\",\"yes yes yes\",\"i mean\",\"you know\",\"like like\",\"ok ok\"]\n","    for f in filler:\n","        text = re.sub(rf'\\b{re.escape(f)}\\b','',text,flags=re.IGNORECASE)\n","\n","    repl = {'access max life':'Axis Max Life','axis max life':'Axis Max Life','g pay':'GPay',\n","            'google pay':'Google Pay','phone pay':'PhonePe','phone pe':'PhonePe','pay tm':'Paytm',\n","            'net banking':'netbanking','some assured':'sum assured','premium do':'premium due',\n","            'do date':'due date','okay sir':'Okay sir'}\n","    low = text.lower()\n","    for w,c in repl.items(): low = low.replace(w,c)\n","    low = re.sub(r'\\brs[.]?\\s*','₹',low)\n","    low = re.sub(r'\\s{2,}',' ',low)\n","    low = re.sub(r'[,]{2,}',',',low)\n","    low = re.sub(r'\\s+,',',',low)\n","    low = re.sub(r'\\s+\\.','.',low)\n","    low = re.sub(r'\\s+[!?]', lambda m:m.group(0).strip(), low)\n","    low = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m:m.group(1)+m.group(2).upper(), low)\n","    return low.strip()\n","\n","def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3, log=None):\n","    log = log or []; log.append(\"🔍 Aggressive repetition detection started...\")\n","    cleaned = []\n","    for seg in segments:\n","        text = seg[\"text\"].strip(); words = text.split()\n","        if len(words) < 2: continue\n","        wc = {}\n","        for w in words:\n","            wl = w.lower().strip('.,!?'); wc[wl] = wc.get(wl,0)+1\n","        dominance = (max(wc.values())/len(words)) if wc else 0\n","        if dominance > 0.4: log.append(f\"🚫 Drop (dominant word): {text[:50]}...\"); continue\n","\n","        consec=max_consec=0\n","        for j in range(1,len(words)):\n","            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                consec += 1; max_consec = max(max_consec, consec+1)\n","            else: consec = 0\n","        if max_consec > 3: log.append(f\"🚫 Drop (consecutive repeats): {text[:50]}...\"); continue\n","\n","        repetitive=False\n","        for L in range(2, min(len(words)//3+1,8)):\n","            for s in range(len(words)-L*2+1):\n","                if ' '.join(words[s:s+L]).lower() == ' '.join(words[s+L:s+2*L]).lower():\n","                    if (L*2)/len(words) > max_repetition_ratio:\n","                        log.append(f\"🚫 Drop (pattern repeat): {text[:50]}...\"); repetitive=True; break\n","            if repetitive: break\n","        if repetitive: continue\n","\n","        is_dup=False\n","        for prev in cleaned[-5:]:\n","            a,b=set(prev['text'].lower().split()), set(w.lower() for w in words)\n","            if a|b and (len(a&b)/len(a|b))>0.7 and abs(len(a)-len(b))<5:\n","                log.append(f\"🚫 Drop (near-duplicate): {text[:50]}...\"); is_dup=True; break\n","        if is_dup: continue\n","        cleaned.append(seg)\n","    log.append(f\"📊 Cleaned {len(segments)} → {len(cleaned)} segments\")\n","    return cleaned\n","\n","# -----------------------\n","# Whisper + diarization\n","# -----------------------\n","@lru_cache(maxsize=1)\n","def get_whisper_model(name: str):\n","    return whisper.load_model(name)\n","\n","def enhanced_whisper_transcription(audio_path: str, model_name: str, log) -> Dict[str, Any]:\n","    log.append(\"--- Enhanced Whisper Transcription ---\")\n","    agents_str = \", \".join(KNOWN_AGENTS)\n","    prompt = (\n","        f\"This is a customer support call from Axis Maxlife Insurance in Tamil regarding insurance renewal call. \"\n","        f\"Agents are always one of these: {agents_str}. Use exact names like 'Jaya Parkash' or 'Swathi'. \"\n","        f\"Discuss policy numbers, due dates, fund values, sum assured, late fees, and payment methods such as \"\n","        f\"Google Pay, PhonePe, Paytm, netbanking.\"\n","    )\n","    model = get_whisper_model(model_name)\n","    result = model.transcribe(\n","        audio_path, language=\"ta\", task=\"translate\", temperature=0.0,\n","        beam_size=5, patience=1.2, condition_on_previous_text=False,\n","        no_speech_threshold=0.8, compression_ratio_threshold=2.0,\n","        logprob_threshold=-0.35, word_timestamps=False,\n","        initial_prompt=prompt, verbose=True,\n","    )\n","    log.append(\"✅ Whisper transcription done.\")\n","    return result\n","\n","def run_diarization(wav_path: str, hf_token: str, log):\n","    try:\n","        token = hf_token or os.getenv(\"HUGGING_FACE_TOKEN\", \"\")\n","        if not token:\n","            log.append(\"ℹ️ No HF token provided: skipping diarization (Speaker_Unknown).\")\n","            return None\n","        try:\n","            pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=token)\n","        except TypeError:\n","            pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", token=token)\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\")); log.append(\"✅ Using GPU for diarization.\")\n","        diarization = pipeline(wav_path); log.append(\"✅ Speaker diarization completed.\")\n","        return diarization\n","    except Exception as e:\n","        log.append(f\"⚠️ Diarization failed: {e}\"); return None\n","\n","# -----------------------\n","# Dialogue + fields\n","# -----------------------\n","def get_dominant_speaker(start_time, end_time, diarization_result):\n","    if not diarization_result: return \"Speaker_Unknown\"\n","    speakers = {}\n","    for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","        overlap_start = max(start_time, segment.start)\n","        overlap_end = min(end_time, segment.end)\n","        dur = max(0.0, overlap_end - overlap_start)\n","        if dur > 0: speakers[speaker] = speakers.get(speaker, 0.0) + dur\n","    return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","def build_dialogue(processed_segments, diarization):\n","    out=[]; cur_spk=None; cur_texts=[]; cur_start=0.0; cur_end=0.0\n","    for seg in processed_segments:\n","        start,end,txt = seg['start'], seg['end'], seg['text'].strip()\n","        spk = get_dominant_speaker(start,end,diarization)\n","        if (spk==cur_spk and cur_spk and (start-cur_end)<3.0):\n","            cur_texts.append(txt); cur_end=end\n","        else:\n","            if cur_spk and cur_texts:\n","                c=' '.join(cur_texts)\n","                if len(c.strip())>10: out.append({'speaker':cur_spk,'text':c,'start_time':cur_start,'end_time':cur_end})\n","            cur_spk,cur_texts,cur_start,cur_end = spk,[txt],start,end\n","    if cur_spk and cur_texts:\n","        c=' '.join(cur_texts)\n","        if len(c.strip())>10: out.append({'speaker':cur_spk,'text':c,'start_time':cur_start,'end_time':cur_end})\n","    return out\n","\n","def format_dialogue_markdown(dialogue: List[Dict[str, Any]]) -> str:\n","    if not dialogue: return \"_No substantial dialogue detected._\"\n","    lines=[\"## 🎭 Dialogue\",\"\"]\n","    for d in dialogue:\n","        lines.append(f\"**{d['speaker']} [{d['start_time']:.1f}s - {d['end_time']:.1f}s]:**\")\n","        lines.append(d['text']); lines.append(\"\")\n","    return \"\\n\".join(lines)\n","\n","def _norm(t: str) -> str: return re.sub(r\"\\s+\",\" \",t).strip()\n","\n","PLAN_HINT_WORDS = [\"plan\", \"policy\", \"smart\", \"wealth\", \"lump\", \"limited\", \"pay\", \"non\", \"par\", \"ulip\", \"term\", \"life\", \"max\"]\n","KNOWN_COHORTS = {\"FRYP\",\"NRYP\",\"RYP\",\"FY\",\"REN\",\"FR\",\"FRY\",\"FRYP1\",\"FRYP2\"}\n","KNOWN_CHANNELS = {\"UCB\",\"DCB\",\"RBS\",\"INBOUND\",\"OUTBOUND\",\"BRU\",\"BRM\",\"BRL\"}\n","KNOWN_DISPO = {\n","    \"call back\":\"Call Back\", \"callback\":\"Call Back\", \"cb\":\"Call Back\",\n","    \"not reachable\":\"Not Reachable\", \"nr\":\"Not Reachable\",\n","    \"rpc\":\"RPC\", \"cb-rpc\":\"CB-RPC\", \"cb rpc\":\"CB-RPC\",\n","    \"paid\":\"Paid\", \"promise to pay\":\"Promise To Pay\", \"ptp\":\"Promise To Pay\"\n","}\n","KNOWN_SUBDISPO = {\n","    \"cb-rpc\":\"CB-RPC\", \"rpc\":\"RPC\", \"na\":\"NA\", \"not interested\":\"Not Interested\",\n","    \"wrong number\":\"Wrong Number\", \"follow up\":\"Follow Up\", \"payment link\":\"Payment Link Sent\"\n","}\n","\n","AGENT_STOP_RE = r\"(?:from|calling|call|here|with|team|branch|,|\\.|$)\"\n","\n","def find_associate_name(full_txt: str) -> str:\n","    # 1) Prefer an exact match from the known agent list\n","    low = full_txt.lower()\n","    for nm in KNOWN_AGENTS:\n","        if nm.lower() in low:\n","            return nm  # keep canonical casing from the list\n","\n","    # 2) Natural phrases like \"my name is <name>\" / \"this is <name>\" / \"I am <name>\"\n","    m = re.search(\n","        rf\"(?:my\\s+name\\s+is|this\\s+is|i\\s*am|speaking)\\s+([A-Za-z][A-Za-z.\\s]{{1,30}}?)(?=\\s+{AGENT_STOP_RE})\",\n","        full_txt, flags=re.I\n","    )\n","    if m:\n","        return _norm(m.group(1).title())\n","\n","    # 3) \"Associate/Agent name: <name>\"\n","    m = re.search(r\"(?:associate|agent)\\s*name\\s*[:\\-]?\\s*([A-Za-z][A-Za-z.\\s]{1,30})\", full_txt, flags=re.I)\n","    if m:\n","        return _norm(m.group(1).title())\n","\n","    return \"—\"\n","\n","\n","\n","def extract_fields_from_dialogue(dialogue):\n","    full_txt = \" \".join(_norm(d[\"text\"]) for d in dialogue)\n","    full = full_txt.lower()\n","    g = {}\n","\n","    # Policy Number (allow \"policy number is 149607129\")\n","    m = re.search(r\"policy\\s*(?:no|number)\\s*(?:is|:|-)?\\s*(\\d{6,16})\", full)\n","    if not m:\n","        # fallback: \"... number 149607129 for your policy\"\n","        m = re.search(r\"\\b(\\d{6,16})\\b(?=.*\\bpolicy\\s*(?:no|number)\\b)\", full)\n","    g[\"Policy No\"] = m.group(1) if m else \"—\"\n","\n","    # Associate (agent) name\n","    g[\"Associate Name\"] = find_associate_name(full_txt)\n","\n","    # Customer name (Mr./Mrs./Ms. …)\n","    m = re.search(r\"(?:mr|mrs|ms)\\.?\\s+([a-zA-Z][a-zA-Z.\\s]{2,30})\", full)\n","    g[\"Customer Name\"] = m.group(1).title().strip() if m else \"—\"\n","\n","    # Mobile\n","    m = re.search(r\"(?:mobile|phone|contact)\\s*(?:no|number)?\\s*[:\\-]?\\s*(\\+?91[\\s\\-]?)?(\\d{10})\", full)\n","    g[\"Mobile Number\"] = ((\"+91 \" if m and m.group(1) else \"\") + (m.group(2) if m else \"—\")) if m else \"—\"\n","\n","    # Due date\n","    m = re.search(r\"(?:due\\s*date|premium\\s*due)\\s*[:\\-]?\\s*([0-3]?\\d[\\/\\-][01]?\\d(?:[\\/\\-]\\d{2,4})?|[0-3]?\\d\\s+[a-zA-Z]{3,9}\\s+\\d{4})\", full)\n","    g[\"Due Date\"] = m.group(1) if m else \"—\"\n","\n","    # Premium / Sum Assured / Fund\n","    m = re.search(r\"(?:premium\\s*(?:amount|due)?\\s*[:\\-]?\\s*)(?:₹|rs\\.?\\s*)\\s*([0-9,]+(?:\\.\\d{1,2})?)\", full)\n","    g[\"Premium Amount\"] = \"₹\"+m.group(1) if m else \"—\"\n","\n","    m = re.search(r\"(?:sum\\s*assured)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+)\", full)\n","    g[\"Sum Assured\"] = \"₹\"+m.group(1) if m else \"—\"\n","\n","    m = re.search(r\"(?:fund\\s*value)\\s*[:\\-]?\\s*(?:₹|rs\\.?\\s*)?\\s*([0-9,]+(?:\\.\\d{1,2})?)\", full)\n","    g[\"Fund Value\"] = \"₹\"+m.group(1) if m else \"—\"\n","\n","    # Policy Status / Payment Mode\n","    m = re.search(r\"(?:policy\\s*status)\\s*[:\\-]?\\s*(active|inactive|lapsed|in force|paid up)\", full)\n","    g[\"Policy Status\"] = m.group(1).title() if m else \"—\"\n","\n","    m = re.search(r\"(?:payment\\s*(?:mode|method))\\s*[:\\-]?\\s*(cash|card|netbanking|upi|google\\s*pay|gpay|phonepe|paytm)\", full)\n","    g[\"Payment Mode\"] = (m.group(1).title().replace(\"Gpay\",\"GPay\").replace(\"Google Pay\",\"Google Pay\").replace(\"Phonepe\",\"PhonePe\")) if m else \"—\"\n","\n","    # --- Plan Name (same robust logic you had) ---\n","    m = re.search(r\"(?:plan\\s*name|policy\\s*plan)\\s*[:\\-]?\\s*([A-Za-z0-9 ,&\\-/]+plan(?:\\s*\\([^)]+\\))?)\", full_txt, flags=re.I)\n","    plan = m.group(1).strip() if m else \"\"\n","    if not plan:\n","        m = re.search(r\"(Max\\s*Life[^\\n,;.]{6,120}?Plan(?:\\s*\\([^)]+\\))?)\", full_txt, flags=re.I)\n","        if m: plan = _norm(m.group(1))\n","    if not plan:\n","        m = re.search(r\"([A-Za-z][A-Za-z0-9 ,&\\-/]{8,120}?plan(?:\\s*\\([^)]+\\))?)\", full_txt, flags=re.I)\n","        if m and any(w in m.group(1).lower() for w in PLAN_HINT_WORDS):\n","            plan = _norm(m.group(1))\n","    g[\"Plan Name\"] = plan if plan else \"—\"\n","\n","    # --- Cohort / Channel / Disposition / Sub-Disposition ---\n","    up = full_txt.upper()\n","    g[\"Cohort\"] = next((c for c in KNOWN_COHORTS if re.search(rf\"\\b{re.escape(c)}\\b\", up)), \"—\")\n","    g[\"Channel Name\"] = next((c for c in KNOWN_CHANNELS if re.search(rf\"\\b{re.escape(c)}\\b\", up)), \"—\")\n","\n","    dispo = \"—\"\n","    for k,v in KNOWN_DISPO.items():\n","        if re.search(rf\"\\b{k}\\b\", full):\n","            dispo = v; break\n","    g[\"Disposition\"] = dispo\n","\n","    subd = \"—\"\n","    for k,v in KNOWN_SUBDISPO.items():\n","        if re.search(rf\"\\b{k}\\b\", full):\n","            subd = v; break\n","    g[\"Sub-Disposition\"] = subd\n","\n","    return g\n","\n","# -----------------------\n","# Scorecard (QA) — spec & HTML\n","# -----------------------\n","QUALITY_ITEMS = [\n","    # Opening a Call\n","    {\"group\":\"Opening a Call\",\"sn\":1, \"text\":\"Greeting, Self-introduction, Branding & Opening.\", \"fatality\":\"Non Fatal\", \"weight\":3},\n","    {\"group\":\"Opening a Call\",\"sn\":2, \"text\":\"Ask for customer's availability to start the conversation.\", \"fatality\":\"Non Fatal\", \"weight\":2},\n","\n","    # Customer Service & Assistance\n","    {\"group\":\"Customer Service & Assistance\",\"sn\":3, \"text\":\"Linkage of current communication with last recorded conversation/disposition code with customer.\", \"fatality\":\"Non Fatal\", \"weight\":5},\n","    {\"group\":\"Customer Service & Assistance\",\"sn\":4, \"text\":\"Understanding/probing customer's need to provide suitable response for collection & retention.\", \"fatality\":\"Non Fatal\", \"weight\":6},\n","    {\"group\":\"Customer Service & Assistance\",\"sn\":5, \"text\":\"Hold and Mute procedure and dead air.\", \"fatality\":\"Non Fatal\", \"weight\":6},\n","    {\"group\":\"Customer Service & Assistance\",\"sn\":6, \"text\":\"Ask if they need any assistance in making the payment & advise on appropriate payment option / ECS Registration / Transaction Status, Draw Date & Hit Date (if applicable).\", \"fatality\":\"Fatal\", \"weight\":6},\n","\n","    # Retention & Knowledge\n","    {\"group\":\"Retention & Knowledge\",\"sn\":7, \"text\":\"Pitch product features (USPs) and persuade the customer to create interest.\", \"fatality\":\"Non Fatal\", \"weight\":6},\n","    {\"group\":\"Retention & Knowledge\",\"sn\":8, \"text\":\"Rebuttals for RTP / deferred payment / misselling / low fund value / financial constraint / Payment & Policy Term.\", \"fatality\":\"Fatal\", \"weight\":10},\n","\n","    # Compliance\n","    {\"group\":\"Compliance\",\"sn\":9, \"text\":\"Inform correct C2P / late fee & due date / Sum assured / maturity & surrender amount.\", \"fatality\":\"Non Fatal\", \"weight\":10},\n","    {\"group\":\"Compliance\",\"sn\":10, \"text\":\"Should not disclose customer's personal information on call to third party.\", \"fatality\":\"Fatal\", \"weight\":10},\n","\n","    # Soft Skill & Communication\n","    {\"group\":\"Soft Skill & Communication\",\"sn\":11, \"text\":\"Active listening & quick response.\", \"fatality\":\"Non Fatal\", \"weight\":6},\n","    {\"group\":\"Soft Skill & Communication\",\"sn\":12, \"text\":\"No rude / negative statement on the call.\", \"fatality\":\"Fatal\", \"weight\":10},\n","    {\"group\":\"Soft Skill & Communication\",\"sn\":13, \"text\":\"Soft skill.\", \"fatality\":\"Non Fatal\", \"weight\":4},\n","\n","    # Call Closing\n","    {\"group\":\"Call Closing\",\"sn\":14, \"text\":\"Call closing as per script.\", \"fatality\":\"Non Fatal\", \"weight\":4},\n","\n","    # Calling Disposition / Data Enrichment\n","    {\"group\":\"Calling Disposition / Data Enrichment\",\"sn\":15, \"text\":\"Alter number and Email ID (if any) and correct follow-up dates.\", \"fatality\":\"Non Fatal\", \"weight\":2},\n","    {\"group\":\"Calling Disposition / Data Enrichment\",\"sn\":16, \"text\":\"Dispose the call correct/Short Codes & ensure no repeat calls/unnecessary callback.\", \"fatality\":\"Non Fatal\", \"weight\":10},\n","\n","]\n","\n","def _build_scorecard_html():\n","    # Build rows with group rowspans\n","    from collections import defaultdict\n","    group_counts = defaultdict(int)\n","    for item in QUALITY_ITEMS:\n","        group_counts[item[\"group\"]] += 1\n","\n","    weight_total = sum(item[\"weight\"] for item in QUALITY_ITEMS)\n","    # If later you set item[\"score\"]=\"Yes\"/\"No\", these totals will auto-calc:\n","    scored_total = 0\n","    fatal_errors = 0\n","    for item in QUALITY_ITEMS:\n","        score = item.get(\"score\", \"—\")  # \"Yes\"/\"No\"/\"—\"\n","        if score == \"Yes\":\n","            scored_total += item[\"weight\"]\n","        elif score == \"No\" and item[\"fatality\"].lower() == \"fatal\":\n","            fatal_errors += 1\n","\n","    final_percent = f\"{round((scored_total/weight_total)*100)}%\" if weight_total else \"—\"\n","\n","    # Styles for scorecard (shares palette with your sheet)\n","    style = \"\"\"\n","    <style>\n","      .qa-card{margin-top:14px}\n","      .qa-table{width:100%;border-collapse:collapse;font-size:13px}\n","      .qa-table th,.qa-table td{border:1px solid #2a2f3a;padding:8px 10px;vertical-align:top}\n","      .qa-head{background:#0b1220;color:#f5f7fb;text-align:center;font-weight:800;font-size:15px}\n","      .qa-col-h{background:#0c1427;color:#b9c3d4;text-align:left;font-weight:700}\n","      .qa-group{background:#0c1427;color:#b9c3d4;font-weight:700;min-width:180px}\n","      .qa-cell{background:#0c0f16;color:#e7e9ee}\n","      .qa-right{text-align:right}\n","      .qa-center{text-align:center}\n","      .qa-sum{background:#0c0f16;color:#e7e9ee;font-weight:700}\n","    </style>\n","    \"\"\"\n","\n","    # Header\n","    head = \"\"\"\n","    <table class=\"qa-table\">\n","      <tr><th class=\"qa-head\" colspan=\"7\">Call Score</th></tr>\n","      <tr>\n","        <th class=\"qa-col-h\">Parameters</th>\n","        <th class=\"qa-col-h qa-center\">S.no.</th>\n","        <th class=\"qa-col-h\">Sub-Parameters</th>\n","        <th class=\"qa-col-h\">Fatality</th>\n","        <th class=\"qa-col-h qa-center\">Score</th>\n","        <th class=\"qa-col-h qa-right\">Weightage</th>\n","        <th class=\"qa-col-h qa-right\">Final Score</th>\n","      </tr>\n","    \"\"\"\n","\n","    # Body rows with rowspans for group\n","    body = []\n","    seen = defaultdict(int)\n","    for item in QUALITY_ITEMS:\n","        group = item[\"group\"]\n","        sn = item[\"sn\"]\n","        text = item[\"text\"]\n","        fatal = item[\"fatality\"]\n","        score = item.get(\"score\", \"—\")\n","        weight = item[\"weight\"]\n","        final = (weight if score == \"Yes\" else 0) if score in (\"Yes\", \"No\") else \"—\"\n","\n","        tds = []\n","        if seen[group] == 0:\n","            rowspan = group_counts[group]\n","            tds.append(f'<td class=\"qa-group\" rowspan=\"{rowspan}\">{escape(group)}</td>')\n","        seen[group] += 1\n","\n","        tds.extend([\n","            f'<td class=\"qa-cell qa-center\">{sn}</td>',\n","            f'<td class=\"qa-cell\">{escape(text)}</td>',\n","            f'<td class=\"qa-cell\">{escape(fatal)}</td>',\n","            f'<td class=\"qa-cell qa-center\">{escape(str(score))}</td>',\n","            f'<td class=\"qa-cell qa-right\">{weight}</td>',\n","            f'<td class=\"qa-cell qa-right\">{final}</td>',\n","        ])\n","        body.append(\"<tr>\" + \"\".join(tds) + \"</tr>\")\n","\n","    # Summary rows\n","    summary = f\"\"\"\n","      <tr>\n","        <td class=\"qa-col-h\" colspan=\"5\">Count of Fatal Error</td>\n","        <td class=\"qa-sum qa-right\" colspan=\"2\">{fatal_errors}</td>\n","      </tr>\n","      <tr>\n","        <td class=\"qa-col-h\" colspan=\"5\">Total Score including Fatal Error</td>\n","        <td class=\"qa-sum qa-right\">{scored_total}</td>\n","        <td class=\"qa-sum qa-right\">{'100%' if weight_total else '—'}</td>\n","      </tr>\n","      <tr>\n","        <td class=\"qa-col-h\" colspan=\"5\">Final Score</td>\n","        <td class=\"qa-sum qa-right\" colspan=\"2\">{final_percent}</td>\n","      </tr>\n","    </table>\n","    \"\"\"\n","\n","    return f'<div class=\"qa-card\">{style}{head}{\"\".join(body)}{summary}</div>'\n","\n","\n","def _apply_scores_to_items(scores):\n","    # mutate QUALITY_ITEMS in-place with the selected scores\n","    for i, s in enumerate(scores):\n","        if i < len(QUALITY_ITEMS) and s in SCORE_CHOICES:\n","            QUALITY_ITEMS[i][\"score\"] = s\n","\n","def update_scorecard_html(*vals):\n","    # last arg is the fields dict from latest_fields_state (Gradio State)\n","    *scores, fields = vals\n","    _apply_scores_to_items(scores)\n","    return build_report_html(fields, title=\"BRU - CQ Sheet\")\n","\n","# -----------------------\n","# Report (HTML) — auto-rendered\n","# -----------------------\n","def build_report_html(fields: dict, title: str = \"BRU - CQ Sheet\") -> str:\n","    from html import escape\n","\n","    wanted_order = [\n","        \"Policy No\",\n","        \"Associate Name\",   # (agent name)\n","        \"TL Name\",\n","        \"Plan Name\",\n","        \"QA Name\",\n","        \"Audit Date\",\n","        \"Cohort\",\n","        \"Channel Name\",\n","        \"Disposition\",\n","        \"Sub-Disposition\",\n","    ]\n","\n","    def fv(k):\n","        v = fields.get(k, \"—\")\n","        return escape(str(v if v not in (None, \"\", []) else \"—\"))\n","\n","    title = escape(title)\n","\n","    style = \"\"\"\n","    <style>\n","      .vs-wrap{max-width:1000px;margin:10px auto 24px;font-family:ui-sans-serif,system-ui,Segoe UI,Roboto,Helvetica,Arial}\n","      .vs-card{background:#111319;border:1px solid #2a2f3a;border-radius:14px;padding:0;color:#e7e9ee;overflow:hidden}\n","      .sheet{width:100%;border-collapse:collapse;font-size:14px}\n","      .sheet th,.sheet td{border:1px solid #2a2f3a;padding:10px 12px}\n","      .hdr{background:#0b1220;color:#f5f7fb;text-align:left;font-weight:800;font-size:16px;letter-spacing:.2px;padding:10px 12px}\n","      .label{background:#0c1427;color:#b9c3d4;width:34%;font-weight:600;text-align:left}\n","      .value{background:#0c0f16;color:#e7e9ee;text-align:left}\n","      .spacer{height:12px}\n","    </style>\n","    \"\"\"\n","\n","    # 10-field sheet (your existing part)\n","    rows_html = [f\"<tr><th colspan='2' class='hdr'>{title}</th></tr>\"]\n","    for key in wanted_order:\n","        rows_html.append(f\"<tr><td class='label'>{escape(key)}</td><td class='value'>{fv(key)}</td></tr>\")\n","    sheet_html = f\"<table class='sheet'>{''.join(rows_html)}</table>\"\n","\n","    # Append the scorecard block\n","    scorecard_html = _build_scorecard_html()\n","\n","    return f\"\"\"{style}\n","    <div class=\"vs-wrap\">\n","      <div class=\"vs-card\">\n","        {sheet_html}\n","        <div class=\"spacer\"></div>\n","        {scorecard_html}\n","      </div>\n","    </div>\n","    \"\"\"\n","\n","\n","\n","def generate_report_html_from_state(dialogue, fields):\n","    try:\n","        if not dialogue:\n","            return (\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div>\"\n","                    \"<div class='muted'>⚠️ Process an audio first to build the report.</div></div></div>\")\n","        if not fields or not isinstance(fields, dict):\n","            fields = extract_fields_from_dialogue(dialogue)\n","        return build_report_html(fields, title=\"BRU - CQ Sheet\")\n","    except Exception as e:\n","        return f\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Error while building report: {escape(str(e))}</div></div></div>\"\n","\n","# -----------------------\n","# Main callback — returns the report HTML directly (no .then needed)\n","# -----------------------\n","def transcribe_and_diarize(audio_file, model_name, enable_diarization, hf_token):\n","    logs=[]\n","    if audio_file is None:\n","        html = \"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Upload an audio first.</div></div></div>\"\n","        return \"_Please upload an audio file._\", \"{}\", \"No audio provided.\", [], {}, html\n","\n","    input_path = audio_file\n","    cleaned_path = os.path.join(os.path.dirname(input_path), \"cleaned_audio_for_asr_and_diarization.wav\")\n","    # Compute Audit Date from the uploaded file (fallback to cleaned file if needed)\n","    audit_date_str = mdy_from_file(input_path) or mdy_from_file(cleaned_path)\n","\n","    ok = smart_audio_preprocessing(input_path, cleaned_path, logs)\n","    if not ok:\n","        html = \"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Preprocessing failed.</div></div></div>\"\n","        return \"_Preprocessing failed._\", \"{}\", \"\\n\".join(logs), [], {}, html\n","\n","    try:\n","        whisper_result = enhanced_whisper_transcription(cleaned_path, model_name, logs)\n","    except Exception as e:\n","        logs.append(f\"❌ Whisper transcription failed: {e}\")\n","        html = f\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>Transcription failed: {escape(str(e))}</div></div></div>\"\n","        return \"_Transcription failed._\", \"{}\", \"\\n\".join(logs), [], {}, html\n","\n","    segments = (whisper_result.get(\"segments\") or [])\n","    cleaned_segments = detect_and_remove_repetitions(segments, log=logs)\n","\n","    processed_segments = []\n","    for seg in cleaned_segments:\n","        processed_text = post_process_text(seg['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:\n","            seg_copy = dict(seg); seg_copy['text'] = processed_text\n","            processed_segments.append(seg_copy)\n","\n","    diarization = run_diarization(cleaned_path, hf_token, logs) if enable_diarization else None\n","\n","    dialogue = build_dialogue(processed_segments, diarization)\n","    fields   = extract_fields_from_dialogue(dialogue)\n","\n","    # Ensure required defaults are present\n","    fields.setdefault(\"TL Name\", DEFAULT_TL_NAME)\n","    fields.setdefault(\"QA Name\", DEFAULT_QA_NAME)\n","    fields[\"Audit Date\"] = audit_date_str or fields.get(\"Audit Date\", \"—\")\n","\n","    md = format_dialogue_markdown(dialogue)\n","    payload = {\n","        \"metadata\": {\n","            \"total_duration\": whisper_result.get('duration', 0),\n","            \"total_speakers\": len(set(d['speaker'] for d in dialogue)),\n","            \"total_segments\": len(dialogue),\n","            \"model_used\": f\"whisper-{model_name}\",\n","            \"processing_successful\": True,\n","            \"anti_repetition_applied\": True,\n","            \"diarization_enabled\": bool(enable_diarization),\n","        },\n","        \"dialogue\": dialogue,\n","        \"fields\": fields,\n","    }\n","\n","    # Build the report HTML HERE and return it as an output\n","    report_html = generate_report_html_from_state(dialogue, fields)\n","    return md, json.dumps(payload, indent=2, ensure_ascii=False), \"\\n\".join(logs), dialogue, fields, report_html\n","\n","# -----------------------\n","# UI\n","# -----------------------\n","latest_dialogue_state = gr.State([])\n","latest_fields_state  = gr.State({})\n","\n","with gr.Blocks(title=\"Speech Analysis - QA\") as demo:\n","    gr.Markdown(\"# 🗣️ Whisper Call Transcriber → 📑 Report\\nUpload a call recording, get a clean dialogue and a report like your reference sheet.\")\n","\n","    with gr.Row():\n","        audio = gr.Audio(type=\"filepath\", label=\"Upload audio (wav/mp3/m4a...)\")\n","    with gr.Row():\n","        model_name = gr.Dropdown(choices=[\"large-v3\",\"large\",\"medium\",\"small\",\"base\",\"tiny\"],\n","                                 value=DEFAULT_MODEL_NAME, label=\"Whisper model\")\n","        enable_diar = gr.Checkbox(value=True, label=\"Enable diarization (Pyannote)\")\n","        hf_token = gr.Textbox(value=DEFAULT_HF_TOKEN,\n","                              label=\"Hugging Face token (required if diarization enabled)\",\n","                              type=\"password\", placeholder=\"hf_xxx...\")\n","\n","    run_btn = gr.Button(\"▶️ Process\")\n","\n","    with gr.Tab(\"Dialogue\"):\n","        dialogue_md = gr.Markdown()\n","\n","    with gr.Tab(\"Report Analysis\"):\n","        gr.Markdown(\"Report generated automatically from the audio-derived dialogue.\")\n","\n","        # --- NEW: scoring controls (dropdowns) ---\n","        with gr.Accordion(\"Set Scores\", open=False):\n","            score_dds = []\n","            # lay them out 4 per row\n","            for start in range(0, len(QUALITY_ITEMS), 4):\n","                with gr.Row():\n","                    for item in QUALITY_ITEMS[start:start+4]:\n","                        label = f\"{item['sn']}. {item['group']} – {item['text'][:28]}...\"\n","                        dd = gr.Dropdown(\n","                            choices=SCORE_CHOICES, value=\"N/A\", label=label, interactive=True\n","                        )\n","                        score_dds.append(dd)\n","\n","        # existing HTML (unchanged)\n","        report_html = gr.HTML(\n","            value=\"<div class='vs-wrap'><div class='vs-card'><div class='vs-title'>BRU - CQ Sheet</div><div class='muted'>_No report yet._</div></div></div>\"\n","        )\n","\n","    # Wire every dropdown to refresh the scorecard HTML when changed\n","    gr.on(\n","        [dd.change for dd in score_dds],\n","        fn=update_scorecard_html,\n","        inputs=score_dds + [latest_fields_state],\n","        outputs=report_html,\n","    )\n","\n","    with gr.Tab(\"JSON\"):\n","        json_out = gr.Code(language=\"json\", label=\"Output JSON\")\n","\n","    with gr.Tab(\"Logs\"):\n","        logs_out = gr.Textbox(label=\"Processing Logs\", lines=18)\n","        dialogue_inline = gr.Markdown(value=\"_Dialogue will appear here_\")\n","\n","    # Single callback returns EVERYTHING, including the report HTML (no .then chain for report)\n","    run_btn.click(\n","        transcribe_and_diarize,\n","        inputs=[audio, model_name, enable_diar, hf_token],\n","        outputs=[dialogue_md, json_out, logs_out, latest_dialogue_state, latest_fields_state, report_html]\n","    ).then(\n","        update_scorecard_html,\n","        inputs=score_dds + [latest_fields_state],\n","        outputs=report_html\n","    ).then(\n","        lambda md, *_: md, inputs=[dialogue_md], outputs=[dialogue_inline]\n","    )\n","\n","if __name__ == \"__main__\":\n","    gr.close_all()\n","    demo.launch(share=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":648},"id":"xA6WId71BQ0I","executionInfo":{"status":"ok","timestamp":1757046400758,"user_tz":-330,"elapsed":30422,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"65b395af-b941-4257-bb48-15141fcfc7bb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  torchaudio.list_audio_backends()\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://2d3f5f6fbcd981b30d.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://2d3f5f6fbcd981b30d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]}]}