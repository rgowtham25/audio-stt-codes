{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":107629,"status":"ok","timestamp":1753435641873,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"APEFy1kWOh5S","outputId":"dd3a58c9-5615-4b9c-e8ae-e9c4552ebc1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-iz71cfms\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-iz71cfms\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.7.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=6f2646214d704afc4600fd8c876d6efc6da3e6710a211ba3fd12ae9515c9bc00\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-dkbhx4n2/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":17949,"status":"ok","timestamp":1753435659844,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ZZ-jXf6mOqp0","outputId":"9996f3d2-d0f5-47ec-dba8-d6325a5bcece"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.33.4)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.7.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=80a93361ff3b480ea215dfda55699747bb1a2c0b188184d85ab8bbfadb480634\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=47d3ddc8be8a37aa5f2ad8a8a9cf991a47a69632bb155aa583955d8a0efca993\n","  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.14.3 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.0\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18p3Psr_Or2M"},"outputs":[],"source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocess\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk9ilZoQRlCr"},"outputs":[],"source":["# Configuration\n","INPUT_AUDIO_PATH = \"call3.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89057,"status":"ok","timestamp":1753435774031,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"bqm2KjivRfeD","outputId":"f3bd364c-56f0-4869-d1a1-7f4e05807b4e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████| 2.88G/2.88G [00:50<00:00, 60.9MiB/s]\n"]}],"source":["model = whisper.load_model(\"large\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":71,"status":"ok","timestamp":1753447376334,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"N4-pUEoiRfeD","outputId":"782839f8-a6c6-44a2-976a-a8fe9d1530e3"},"outputs":[{"data":{"text/plain":["Whisper(\n","  (encoder): AudioEncoder(\n","    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (decoder): TextDecoder(\n","    (token_embedding): Embedding(51866, 1280)\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (cross_attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XecuZI1O5xs"},"outputs":[],"source":["def get_audio_duration(audio_path):\n","    \"\"\"Get audio duration using ffprobe\"\"\"\n","    try:\n","        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","        return float(result.stdout.strip())\n","    except Exception as e:\n","        print(f\"Could not get duration: {e}\")\n","        return 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chymACVjO50S"},"outputs":[],"source":["def audio_preprocessing_v1(input_path, output_path):\n","    \"\"\"Advanced audio preprocessing with better parameters\"\"\"\n","    print(\"--- Trying Advanced Audio Preprocessing ---\")\n","\n","    # Improved ffmpeg command - less aggressive filtering to preserve speech\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",  # Mono\n","        \"-ar\", \"16000\",  # 16kHz sample rate\n","        \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Advanced preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Advanced preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v2(input_path, output_path):\n","    \"\"\"Simplified but effective preprocessing\"\"\"\n","    print(\"--- Trying Simplified Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Simplified preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Simplified preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v3(input_path, output_path):\n","    \"\"\"Basic but reliable preprocessing\"\"\"\n","    print(\"--- Trying Basic Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Basic preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Basic preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v4(input_path, output_path):\n","    \"\"\"Minimal processing - just format conversion\"\"\"\n","    print(\"--- Trying Minimal Audio Processing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Minimal processing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Minimal processing failed: {e.returncode}\")\n","        return False\n","\n","def smart_audio_preprocessing(input_path, output_path):\n","    \"\"\"Try different preprocessing methods in order of preference\"\"\"\n","    original_duration = get_audio_duration(input_path)\n","    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n","\n","    methods = [\n","        audio_preprocessing_v1,\n","        audio_preprocessing_v2,\n","        audio_preprocessing_v3,\n","        audio_preprocessing_v4\n","    ]\n","\n","    for i, method in enumerate(methods, 1):\n","        if method(input_path, output_path):\n","            if os.path.exists(output_path):\n","                processed_duration = get_audio_duration(output_path)\n","                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n","\n","                if abs(original_duration - processed_duration) < 1.0:\n","                    print(f\"✅ Audio preprocessing successful with method {i}\")\n","                    return True\n","                else:\n","                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n","                    continue\n","\n","    print(\"❌ All preprocessing methods failed!\")\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RepgUy7BO55M"},"outputs":[],"source":["# def enhanced_whisper_transcription(audio_path):\n","#     \"\"\"\n","#     Enhanced Whisper transcription with optimal anti-repetition parameters\n","#     \"\"\"\n","#     print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","\n","#     initial_prompt1 = (\n","#         \"Axis Maxlife Insurance, Policy number, fund value, Due date,\"\n","#         \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking,\"\n","#     )\n","#     # whisper_result = model.transcribe(clean_audio_path, task=\"translate\", verbose=True) # or task=\"transcribe\"\n","#     # initial_prompt1=(\n","#     #     \"Maxlife Insurance call, Policy number 600603260, due date 30th September 2021, amount Rs 1,20,000, \"\n","#     #     \"status discontinued, payment, Google Pay, PhonePe, financial problem, fund value, locking period, \"\n","#     #     \"Axis Maxlife, online payment, partial withdrawal, premium, 5 years, 10 years.\"\n","#     # )\n","\n","#     # Single optimal strategy - no need for multiple attempts\n","#     result = model.transcribe(\n","#         audio_path,\n","#         # language=\"ta\",                      # Tamil\n","#         task=\"translate\",                   # Translate to English\n","#         verbose=True,                       # Keep verbose for monitoring\n","\n","#         # temperature=[0.0],\n","#         # beam_size=5,\n","#         # best_of=5,\n","#         # temperature=[0.2, 0.4],\n","#         # beam_size=1,\n","#         # best_of=1,\n","#         # logprob_threshold=-1.0,\n","#         # compression_ratio_threshold=2.4,\n","#         # no_speech_threshold=0.3,\n","#         # condition_on_previous_text=False,\n","#         # # initial_prompt=None,\n","#         # word_timestamps=False,\n","#     )\n","\n","#     print(\"✅ Whisper transcription completed with optimal parameters\")\n","#     return result\n","\n","# def calculate_repetition_score(segments):\n","#     \"\"\"\n","#     Calculate a repetition score for transcription segments\n","#     Lower score = less repetition = better\n","#     \"\"\"\n","#     if not segments:\n","#         return 0.0\n","\n","#     total_repetition = 0\n","#     total_words = 0\n","\n","#     for segment in segments:\n","#         text = segment.get('text', '').strip().lower()\n","#         words = text.split()\n","\n","#         if len(words) < 2:\n","#             continue\n","\n","#         total_words += len(words)\n","\n","#         # Count immediate word repetitions\n","#         for i in range(len(words) - 1):\n","#             if words[i] == words[i + 1]:\n","#                 total_repetition += 1\n","\n","#         # Count phrase repetitions within segment\n","#         for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n","#             for start in range(len(words) - phrase_len * 2 + 1):\n","#                 phrase1 = ' '.join(words[start:start + phrase_len])\n","#                 phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n","#                 if phrase1 == phrase2:\n","#                     total_repetition += phrase_len * 2  # Heavy penalty\n","\n","#     return total_repetition / max(total_words, 1)\n","\n","# def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n","#     \"\"\"\n","#     AGGRESSIVE post-processing function to detect and remove repetitive segments\n","#     \"\"\"\n","#     print(\"🔍 Starting aggressive repetition detection...\")\n","#     cleaned_segments = []\n","\n","#     for i, segment in enumerate(segments):\n","#         text = segment['text'].strip()\n","#         words = text.split()\n","\n","#         # Skip very short segments\n","#         if len(words) < 2:\n","#             continue\n","\n","#         # AGGRESSIVE: Check for excessive word repetition\n","#         is_repetitive = False\n","\n","#         # Count word frequencies\n","#         word_counts = {}\n","#         for word in words:\n","#             word_lower = word.lower().strip('.,!?')\n","#             word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","#         # Check if any single word dominates the segment\n","#         max_word_count = max(word_counts.values()) if word_counts else 0\n","#         word_dominance = max_word_count / len(words) if words else 0\n","\n","#         if word_dominance > 0.4:  # If any word is >40% of the segment\n","#             print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n","#             continue\n","\n","#         # Check for immediate repetitions (same word repeated consecutively)\n","#         consecutive_repeats = 0\n","#         max_consecutive = 0\n","\n","#         for j in range(1, len(words)):\n","#             if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","#                 consecutive_repeats += 1\n","#                 max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","#             else:\n","#                 consecutive_repeats = 0\n","\n","#         if max_consecutive > 3:  # More than 3 consecutive identical words\n","#             print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n","#             continue\n","\n","#         # Check for pattern repetitions within segment\n","#         for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","#             for start in range(len(words) - phrase_len * 2 + 1):\n","#                 phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n","#                 phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n","\n","#                 if phrase1 == phrase2:\n","#                     repetition_coverage = (phrase_len * 2) / len(words)\n","#                     if repetition_coverage > max_repetition_ratio:\n","#                         print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n","#                         is_repetitive = True\n","#                         break\n","#             if is_repetitive:\n","#                 break\n","\n","#         if is_repetitive:\n","#             continue\n","\n","#         # Check for similarity with recent segments (avoid near-duplicates)\n","#         is_near_duplicate = False\n","#         for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n","#             prev_words = prev_segment['text'].lower().split()\n","#             current_words = [w.lower() for w in words]\n","\n","#             if prev_words and current_words:\n","#                 # Calculate Jaccard similarity\n","#                 prev_set = set(prev_words)\n","#                 current_set = set(current_words)\n","#                 intersection = len(prev_set.intersection(current_set))\n","#                 union = len(prev_set.union(current_set))\n","\n","#                 similarity = intersection / union if union > 0 else 0\n","\n","#                 if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n","#                     print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n","#                     is_near_duplicate = True\n","#                     break\n","\n","#         if is_near_duplicate:\n","#             continue\n","\n","#         # If we reach here, the segment passed all checks\n","#         cleaned_segments.append(segment)\n","\n","#     removed_count = len(segments) - len(cleaned_segments)\n","#     print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","#     print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n","\n","#     return cleaned_segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPZYyKNxO575"},"outputs":[],"source":["def post_process_text(text):\n","    \"\"\"Clean up transcribed text\"\"\"\n","    if not text:\n","        return \"\"\n","\n","    # Remove excessive repetitions within text\n","    words = text.split()\n","    cleaned_words = []\n","\n","    i = 0\n","    while i < len(words):\n","        current_word = words[i].lower()\n","\n","        # Look for immediate repetitions (same word repeated 3+ times)\n","        repetition_count = 1\n","        j = i + 1\n","        while j < len(words) and words[j].lower() == current_word:\n","            repetition_count += 1\n","            j += 1\n","\n","        # Keep only 1-2 repetitions maximum\n","        keep_count = min(repetition_count, 2) if repetition_count <= 3 else 1\n","        for _ in range(keep_count):\n","            cleaned_words.append(words[i])\n","\n","        i = i + repetition_count\n","\n","    text = ' '.join(cleaned_words)\n","\n","    # Common corrections for Indian insurance context\n","    corrections = {\n","        'access max life': 'Axis Max Life',\n","        'axis max life': 'Axis Max Life',\n","        'g pay': 'GPay',\n","        'google pay': 'Google Pay',\n","        'phone pay': 'PhonePe',\n","        'phone pe': 'PhonePe',\n","        'pay tm': 'Paytm',\n","        'net banking': 'netbanking',\n","        'some assured': 'sum assured',\n","        'premium do': 'premium due',\n","        'do date': 'due date',\n","    }\n","\n","    text_lower = text.lower()\n","    for wrong, correct in corrections.items():\n","        text_lower = text_lower.replace(wrong, correct)\n","\n","    # Capitalize first letter of sentences\n","    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n","                       lambda m: m.group(1) + m.group(2).upper(), text_lower)\n","\n","    return text_lower.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RY9MBqKZO59P"},"outputs":[],"source":["# def main():\n","#     \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","#     print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","#     print(\"=\" * 60)\n","\n","#     # Step 1: Smart Audio Preprocessing\n","#     if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","#         print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","#         return\n","\n","#     # model = whisper.load_model(\"large\")\n","\n","#     # Step 2: Enhanced Whisper Transcription with anti-repetition\n","#     try:\n","#         whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","#         print(\"✅ Whisper transcription completed\")\n","#     except Exception as e:\n","#         print(f\"❌ Whisper transcription failed: {e}\")\n","#         return\n","\n","#     # Step 3: Remove repetitive segments BEFORE post-processing\n","#     print(\"\\n--- Removing Repetitive Segments ---\")\n","#     cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","#     # Step 4: Post-process remaining transcription\n","#     processed_segments = []\n","#     for segment in cleaned_segments:\n","#         processed_text = post_process_text(segment['text'])\n","#         if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","#             segment_copy = segment.copy()\n","#             segment_copy['text'] = processed_text\n","#             processed_segments.append(segment_copy)\n","\n","#     whisper_result[\"segments\"] = processed_segments\n","\n","#     # Step 5: Speaker Diarization\n","#     print(\"\\n--- Speaker Diarization ---\")\n","#     try:\n","#         pipeline = Pipeline.from_pretrained(\n","#             \"pyannote/speaker-diarization-3.1\",\n","#             use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","#         )\n","\n","#         if torch.cuda.is_available():\n","#             pipeline.to(torch.device(\"cuda\"))\n","#             print(\"✅ Using GPU for diarization\")\n","\n","#         diarization = pipeline(CLEAN_AUDIO_PATH)\n","#         print(\"✅ Speaker diarization completed\")\n","\n","#     except Exception as e:\n","#         print(f\"⚠️  Speaker diarization failed: {e}\")\n","#         diarization = None\n","\n","#     # Step 6: Generate Enhanced Dialogue\n","#     print(\"\\n--- Generating Dialogue ---\")\n","\n","#     def get_dominant_speaker(start_time, end_time, diarization_result):\n","#         if not diarization_result:\n","#             return \"Speaker_Unknown\"\n","\n","#         speakers = {}\n","#         for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","#             overlap_start = max(start_time, segment.start)\n","#             overlap_end = min(end_time, segment.end)\n","#             overlap_duration = max(0, overlap_end - overlap_start)\n","\n","#             if overlap_duration > 0:\n","#                 speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","#         return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","#     # Combine segments by speaker\n","#     dialogue = []\n","#     current_speaker = None\n","#     current_texts = []\n","#     current_start = 0\n","#     current_end = 0\n","\n","#     for segment in processed_segments:\n","#         start = segment['start']\n","#         end = segment['end']\n","#         text = segment['text'].strip()\n","\n","#         speaker = get_dominant_speaker(start, end, diarization)\n","\n","#         # Merge consecutive segments from same speaker (within 3 seconds)\n","#         if (speaker == current_speaker and\n","#             current_speaker and\n","#             (start - current_end) < 3.0):\n","#             current_texts.append(text)\n","#             current_end = end\n","#         else:\n","#             # Save previous speaker's dialogue\n","#             if current_speaker and current_texts:\n","#                 combined_text = ' '.join(current_texts)\n","#                 # Final check for repetition in combined text\n","#                 if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","#                     dialogue.append({\n","#                         'speaker': current_speaker,\n","#                         'text': combined_text,\n","#                         'start_time': current_start,\n","#                         'end_time': current_end\n","#                     })\n","\n","#             # Start new speaker segment\n","#             current_speaker = speaker\n","#             current_texts = [text]\n","#             current_start = start\n","#             current_end = end\n","\n","#     # Add final segment\n","#     if current_speaker and current_texts:\n","#         combined_text = ' '.join(current_texts)\n","#         if len(combined_text.strip()) > 10:\n","#             dialogue.append({\n","#                 'speaker': current_speaker,\n","#                 'text': combined_text,\n","#                 'start_time': current_start,\n","#                 'end_time': current_end\n","#             })\n","\n","#     # Step 7: Display Results\n","#     print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","#     for entry in dialogue:\n","#         timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","#         print(f\"\\n{entry['speaker']} {timestamp}:\")\n","#         print(f\"  📝 {entry['text']}\")\n","\n","#     # Step 8: Save Results\n","#     output_data = {\n","#         'metadata': {\n","#             'total_duration': whisper_result.get('duration', 0),\n","#             'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","#             'total_segments': len(dialogue),\n","#             'model_used': 'whisper-large',\n","#             'processing_successful': True,\n","#             'anti_repetition_applied': True\n","#         },\n","#         'dialogue': dialogue,\n","#         'raw_transcription': whisper_result\n","#     }\n","\n","#     with open('enhanced_transcription_results.json', 'w', encoding='utf-8') as f:\n","#         json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","#     print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n","#     print(\"✅ Processing completed successfully!\")\n","\n","# if __name__ == \"__main__\":\n","#     main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6YH_X4_3s2k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjNJEfwL3s8b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiBGYz9q3s-6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SO7IaoF63tBx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLz4SAUR3tEq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6_Jl0TC3tHn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gWkFReZ3tK5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aS57Yjh3tzD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aONKDD403t5u"},"outputs":[],"source":["# # Configuration\n","# INPUT_AUDIO_PATH = \"call3.wav\"\n","# CLEAN_AUDIO_PATH = \"test.wav\"\n","# HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jyklf4a3t5v"},"outputs":[],"source":["# def enhanced_whisper_transcription(audio_path):\n","#     \"\"\"\n","#     Enhanced Whisper transcription with optimal anti-repetition parameters\n","#     \"\"\"\n","#     print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","\n","#     prompt = (\n","#         \"Axis Maxlife Insurance, Policy number, fund value, Due date,\"\n","#         \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking,\"\n","#     )\n","#     # whisper_result = model.transcribe(clean_audio_path, task=\"translate\", verbose=True) # or task=\"transcribe\"\n","#     # prompt=(\n","#     #     \"Maxlife Insurance call, Policy number 600603260, due date 30th September 2021, amount Rs 1,20,000, \"\n","#     #     \"status discontinued, payment, Google Pay, PhonePe, financial problem, fund value, locking period, \"\n","#     #     \"Axis Maxlife, online payment, partial withdrawal, premium, 5 years, 10 years.\"\n","#     # )\n","\n","#     # Single optimal strategy - no need for multiple attempts\n","#     result = model.transcribe(\n","#         audio_path,\n","#         language=\"ta\",                      # Tamil\n","#         task=\"translate\",                   # Translate to English\n","#         verbose=True,                       # Keep verbose for monitoring\n","\n","#         temperature=0.0,\n","#         beam_size=3,\n","#         best_of=3,\n","#         patience=1.0,                   # Add patience\n","#         suppress_tokens=[50362, 50363, 50257],\n","#         condition_on_previous_text=False,\n","#         initial_prompt=prompt,\n","#         no_speech_threshold=0.6,\n","#         compression_ratio_threshold=2.4,\n","#         logprob_threshold=-1.0,\n","#         word_timestamps=True,\n","#     )\n","\n","#     print(\"✅ Whisper transcription completed with optimal parameters\")\n","#     return result\n","\n","# def detect_and_remove_repetitions(segments, max_repetition_ratio=0.2):  # Stricter threshold\n","#     \"\"\"\n","#     Optimized repetition detection with better performance\n","#     \"\"\"\n","#     print(\"🔍 Starting optimized repetition detection...\")\n","#     cleaned_segments = []\n","#     seen_texts = set()  # Track exact duplicates\n","#     recent_texts = []   # Track recent similar texts\n","\n","#     for segment in segments:\n","#         text = segment['text'].strip()\n","#         words = text.split()\n","\n","#         # Skip very short or empty segments\n","#         if len(words) < 2:\n","#             continue\n","\n","#         # Skip exact duplicates\n","#         text_normalized = text.lower().strip('.,!?')\n","#         if text_normalized in seen_texts:\n","#             print(f\"🚫 Skipping exact duplicate: {text[:30]}...\")\n","#             continue\n","\n","#         # Quick repetition check within segment\n","#         repetition_score = calculate_quick_repetition_score(words)\n","#         if repetition_score > max_repetition_ratio:\n","#             print(f\"🚫 High repetition score ({repetition_score:.2f}): {text[:30]}...\")\n","#             continue\n","\n","#         # Check against recent segments for similarity\n","#         is_similar = False\n","#         for recent_text in recent_texts[-3:]:  # Check last 3 segments\n","#             if calculate_similarity(text, recent_text) > 0.8:\n","#                 print(f\"🚫 Too similar to recent segment: {text[:30]}...\")\n","#                 is_similar = True\n","#                 break\n","\n","#         if is_similar:\n","#             continue\n","\n","#         # Segment passed all checks\n","#         cleaned_segments.append(segment)\n","#         seen_texts.add(text_normalized)\n","#         recent_texts.append(text)\n","\n","#         # Keep recent_texts manageable\n","#         if len(recent_texts) > 10:\n","#             recent_texts = recent_texts[-10:]\n","\n","#     removed_count = len(segments) - len(cleaned_segments)\n","#     print(f\"📊 Optimized cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","#     print(f\"🗑️ Removed {removed_count} repetitive/problematic segments\")\n","\n","#     return cleaned_segments\n","\n","# def calculate_quick_repetition_score(words):\n","#     \"\"\"Fast repetition score calculation\"\"\"\n","#     if len(words) < 4:\n","#         return 0.0\n","\n","#     # Count consecutive identical words\n","#     consecutive_count = 0\n","#     for i in range(1, len(words)):\n","#         if words[i].lower() == words[i-1].lower():\n","#             consecutive_count += 1\n","\n","#     return consecutive_count / len(words)\n","\n","# def calculate_similarity(text1, text2):\n","#     \"\"\"Calculate text similarity using word overlap\"\"\"\n","#     words1 = set(text1.lower().split())\n","#     words2 = set(text2.lower().split())\n","\n","#     if not words1 or not words2:\n","#         return 0.0\n","\n","#     intersection = len(words1.intersection(words2))\n","#     union = len(words1.union(words2))\n","\n","#     return intersection / union if union > 0 else 0.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["87f6672852654c999d95f04f7572139f","1b26c0af8e8c4ab4a7ef439c808bbacd","3bd1e05b2f41453a81ff4ee8d2ba4f75","1f53e7b1bba2487bb55ba39e80091f14","751e900f19144883b45c20688f998d46","bd3a0664b78a414e9b738eacd0f1c363","88ed8843121047bf8bd27b2e01507961","06c7258dd2394d31b2232c2d9fa557b6","efb72301ca45458fbb61f6303c498645","9fb904b152fa4106b84883582fd39023","d37613a83fbf4ea7a3f34ac084da7a72","fa1c5f371e3b4587b8fdf4540e21e4dc","1ae94703895b4f34b38e9d2bfc1b7ca8","ed630c950b154b66a1aba385f5f8ca65","0391556884d143329aa84ce11c6e185d","6e5640b3c86c47be9d00d805b90d283f","2642909c62f441d4803861b922676eb6","84cbf6a9aee5496b9ff8dda02e682201","0896a0c4c6c246ee81048521f8b7a5f4","76fff81da3b74b0aa5ab3a74f1ebf0bb","fc78ef9427254165bd2cfe98c7a6116c","4b465cfd8a844e2ab7d72795ff852ab7","be20b1a773bf492db08addbdaa379424","744b70da37b04dde96c32e3cbdebc651","a95298f1f5c54f6893150f4f1a8cbf07","cc7156d882de4ca48e083c65fd64c886","5d0105d29e134c1b8dc36736a88525e8","c2ff19cf348844e1ab178f5ddfae8b0a","08b71e1477994dd69ecacfdb94a31f60","9a50f59108184c09a0a728243cb8192a","3ffcede934144fb59e2b15a4a6fcefb4","ea7ebab2f8564f6ba75320b85a057984","f7790812fc0c4ef1a4c5e2b749398257","c561dab6c5384bdb85ed6fee5a648858","09fd20d9dd5743aca17d2ad72f4d4918","f2adefd0942446bd999a6f9d12ee0fcb","86757b15562d45adacbc0768fc3f1138","661f65dda7284b61aa4d410227078045","6cd7e75b2aee4965bfbfa59d1c72ee0a","dd5df57b5d834cd8bcb474d2977d7be9","39d9dfd18a8042d4b7c832adc3de2f73","9b23efc2b31444eb9f0acbc9d31b1f9d","79aea07bf23d46aab96d45f30d1fd2ae","b0d859c96e2d4a9ba2a5199302e2ff41","3a575684b7a546158f69179d3f9f81cf","1d4f965602c3413281ab548ca71d1d5e","779bbeffb48045b0890304a26c928283","a8b727f691884d499c32469dd40f8cb5","b3854fbd8fec458ba546a5737df7b09a","6a0c5153f68b4f23a3c6cc7c27b5cb6a","8ade7c66ed0c49e7a4f7524be66b4db5","f945b0b4db2a4fe894a28f207ca93e52","adbfc1390ce54bce9d8aa5ae09ad3ea1","267ec4f22ea24817afe644f41871a93e","e8b10f712b1b4676b037cca2046248bf"]},"executionInfo":{"elapsed":5252831,"status":"ok","timestamp":1753426278306,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"xW72cTcG3t5x","outputId":"0c494547-3209-443b-ac6d-236ad416ed42"},"outputs":[{"name":"stdout","output_type":"stream","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition v2)\n","============================================================\n","Original audio duration: 140.72 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 140.69 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:182: UserWarning: Word-level timestamps on translations may not be reliable.\n","  warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n"]},{"name":"stdout","output_type":"stream","text":["[00:00.960 --> 00:01.560]  Hello!\n","[00:02.280 --> 00:04.480]  Hello! I am talking to Maxlife Insurance Company.\n","[00:05.540 --> 00:06.240]  What is this?\n","[00:07.500 --> 00:08.480]  Maxlife Insurance.\n","[00:10.000 --> 00:10.620]  Yes, tell me.\n","[00:11.300 --> 00:14.060]  I am talking about Maxlife Insurance Policy. Can you speak for a minute?\n","[00:16.460 --> 00:17.520]  Yes, I will.\n","[00:18.700 --> 00:20.800]  One minute. I will set the date.\n","[00:21.220 --> 00:25.000]  This is Russell B. Madan speaking.\n","[00:25.520 --> 00:26.120]  Yes.\n","[00:26.420 --> 00:29.820]  I am talking about Maxlife Insurance Policy. Can you speak for a minute?\n","[00:30.400 --> 00:38.660]  Our policy number is 142115666. The cost of building is Rs.3,250. The cost of building is Rs.2,500.\n","[00:38.800 --> 00:42.180]  The cost of building is Rs.1,10,800. We have been building for a year.\n","[00:43.680 --> 00:45.680]  When are you going to build this? What are the reasons for building this?\n","[00:47.580 --> 00:49.420]  We are going to build it soon.\n","[00:50.800 --> 00:52.320]  What are the reasons for building this?\n","[00:53.100 --> 00:56.320]  We have a lot of work to do. I will tell you.\n","[00:56.440 --> 00:58.580]  I will tell you. We will build it soon.\n","[00:58.840 --> 00:59.480]  We will build it soon.\n","[01:01.720 --> 01:03.780]  Yes, I have already informed them.\n","[01:04.300 --> 01:05.140]  Yes, yes.\n","[01:05.400 --> 01:06.600]  I have not seen your son yet.\n","[01:07.340 --> 01:08.120]  No, no.\n","[01:08.320 --> 01:10.500]  He called me.\n","[01:10.700 --> 01:11.220]  Ok.\n","[01:12.620 --> 01:14.840]  I told him that I will get you married.\n","[01:14.900 --> 01:16.660]  He said that he will get you married.\n","[01:17.460 --> 01:18.620]  When are you getting married?\n","[01:19.640 --> 01:20.820]  I am not getting married till the end of this month.\n","[01:21.640 --> 01:23.100]  There is a lapse in the policy.\n","[01:23.180 --> 01:25.240]  There is no risk coverage for lapse.\n","[01:25.640 --> 01:27.400]  So, you have to add the lump sum in the last.\n","[01:28.140 --> 01:29.380]  4 years guarantee addition.\n","[01:29.440 --> 01:29.460]  Ok.\n","[01:30.280 --> 01:31.800]  The benefit of this is that you will get a loss.\n","[01:33.660 --> 01:35.420]  Plus, the rate fee charges will be added on a daily basis.\n","[01:36.000 --> 01:37.220]  If the rate fee charges have already been added,\n","[01:38.120 --> 01:40.240]  that Rs.8,638 will be added.\n","[01:41.460 --> 01:42.380]  Now, if you make a delay,\n","[01:43.140 --> 01:45.520]  the rate fee charges will be added more.\n","[01:46.180 --> 01:46.980]  So, you are pending for the next year.\n","[01:51.100 --> 01:51.480]  Priya!\n","[01:51.620 --> 01:51.620]  Yes?\n","[01:51.620 --> 01:52.580]  Yes, we will take care of it.\n","[01:53.060 --> 02:03.640]  Yes, you can take care of it.\n","[02:03.740 --> 02:04.840]  Yes, I will take care of it.\n","[02:04.980 --> 02:06.300]  It will take 2 months to complete.\n","[02:06.600 --> 02:07.460]  It will take 2 months to complete.\n","[02:08.240 --> 02:09.880]  The health declaration form is also pending.\n","[02:10.760 --> 02:13.640]  You will have to take care of it.\n","[02:13.980 --> 02:14.420]  You will have to take care of it.\n","[02:15.220 --> 02:16.560]  Yes, I will take care of it.\n","[02:16.600 --> 02:17.200]  Thank you.\n","[02:17.360 --> 02:17.640]  Thank you.\n","[02:17.640 --> 02:18.580]  Thank you for watching this video.\n","[02:18.660 --> 02:19.200]  Thank you for watching this video.\n","[02:19.520 --> 02:19.880]  Thank you for watching this video.\n","[02:20.240 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.600]  Thank you for watching this video.\n","[02:20.600 --> 02:20.620]  'Intermission'\n","[02:20.620 --> 02:20.620]  'Intermission'\n","[02:20.620 --> 02:20.620]  'Intermission'\n","[02:20.620 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.660 --> 02:20.660]  'Intermission'\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Please Subscribe to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Please Subscribe to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","[02:20.640 --> 02:20.640]  Thank you.\n","[02:20.640 --> 02:20.640]  Welcome to our Channel.\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","📊 Initial segments: 99\n","\n","--- Removing Repetitive Segments (Optimized) ---\n","🔍 Starting optimized repetition detection...\n","🚫 Skipping exact duplicate: I am talking about Maxlife Ins...\n","🚫 Skipping exact duplicate: It will take 2 months to compl...\n","🚫 Skipping exact duplicate: You will have to take care of ...\n","🚫 Skipping exact duplicate: Yes, I will take care of it....\n","🚫 Skipping exact duplicate: Thank you....\n","🚫 Skipping exact duplicate: Thank you for watching this vi...\n","🚫 Skipping exact duplicate: Thank you for watching this vi...\n","🚫 Skipping exact duplicate: Thank you for watching this vi...\n","📊 Optimized cleaning: 99 → 44 segments\n","🗑️ Removed 55 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87f6672852654c999d95f04f7572139f","version_major":2,"version_minor":0},"text/plain":["config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa1c5f371e3b4587b8fdf4540e21e4dc","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be20b1a773bf492db08addbdaa379424","version_major":2,"version_minor":0},"text/plain":["config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c561dab6c5384bdb85ed6fee5a648858","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a575684b7a546158f69179d3f9f81cf","version_major":2,"version_minor":0},"text/plain":["config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"name":"stdout","output_type":"stream","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_00 [2.3s - 4.5s]:\n","  📝 Hello! I am talking to Maxlife Insurance Company.\n","\n","SPEAKER_02 [5.5s - 6.2s]:\n","  📝 What is this?\n","\n","SPEAKER_00 [7.5s - 8.5s]:\n","  📝 Maxlife Insurance.\n","\n","SPEAKER_02 [10.0s - 10.6s]:\n","  📝 Yes, tell me.\n","\n","SPEAKER_00 [11.3s - 14.1s]:\n","  📝 I am talking about Maxlife Insurance Policy. Can you speak for a minute?\n","\n","SPEAKER_02 [16.5s - 17.5s]:\n","  📝 Yes, I will.\n","\n","SPEAKER_00 [18.7s - 25.0s]:\n","  📝 One minute. I will set the date. This is Russell B. Madan speaking.\n","\n","SPEAKER_00 [30.4s - 45.7s]:\n","  📝 Our policy number is 142115666. The cost of building is Rs.3,250. The cost of building is Rs.2,500. The cost of building is Rs.1,10,800. We have been building for a year. When are you going to build this? What are the reasons for building this?\n","\n","SPEAKER_02 [47.6s - 49.4s]:\n","  📝 We are going to build it soon.\n","\n","SPEAKER_00 [50.8s - 52.3s]:\n","  📝 What are the reasons for building this?\n","\n","SPEAKER_02 [53.1s - 59.5s]:\n","  📝 We have a lot of work to do. I will tell you. I will tell you. We will build it soon. We will build it soon.\n","\n","SPEAKER_00 [61.7s - 63.8s]:\n","  📝 Yes, I have already informed them.\n","\n","SPEAKER_00 [65.4s - 68.1s]:\n","  📝 I have not seen your son yet. No, no.\n","\n","SPEAKER_02 [68.3s - 76.7s]:\n","  📝 He called me. I told him that I will get you married. He said that he will get you married.\n","\n","SPEAKER_00 [77.5s - 78.6s]:\n","  📝 When are you getting married?\n","\n","SPEAKER_02 [79.6s - 80.8s]:\n","  📝 I am not getting married till the end of this month.\n","\n","SPEAKER_00 [81.6s - 107.0s]:\n","  📝 There is a lapse in the policy. There is no risk coverage for lapse. So, you have to add the lump sum in the last. 4 years guarantee addition. The benefit of this is that you will get a loss. Plus, the rate fee charges will be added on a daily basis. If the rate fee charges have already been added, that Rs.8,638 will be added. Now, if you make a delay, the rate fee charges will be added more. So, you are pending for the next year.\n","\n","SPEAKER_01 [111.6s - 124.8s]:\n","  📝 Yes, we will take care of it. Yes, you can take care of it. Yes, I will take care of it.\n","\n","SPEAKER_00 [125.0s - 138.6s]:\n","  📝 It will take 2 months to complete. The health declaration form is also pending. You will have to take care of it. Thank you. Thank you for watching this video.\n","\n","💾 Results saved to: enhanced_transcription_results.json\n","✅ Processing completed successfully!\n"]}],"source":["# def main():\n","#     \"\"\"Main processing pipeline with improved anti-repetition\"\"\"\n","#     print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition v2)\")\n","#     print(\"=\" * 60)\n","\n","#     # Step 1: Smart Audio Preprocessing\n","#     if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","#         print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","#         return\n","\n","#     # Step 2: Enhanced Whisper Transcription (now with anti-repetition)\n","#     try:\n","#         whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","#         print(\"✅ Whisper transcription completed\")\n","\n","#         # Check initial quality\n","#         initial_segments = len(whisper_result[\"segments\"])\n","#         print(f\"📊 Initial segments: {initial_segments}\")\n","\n","#     except Exception as e:\n","#         print(f\"❌ Whisper transcription failed: {e}\")\n","#         return\n","\n","#     # Step 3: Optimized repetition removal\n","#     print(\"\\n--- Removing Repetitive Segments (Optimized) ---\")\n","#     cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","#     # Step 4: Optional translation (if you need English output)\n","#     if len(cleaned_segments) > 0:\n","#         # You can add translation here if needed\n","#         # translated_segments = translate_tamil_to_english(cleaned_segments)\n","#         processed_segments = cleaned_segments\n","#     else:\n","#         print(\"⚠️ No segments survived cleaning - using original\")\n","#         processed_segments = whisper_result[\"segments\"]\n","\n","#     # Step 5: Text post-processing\n","#     final_segments = []\n","#     for segment in processed_segments:\n","#         processed_text = post_process_text(segment['text'])\n","#         if processed_text.strip() and len(processed_text.strip()) > 3:\n","#             segment_copy = segment.copy()\n","#             segment_copy['text'] = processed_text\n","#             final_segments.append(segment_copy)\n","\n","#     whisper_result[\"segments\"] = final_segments\n","\n","#     # Step 5: Speaker Diarization\n","#     print(\"\\n--- Speaker Diarization ---\")\n","#     try:\n","#         pipeline = Pipeline.from_pretrained(\n","#             \"pyannote/speaker-diarization-3.1\",\n","#             use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","#         )\n","\n","#         if torch.cuda.is_available():\n","#             pipeline.to(torch.device(\"cuda\"))\n","#             print(\"✅ Using GPU for diarization\")\n","\n","#         diarization = pipeline(CLEAN_AUDIO_PATH)\n","#         print(\"✅ Speaker diarization completed\")\n","\n","#     except Exception as e:\n","#         print(f\"⚠️  Speaker diarization failed: {e}\")\n","#         diarization = None\n","\n","#     # Step 6: Generate Enhanced Dialogue\n","#     print(\"\\n--- Generating Dialogue ---\")\n","\n","#     def get_dominant_speaker(start_time, end_time, diarization_result):\n","#         if not diarization_result:\n","#             return \"Speaker_Unknown\"\n","\n","#         speakers = {}\n","#         for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","#             overlap_start = max(start_time, segment.start)\n","#             overlap_end = min(end_time, segment.end)\n","#             overlap_duration = max(0, overlap_end - overlap_start)\n","\n","#             if overlap_duration > 0:\n","#                 speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","#         return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","#     # Combine segments by speaker\n","#     dialogue = []\n","#     current_speaker = None\n","#     current_texts = []\n","#     current_start = 0\n","#     current_end = 0\n","\n","#     for segment in processed_segments:\n","#         start = segment['start']\n","#         end = segment['end']\n","#         text = segment['text'].strip()\n","\n","#         speaker = get_dominant_speaker(start, end, diarization)\n","\n","#         # Merge consecutive segments from same speaker (within 3 seconds)\n","#         if (speaker == current_speaker and\n","#             current_speaker and\n","#             (start - current_end) < 3.0):\n","#             current_texts.append(text)\n","#             current_end = end\n","#         else:\n","#             # Save previous speaker's dialogue\n","#             if current_speaker and current_texts:\n","#                 combined_text = ' '.join(current_texts)\n","#                 # Final check for repetition in combined text\n","#                 if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","#                     dialogue.append({\n","#                         'speaker': current_speaker,\n","#                         'text': combined_text,\n","#                         'start_time': current_start,\n","#                         'end_time': current_end\n","#                     })\n","\n","#             # Start new speaker segment\n","#             current_speaker = speaker\n","#             current_texts = [text]\n","#             current_start = start\n","#             current_end = end\n","\n","#     # Add final segment\n","#     if current_speaker and current_texts:\n","#         combined_text = ' '.join(current_texts)\n","#         if len(combined_text.strip()) > 10:\n","#             dialogue.append({\n","#                 'speaker': current_speaker,\n","#                 'text': combined_text,\n","#                 'start_time': current_start,\n","#                 'end_time': current_end\n","#             })\n","\n","#     # Step 7: Display Results\n","#     print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","#     for entry in dialogue:\n","#         timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","#         print(f\"\\n{entry['speaker']} {timestamp}:\")\n","#         print(f\"  📝 {entry['text']}\")\n","\n","#     # Step 8: Save Results\n","#     output_data = {\n","#         'metadata': {\n","#             'total_duration': whisper_result.get('duration', 0),\n","#             'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","#             'total_segments': len(dialogue),\n","#             'model_used': 'whisper-large',\n","#             'processing_successful': True,\n","#             'anti_repetition_applied': True\n","#         },\n","#         'dialogue': dialogue,\n","#         'raw_transcription': whisper_result\n","#     }\n","\n","#     with open('test.json', 'w', encoding='utf-8') as f:\n","#         json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","#     print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n","#     print(\"✅ Processing completed successfully!\")\n","\n","# if __name__ == \"__main__\":\n","#     main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__-GvlEXKqj7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohxHcUMVU0BM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ir1GbtYCU0K6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q15QHzTSU0QC"},"outputs":[],"source":["# Configuration\n","# INPUT_AUDIO_PATH = \"call3.wav\"\n","INPUT_AUDIO_PATH = \"001_t2.wav\"\n","CLEAN_AUDIO_PATH = \"clear_audio.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vy-G7YeEU0TU"},"outputs":[],"source":["def enhanced_whisper_transcription(audio_path):\n","    \"\"\"\n","    Enhanced Whisper transcription with optimal anti-repetition parameters\n","    \"\"\"\n","    print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","\n","    # prompt = (\n","    #     \"Axis Maxlife Insurance, Policy number, fund value, Due date,\"\n","    #     \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking,\"\n","    # )\n","\n","    prompt = (\n","        \"This is a customer support call for Axis Maxlife Insurance. \"\n","        \"We will discuss policy numbers, due date, fund value, sum assured, late fee, \"\n","        \"and payment methods such as Google Pay, PhonePe, Paytm and net banking.\"\n","    )\n","\n","    # Single optimal strategy - no need for multiple attempts\n","    result = model.transcribe(\n","        audio_path,\n","        language=\"ta\",\n","        task=\"translate\",\n","        temperature=0.0,\n","        beam_size=5,\n","        patience=1.2,\n","        condition_on_previous_text=False,\n","        no_speech_threshold=0.8,\n","        compression_ratio_threshold=2.0,\n","        logprob_threshold=-0.35,\n","        word_timestamps=False,\n","        initial_prompt=prompt,\n","        verbose=True,\n","    )\n","\n","    print(\"✅ Whisper transcription completed with optimal parameters\")\n","    return result\n","\n","\n","def calculate_repetition_score(segments):\n","    \"\"\"\n","    Calculate a repetition score for transcription segments\n","    Lower score = less repetition = better\n","    \"\"\"\n","    if not segments:\n","        return 0.0\n","\n","    total_repetition = 0\n","    total_words = 0\n","\n","    for segment in segments:\n","        text = segment.get('text', '').strip().lower()\n","        words = text.split()\n","\n","        if len(words) < 2:\n","            continue\n","\n","        total_words += len(words)\n","\n","        # Count immediate word repetitions\n","        for i in range(len(words) - 1):\n","            if words[i] == words[i + 1]:\n","                total_repetition += 1\n","\n","        # Count phrase repetitions within segment\n","        for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len])\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n","                if phrase1 == phrase2:\n","                    total_repetition += phrase_len * 2  # Heavy penalty\n","\n","    return total_repetition / max(total_words, 1)\n","\n","def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n","    \"\"\"\n","    AGGRESSIVE post-processing function to detect and remove repetitive segments\n","    \"\"\"\n","    print(\"🔍 Starting aggressive repetition detection...\")\n","    cleaned_segments = []\n","\n","    for i, segment in enumerate(segments):\n","        text = segment['text'].strip()\n","        words = text.split()\n","\n","        # Skip very short segments\n","        if len(words) < 2:\n","            continue\n","\n","        # AGGRESSIVE: Check for excessive word repetition\n","        is_repetitive = False\n","\n","        # Count word frequencies\n","        word_counts = {}\n","        for word in words:\n","            word_lower = word.lower().strip('.,!?')\n","            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","        # Check if any single word dominates the segment\n","        max_word_count = max(word_counts.values()) if word_counts else 0\n","        word_dominance = max_word_count / len(words) if words else 0\n","\n","        if word_dominance > 0.4:  # If any word is >40% of the segment\n","            print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n","            continue\n","\n","        # Check for immediate repetitions (same word repeated consecutively)\n","        consecutive_repeats = 0\n","        max_consecutive = 0\n","\n","        for j in range(1, len(words)):\n","            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                consecutive_repeats += 1\n","                max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","            else:\n","                consecutive_repeats = 0\n","\n","        if max_consecutive > 3:  # More than 3 consecutive identical words\n","            print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n","            continue\n","\n","        # Check for pattern repetitions within segment\n","        for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n","\n","                if phrase1 == phrase2:\n","                    repetition_coverage = (phrase_len * 2) / len(words)\n","                    if repetition_coverage > max_repetition_ratio:\n","                        print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n","                        is_repetitive = True\n","                        break\n","            if is_repetitive:\n","                break\n","\n","        if is_repetitive:\n","            continue\n","\n","        # Check for similarity with recent segments (avoid near-duplicates)\n","        is_near_duplicate = False\n","        for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n","            prev_words = prev_segment['text'].lower().split()\n","            current_words = [w.lower() for w in words]\n","\n","            if prev_words and current_words:\n","                # Calculate Jaccard similarity\n","                prev_set = set(prev_words)\n","                current_set = set(current_words)\n","                intersection = len(prev_set.intersection(current_set))\n","                union = len(prev_set.union(current_set))\n","\n","                similarity = intersection / union if union > 0 else 0\n","\n","                if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n","                    print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n","                    is_near_duplicate = True\n","                    break\n","\n","        if is_near_duplicate:\n","            continue\n","\n","        # If we reach here, the segment passed all checks\n","        cleaned_segments.append(segment)\n","\n","    removed_count = len(segments) - len(cleaned_segments)\n","    print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","    print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n","\n","    return cleaned_segments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wB3OJ0-WU6kA","outputId":"605e65e8-8fdb-413c-e2e2-e99160a897a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n","============================================================\n","Original audio duration: 238.30 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 238.28 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"name":"stdout","output_type":"stream","text":["[00:00.000 --> 00:10.000]  Please press any key to select the next option.\n","[00:10.000 --> 00:15.000]  Hello sir, my name is Arookya Mehri. Can I speak to Mr. Subbaraj sir?\n","[00:15.000 --> 00:17.000]  Yes, I am coming.\n","[00:17.000 --> 00:23.000]  I have received a service call from Maxlife Insurance. Can I speak to you about the policy? Are you free?\n","[00:23.000 --> 00:25.000]  Yes, I am.\n","[00:55.000 --> 00:57.000]  Can we take this amount from 187,551?\n","[00:57.000 --> 00:58.000]  How long can we take?\n","[00:58.000 --> 01:00.000]  Sir, there is a locking period of 5 years.\n","[01:00.000 --> 01:02.000]  You have put a policy in 2018.\n","[01:02.000 --> 01:04.000]  You have paid 18, 19 and 23 years.\n","[01:04.000 --> 01:06.000]  You can take the amount after paying this due.\n","[01:06.000 --> 01:09.000]  Sir, you can take the amount if the locking period is completed.\n","[01:09.000 --> 01:11.000]  How much will I get?\n","[01:11.000 --> 01:14.000]  Sir, you can take the amount according to your fund value.\n","[01:14.000 --> 01:17.000]  If not, you can also withdraw partially.\n","[01:17.000 --> 01:19.000]  No, I don't have a fund value.\n","[01:19.000 --> 01:21.000]  Can I withdraw partially?\n","[01:21.000 --> 01:25.000]  Sir, as of now, there is 1,87,551 rupees.\n","[01:25.000 --> 01:32.000]  But if you pay now, your fund will participate in the share market, so there are chances of getting high returns, sir.\n","[01:32.000 --> 01:38.000]  It has come to Rs. 187,000 for 3 years. It is fair at any rate.\n","[01:38.000 --> 01:43.000]  Sir, your policy status is in discontinue, right? All your funds are going in discontinue funds.\n","[01:43.000 --> 01:46.000]  Your fund will participate only if your policy status is active, sir.\n","[01:46.000 --> 01:52.000]  Sir, you have not paid from 2021 for so many years, right? So how will your fund participate?\n","[01:52.000 --> 01:55.000]  He has taken a loan of Rs. 1,20,000 for 6 days.\n","[01:55.000 --> 01:58.000]  When is the end of the policy?\n","[01:58.000 --> 01:59.000]  5 years.\n","[01:59.000 --> 02:01.000]  You have to pay 5 years.\n","[02:01.000 --> 02:03.000]  But the risk is for 10 years.\n","[02:03.000 --> 02:05.000]  Your fund will participate for 10 years.\n","[02:05.000 --> 02:09.000]  If you need any emergency amount, you can withdraw partially.\n","[02:09.000 --> 02:11.000]  If not, you can withdraw.\n","[02:11.000 --> 02:14.000]  You can withdraw until the locking period is completed.\n","[02:14.000 --> 02:18.000]  When did the locking period start?\n","[02:18.000 --> 02:21.000]  It started in 2018.\n","[02:21.000 --> 02:22.300]  What month is it?\n","[02:23.040 --> 02:25.820]  It is the 30th month of the 9th month.\n","[02:26.600 --> 02:29.880]  Can I take Rs.39,000 or Rs.2,023,000?\n","[02:29.880 --> 02:32.800]  You can take it as soon as you complete the payment.\n","[02:33.560 --> 02:36.560]  Can I take it as soon as I complete the payment?\n","[02:36.560 --> 02:39.560]  Yes, your locking period is completed with Rs.2022,000.\n","[02:39.560 --> 02:42.160]  But if you complete the payment, you can take the amount.\n","[02:42.680 --> 02:45.680]  Where can I get the amount if I complete the payment online?\n","[02:45.680 --> 02:48.680]  You can pay online or you can pay online.\n","[02:48.680 --> 02:55.680]  You can do online payment through google pay, phone pay, links and website.\n","[02:55.680 --> 02:58.680]  Can I pay in cash?\n","[02:58.680 --> 03:01.680]  Yes, you can. I will guide you.\n","[03:01.680 --> 03:06.680]  You are doing online payment. I can help you if there is any error.\n","[03:06.680 --> 03:11.680]  I don't know how to do online payment.\n","[03:11.680 --> 03:13.680]  I don't understand.\n","[03:13.680 --> 03:18.680]  Yes, I am doing it now.\n","[03:18.680 --> 03:21.680]  How much are you paying?\n","[03:21.680 --> 03:25.680]  I am doing it through Netbuying.\n","[03:25.680 --> 03:29.680]  If you do it on your website, it will be 100% secure.\n","[03:29.680 --> 03:34.680]  If you give any details, you will only give your data and contact number.\n","[03:34.680 --> 03:39.680]  You will not ask any other details.\n","[03:39.680 --> 03:43.680]  I will try to do it. I will pay you directly.\n","[03:43.680 --> 03:47.680]  I want to take the amount by 30th September.\n","[03:47.680 --> 03:49.680]  Can I take it by 30th September?\n","[03:49.680 --> 03:50.680]  Yes, you can take it.\n","[03:50.680 --> 03:52.680]  I haven't blocked it yet.\n","[03:52.680 --> 03:56.680]  No, sir. You can take the amount after the locking period is completed.\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","\n","--- Removing Repetitive Segments ---\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting word-dominated segment: 5 years.... (dominance: 0.50)\n","📊 Aggressive cleaning: 61 → 60 segments\n","🗑️  Removed 1 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"name":"stdout","output_type":"stream","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_01 [0.0s - 15.0s]:\n","  📝 Please press any key to select the next option. Hello sir, my name is arookya mehri. Can i speak to mr. Subbaraj sir?\n","\n","SPEAKER_00 [15.0s - 17.0s]:\n","  📝 Yes, i am coming.\n","\n","SPEAKER_01 [17.0s - 23.0s]:\n","  📝 I have received a service call from maxlife insurance. Can i speak to you about the policy? Are you free?\n","\n","SPEAKER_00 [55.0s - 58.0s]:\n","  📝 Can we take this amount from 187,551? How long can we take?\n","\n","SPEAKER_01 [58.0s - 69.0s]:\n","  📝 Sir, there is a locking period of 5 years. You have put a policy in 2018. You have paid 18, 19 and 23 years. You can take the amount after paying this due. Sir, you can take the amount if the locking period is completed.\n","\n","SPEAKER_00 [69.0s - 71.0s]:\n","  📝 How much will i get?\n","\n","SPEAKER_01 [71.0s - 77.0s]:\n","  📝 Sir, you can take the amount according to your fund value. If not, you can also withdraw partially.\n","\n","SPEAKER_00 [77.0s - 81.0s]:\n","  📝 No, i don't have a fund value. Can i withdraw partially?\n","\n","SPEAKER_01 [81.0s - 92.0s]:\n","  📝 Sir, as of now, there is 1,87,551 rupees. But if you pay now, your fund will participate in the share market, so there are chances of getting high returns, sir.\n","\n","SPEAKER_00 [92.0s - 98.0s]:\n","  📝 It has come to rs. 187,000 for 3 years. It is fair at any rate.\n","\n","SPEAKER_01 [98.0s - 112.0s]:\n","  📝 Sir, your policy status is in discontinue, right? All your funds are going in discontinue funds. Your fund will participate only if your policy status is active, sir. Sir, you have not paid from 2021 for so many years, right? So how will your fund participate?\n","\n","SPEAKER_00 [112.0s - 118.0s]:\n","  📝 He has taken a loan of rs. 1,20,000 for 6 days. When is the end of the policy?\n","\n","SPEAKER_01 [119.0s - 134.0s]:\n","  📝 You have to pay 5 years. But the risk is for 10 years. Your fund will participate for 10 years. If you need any emergency amount, you can withdraw partially. If not, you can withdraw. You can withdraw until the locking period is completed.\n","\n","SPEAKER_00 [134.0s - 138.0s]:\n","  📝 When did the locking period start?\n","\n","SPEAKER_01 [138.0s - 141.0s]:\n","  📝 It started in 2018.\n","\n","SPEAKER_00 [141.0s - 142.3s]:\n","  📝 What month is it?\n","\n","SPEAKER_01 [143.0s - 145.8s]:\n","  📝 It is the 30th month of the 9th month.\n","\n","SPEAKER_00 [146.6s - 149.9s]:\n","  📝 Can i take rs.39,000 or rs.2,023,000?\n","\n","SPEAKER_01 [149.9s - 162.2s]:\n","  📝 You can take it as soon as you complete the payment. Can i take it as soon as i complete the payment? Yes, your locking period is completed with rs.2022,000. But if you complete the payment, you can take the amount.\n","\n","SPEAKER_00 [162.7s - 165.7s]:\n","  📝 Where can i get the amount if i complete the payment online?\n","\n","SPEAKER_01 [165.7s - 175.7s]:\n","  📝 You can pay online or you can pay online. You can do online payment through Google Pay, PhonePe, links and website.\n","\n","SPEAKER_00 [175.7s - 178.7s]:\n","  📝 Can i pay in cash?\n","\n","SPEAKER_01 [178.7s - 186.7s]:\n","  📝 Yes, you can. I will guide you. You are doing online payment. I can help you if there is any error.\n","\n","SPEAKER_00 [186.7s - 191.7s]:\n","  📝 I don't know how to do online payment.\n","\n","SPEAKER_01 [191.7s - 193.7s]:\n","  📝 I don't understand.\n","\n","SPEAKER_00 [193.7s - 198.7s]:\n","  📝 Yes, i am doing it now.\n","\n","SPEAKER_01 [198.7s - 201.7s]:\n","  📝 How much are you paying?\n","\n","SPEAKER_00 [201.7s - 205.7s]:\n","  📝 I am doing it through netbuying.\n","\n","SPEAKER_01 [205.7s - 219.7s]:\n","  📝 If you do it on your website, it will be 100% secure. If you give any details, you will only give your data and contact number. You will not ask any other details.\n","\n","SPEAKER_00 [219.7s - 232.7s]:\n","  📝 I will try to do it. I will pay you directly. I want to take the amount by 30th september. Can i take it by 30th september? Yes, you can take it. I haven't blocked it yet.\n","\n","SPEAKER_01 [232.7s - 236.7s]:\n","  📝 No, sir. You can take the amount after the locking period is completed.\n","\n","💾 Results saved to: enhanced_transcription_results.json\n","✅ Processing completed successfully!\n"]}],"source":["def main():\n","    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Smart Audio Preprocessing\n","    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","        return\n","\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 2: Enhanced Whisper Transcription with anti-repetition\n","    try:\n","        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","        print(\"✅ Whisper transcription completed\")\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    # Step 3: Remove repetitive segments BEFORE post-processing\n","    print(\"\\n--- Removing Repetitive Segments ---\")\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    # Step 4: Post-process remaining transcription\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = post_process_text(segment['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    # Step 5: Speaker Diarization\n","    print(\"\\n--- Speaker Diarization ---\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU for diarization\")\n","\n","        diarization = pipeline(CLEAN_AUDIO_PATH)\n","        print(\"✅ Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"⚠️  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 6: Generate Enhanced Dialogue\n","    print(\"\\n--- Generating Dialogue ---\")\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Combine segments by speaker\n","    dialogue = []\n","    current_speaker = None\n","    current_texts = []\n","    current_start = 0\n","    current_end = 0\n","\n","    for segment in processed_segments:\n","        start = segment['start']\n","        end = segment['end']\n","        text = segment['text'].strip()\n","\n","        speaker = get_dominant_speaker(start, end, diarization)\n","\n","        # Merge consecutive segments from same speaker (within 3 seconds)\n","        if (speaker == current_speaker and\n","            current_speaker and\n","            (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            # Save previous speaker's dialogue\n","            if current_speaker and current_texts:\n","                combined_text = ' '.join(current_texts)\n","                # Final check for repetition in combined text\n","                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined_text,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","\n","            # Start new speaker segment\n","            current_speaker = speaker\n","            current_texts = [text]\n","            current_start = start\n","            current_end = end\n","\n","    # Add final segment\n","    if current_speaker and current_texts:\n","        combined_text = ' '.join(current_texts)\n","        if len(combined_text.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined_text,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Step 7: Display Results\n","    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  📝 {entry['text']}\")\n","\n","    # Step 8: Save Results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open('enhanced_transcription_results.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n","    print(\"✅ Processing completed successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4GRSzdCwU7eW"},"outputs":[],"source":["\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMs5agD1A/kZAHPrS0pFm4A"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0391556884d143329aa84ce11c6e185d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc78ef9427254165bd2cfe98c7a6116c","placeholder":"​","style":"IPY_MODEL_4b465cfd8a844e2ab7d72795ff852ab7","value":" 5.91M/5.91M [00:00&lt;00:00, 8.41MB/s]"}},"06c7258dd2394d31b2232c2d9fa557b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0896a0c4c6c246ee81048521f8b7a5f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b71e1477994dd69ecacfdb94a31f60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09fd20d9dd5743aca17d2ad72f4d4918":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cd7e75b2aee4965bfbfa59d1c72ee0a","placeholder":"​","style":"IPY_MODEL_dd5df57b5d834cd8bcb474d2977d7be9","value":"pytorch_model.bin: 100%"}},"1ae94703895b4f34b38e9d2bfc1b7ca8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2642909c62f441d4803861b922676eb6","placeholder":"​","style":"IPY_MODEL_84cbf6a9aee5496b9ff8dda02e682201","value":"pytorch_model.bin: 100%"}},"1b26c0af8e8c4ab4a7ef439c808bbacd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd3a0664b78a414e9b738eacd0f1c363","placeholder":"​","style":"IPY_MODEL_88ed8843121047bf8bd27b2e01507961","value":"config.yaml: 100%"}},"1d4f965602c3413281ab548ca71d1d5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a0c5153f68b4f23a3c6cc7c27b5cb6a","placeholder":"​","style":"IPY_MODEL_8ade7c66ed0c49e7a4f7524be66b4db5","value":"config.yaml: 100%"}},"1f53e7b1bba2487bb55ba39e80091f14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fb904b152fa4106b84883582fd39023","placeholder":"​","style":"IPY_MODEL_d37613a83fbf4ea7a3f34ac084da7a72","value":" 469/469 [00:00&lt;00:00, 25.2kB/s]"}},"2642909c62f441d4803861b922676eb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"267ec4f22ea24817afe644f41871a93e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d9dfd18a8042d4b7c832adc3de2f73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a575684b7a546158f69179d3f9f81cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d4f965602c3413281ab548ca71d1d5e","IPY_MODEL_779bbeffb48045b0890304a26c928283","IPY_MODEL_a8b727f691884d499c32469dd40f8cb5"],"layout":"IPY_MODEL_b3854fbd8fec458ba546a5737df7b09a"}},"3bd1e05b2f41453a81ff4ee8d2ba4f75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06c7258dd2394d31b2232c2d9fa557b6","max":469,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efb72301ca45458fbb61f6303c498645","value":469}},"3ffcede934144fb59e2b15a4a6fcefb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b465cfd8a844e2ab7d72795ff852ab7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d0105d29e134c1b8dc36736a88525e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"661f65dda7284b61aa4d410227078045":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a0c5153f68b4f23a3c6cc7c27b5cb6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd7e75b2aee4965bfbfa59d1c72ee0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e5640b3c86c47be9d00d805b90d283f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744b70da37b04dde96c32e3cbdebc651":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2ff19cf348844e1ab178f5ddfae8b0a","placeholder":"​","style":"IPY_MODEL_08b71e1477994dd69ecacfdb94a31f60","value":"config.yaml: 100%"}},"751e900f19144883b45c20688f998d46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76fff81da3b74b0aa5ab3a74f1ebf0bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"779bbeffb48045b0890304a26c928283":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f945b0b4db2a4fe894a28f207ca93e52","max":221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adbfc1390ce54bce9d8aa5ae09ad3ea1","value":221}},"79aea07bf23d46aab96d45f30d1fd2ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84cbf6a9aee5496b9ff8dda02e682201":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86757b15562d45adacbc0768fc3f1138":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79aea07bf23d46aab96d45f30d1fd2ae","placeholder":"​","style":"IPY_MODEL_b0d859c96e2d4a9ba2a5199302e2ff41","value":" 26.6M/26.6M [00:00&lt;00:00, 38.1MB/s]"}},"87f6672852654c999d95f04f7572139f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b26c0af8e8c4ab4a7ef439c808bbacd","IPY_MODEL_3bd1e05b2f41453a81ff4ee8d2ba4f75","IPY_MODEL_1f53e7b1bba2487bb55ba39e80091f14"],"layout":"IPY_MODEL_751e900f19144883b45c20688f998d46"}},"88ed8843121047bf8bd27b2e01507961":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ade7c66ed0c49e7a4f7524be66b4db5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a50f59108184c09a0a728243cb8192a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b23efc2b31444eb9f0acbc9d31b1f9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fb904b152fa4106b84883582fd39023":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8b727f691884d499c32469dd40f8cb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_267ec4f22ea24817afe644f41871a93e","placeholder":"​","style":"IPY_MODEL_e8b10f712b1b4676b037cca2046248bf","value":" 221/221 [00:00&lt;00:00, 12.8kB/s]"}},"a95298f1f5c54f6893150f4f1a8cbf07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a50f59108184c09a0a728243cb8192a","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ffcede934144fb59e2b15a4a6fcefb4","value":399}},"adbfc1390ce54bce9d8aa5ae09ad3ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0d859c96e2d4a9ba2a5199302e2ff41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3854fbd8fec458ba546a5737df7b09a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd3a0664b78a414e9b738eacd0f1c363":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be20b1a773bf492db08addbdaa379424":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_744b70da37b04dde96c32e3cbdebc651","IPY_MODEL_a95298f1f5c54f6893150f4f1a8cbf07","IPY_MODEL_cc7156d882de4ca48e083c65fd64c886"],"layout":"IPY_MODEL_5d0105d29e134c1b8dc36736a88525e8"}},"c2ff19cf348844e1ab178f5ddfae8b0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c561dab6c5384bdb85ed6fee5a648858":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09fd20d9dd5743aca17d2ad72f4d4918","IPY_MODEL_f2adefd0942446bd999a6f9d12ee0fcb","IPY_MODEL_86757b15562d45adacbc0768fc3f1138"],"layout":"IPY_MODEL_661f65dda7284b61aa4d410227078045"}},"cc7156d882de4ca48e083c65fd64c886":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea7ebab2f8564f6ba75320b85a057984","placeholder":"​","style":"IPY_MODEL_f7790812fc0c4ef1a4c5e2b749398257","value":" 399/399 [00:00&lt;00:00, 21.9kB/s]"}},"d37613a83fbf4ea7a3f34ac084da7a72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd5df57b5d834cd8bcb474d2977d7be9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8b10f712b1b4676b037cca2046248bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea7ebab2f8564f6ba75320b85a057984":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed630c950b154b66a1aba385f5f8ca65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0896a0c4c6c246ee81048521f8b7a5f4","max":5905440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76fff81da3b74b0aa5ab3a74f1ebf0bb","value":5905440}},"efb72301ca45458fbb61f6303c498645":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2adefd0942446bd999a6f9d12ee0fcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39d9dfd18a8042d4b7c832adc3de2f73","max":26645418,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b23efc2b31444eb9f0acbc9d31b1f9d","value":26645418}},"f7790812fc0c4ef1a4c5e2b749398257":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f945b0b4db2a4fe894a28f207ca93e52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa1c5f371e3b4587b8fdf4540e21e4dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ae94703895b4f34b38e9d2bfc1b7ca8","IPY_MODEL_ed630c950b154b66a1aba385f5f8ca65","IPY_MODEL_0391556884d143329aa84ce11c6e185d"],"layout":"IPY_MODEL_6e5640b3c86c47be9d00d805b90d283f"}},"fc78ef9427254165bd2cfe98c7a6116c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}