{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":109248,"status":"ok","timestamp":1753264631275,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"APEFy1kWOh5S","outputId":"77ef640c-2ff7-405c-8da3-062353cc2298"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-hargemjw\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-hargemjw\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.7.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=56204995e8fdb23911f03b4eb2132eab18649b99ba86bd0fc9c12e0365362bb7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kab2ddiz/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":23588,"status":"ok","timestamp":1753264654852,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ZZ-jXf6mOqp0","outputId":"08f1b1ca-0096-4a0f-8f84-86843d5478a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.33.4)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.7.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=40bfb552af5f7b7a5d2cb0e97865fa4ab6d681177cb89cb2045ad20a751c9a70\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=88f0c19a02bf53ed15ee9db20cdffa1f32e98c685ed1b465165f3badf6e31ed5\n","  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.14.3 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.7.4\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":29398,"status":"ok","timestamp":1753264684235,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"18p3Psr_Or2M"},"outputs":[],"source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocess\n","import json"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1753264684256,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"iWd4n4ySOsEV"},"outputs":[],"source":["# Configuration\n","INPUT_AUDIO_PATH = \"/content/001_t2.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121018,"status":"ok","timestamp":1753264805278,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"rMrv9-WjO5s0","outputId":"20d81d00-3925-40a0-9b1c-4d1e3987d9c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 2.88G/2.88G [01:20<00:00, 38.3MiB/s]\n"]}],"source":["model = whisper.load_model(\"large\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":133,"status":"ok","timestamp":1753264805416,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"91AeiQe0O5vN","outputId":"c2a6b4ad-4c9c-4b88-b022-1da27bc3af4c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Whisper(\n","  (encoder): AudioEncoder(\n","    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (decoder): TextDecoder(\n","    (token_embedding): Embedding(51866, 1280)\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (cross_attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":6}],"source":["model"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1753264805423,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"9XecuZI1O5xs"},"outputs":[],"source":["def get_audio_duration(audio_path):\n","    \"\"\"Get audio duration using ffprobe\"\"\"\n","    try:\n","        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","        return float(result.stdout.strip())\n","    except Exception as e:\n","        print(f\"Could not get duration: {e}\")\n","        return 0\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1753264805432,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"chymACVjO50S"},"outputs":[],"source":["def audio_preprocessing_v1(input_path, output_path):\n","    \"\"\"Advanced audio preprocessing with better parameters\"\"\"\n","    print(\"--- Trying Advanced Audio Preprocessing ---\")\n","\n","    # Improved ffmpeg command - less aggressive filtering to preserve speech\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",  # Mono\n","        \"-ar\", \"16000\",  # 16kHz sample rate\n","        \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Advanced preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Advanced preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v2(input_path, output_path):\n","    \"\"\"Simplified but effective preprocessing\"\"\"\n","    print(\"--- Trying Simplified Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Simplified preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Simplified preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v3(input_path, output_path):\n","    \"\"\"Basic but reliable preprocessing\"\"\"\n","    print(\"--- Trying Basic Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Basic preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Basic preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v4(input_path, output_path):\n","    \"\"\"Minimal processing - just format conversion\"\"\"\n","    print(\"--- Trying Minimal Audio Processing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Minimal processing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Minimal processing failed: {e.returncode}\")\n","        return False\n","\n","def smart_audio_preprocessing(input_path, output_path):\n","    \"\"\"Try different preprocessing methods in order of preference\"\"\"\n","    original_duration = get_audio_duration(input_path)\n","    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n","\n","    methods = [\n","        audio_preprocessing_v1,\n","        audio_preprocessing_v2,\n","        audio_preprocessing_v3,\n","        audio_preprocessing_v4\n","    ]\n","\n","    for i, method in enumerate(methods, 1):\n","        if method(input_path, output_path):\n","            if os.path.exists(output_path):\n","                processed_duration = get_audio_duration(output_path)\n","                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n","\n","                if abs(original_duration - processed_duration) < 1.0:\n","                    print(f\"✅ Audio preprocessing successful with method {i}\")\n","                    return True\n","                else:\n","                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n","                    continue\n","\n","    print(\"❌ All preprocessing methods failed!\")\n","    return False"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1753264982379,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"RepgUy7BO55M"},"outputs":[],"source":["def enhanced_whisper_transcription(audio_path):\n","    \"\"\"\n","    Enhanced Whisper transcription with optimal anti-repetition parameters\n","    \"\"\"\n","    print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","\n","    initial_prompt1 = (\n","        \"Axis Maxlife Insurance, Policy number, fund value, Due date,\"\n","        \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking,\"\n","    )\n","\n","    # initial_prompt1=(\n","    #     \"Maxlife Insurance call, Policy number 600603260, due date 30th September 2021, amount Rs 1,20,000, \"\n","    #     \"status discontinued, payment, Google Pay, PhonePe, financial problem, fund value, locking period, \"\n","    #     \"Axis Maxlife, online payment, partial withdrawal, premium, 5 years, 10 years.\"\n","    # )\n","\n","    # Single optimal strategy - no need for multiple attempts\n","    result = model.transcribe(\n","        audio_path,\n","        # language=\"ta\",                      # Tamil\n","        task=\"translate\",                   # Translate to English\n","        verbose=True,                       # Keep verbose for monitoring\n","\n","        # temperature=[0.0],\n","        # beam_size=5,\n","        # best_of=5,\n","        temperature=[0.2, 0.4],\n","        beam_size=1,\n","        best_of=1,\n","        logprob_threshold=-1.0,\n","        compression_ratio_threshold=2.4,\n","        no_speech_threshold=0.3,\n","        condition_on_previous_text=False,\n","        # initial_prompt=None,\n","        word_timestamps=False,\n","    )\n","\n","    print(\"✅ Whisper transcription completed with optimal parameters\")\n","    return result\n","\n","def calculate_repetition_score(segments):\n","    \"\"\"\n","    Calculate a repetition score for transcription segments\n","    Lower score = less repetition = better\n","    \"\"\"\n","    if not segments:\n","        return 0.0\n","\n","    total_repetition = 0\n","    total_words = 0\n","\n","    for segment in segments:\n","        text = segment.get('text', '').strip().lower()\n","        words = text.split()\n","\n","        if len(words) < 2:\n","            continue\n","\n","        total_words += len(words)\n","\n","        # Count immediate word repetitions\n","        for i in range(len(words) - 1):\n","            if words[i] == words[i + 1]:\n","                total_repetition += 1\n","\n","        # Count phrase repetitions within segment\n","        for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len])\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n","                if phrase1 == phrase2:\n","                    total_repetition += phrase_len * 2  # Heavy penalty\n","\n","    return total_repetition / max(total_words, 1)\n","\n","def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n","    \"\"\"\n","    AGGRESSIVE post-processing function to detect and remove repetitive segments\n","    \"\"\"\n","    print(\"🔍 Starting aggressive repetition detection...\")\n","    cleaned_segments = []\n","\n","    for i, segment in enumerate(segments):\n","        text = segment['text'].strip()\n","        words = text.split()\n","\n","        # Skip very short segments\n","        if len(words) < 2:\n","            continue\n","\n","        # AGGRESSIVE: Check for excessive word repetition\n","        is_repetitive = False\n","\n","        # Count word frequencies\n","        word_counts = {}\n","        for word in words:\n","            word_lower = word.lower().strip('.,!?')\n","            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","        # Check if any single word dominates the segment\n","        max_word_count = max(word_counts.values()) if word_counts else 0\n","        word_dominance = max_word_count / len(words) if words else 0\n","\n","        if word_dominance > 0.4:  # If any word is >40% of the segment\n","            print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n","            continue\n","\n","        # Check for immediate repetitions (same word repeated consecutively)\n","        consecutive_repeats = 0\n","        max_consecutive = 0\n","\n","        for j in range(1, len(words)):\n","            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                consecutive_repeats += 1\n","                max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","            else:\n","                consecutive_repeats = 0\n","\n","        if max_consecutive > 3:  # More than 3 consecutive identical words\n","            print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n","            continue\n","\n","        # Check for pattern repetitions within segment\n","        for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n","\n","                if phrase1 == phrase2:\n","                    repetition_coverage = (phrase_len * 2) / len(words)\n","                    if repetition_coverage > max_repetition_ratio:\n","                        print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n","                        is_repetitive = True\n","                        break\n","            if is_repetitive:\n","                break\n","\n","        if is_repetitive:\n","            continue\n","\n","        # Check for similarity with recent segments (avoid near-duplicates)\n","        is_near_duplicate = False\n","        for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n","            prev_words = prev_segment['text'].lower().split()\n","            current_words = [w.lower() for w in words]\n","\n","            if prev_words and current_words:\n","                # Calculate Jaccard similarity\n","                prev_set = set(prev_words)\n","                current_set = set(current_words)\n","                intersection = len(prev_set.intersection(current_set))\n","                union = len(prev_set.union(current_set))\n","\n","                similarity = intersection / union if union > 0 else 0\n","\n","                if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n","                    print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n","                    is_near_duplicate = True\n","                    break\n","\n","        if is_near_duplicate:\n","            continue\n","\n","        # If we reach here, the segment passed all checks\n","        cleaned_segments.append(segment)\n","\n","    removed_count = len(segments) - len(cleaned_segments)\n","    print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","    print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n","\n","    return cleaned_segments"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1753264982474,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"xPZYyKNxO575"},"outputs":[],"source":["def post_process_text(text):\n","    \"\"\"Clean up transcribed text\"\"\"\n","    if not text:\n","        return \"\"\n","\n","    # Remove excessive repetitions within text\n","    words = text.split()\n","    cleaned_words = []\n","\n","    i = 0\n","    while i < len(words):\n","        current_word = words[i].lower()\n","\n","        # Look for immediate repetitions (same word repeated 3+ times)\n","        repetition_count = 1\n","        j = i + 1\n","        while j < len(words) and words[j].lower() == current_word:\n","            repetition_count += 1\n","            j += 1\n","\n","        # Keep only 1-2 repetitions maximum\n","        keep_count = min(repetition_count, 2) if repetition_count <= 3 else 1\n","        for _ in range(keep_count):\n","            cleaned_words.append(words[i])\n","\n","        i = i + repetition_count\n","\n","    text = ' '.join(cleaned_words)\n","\n","    # Common corrections for Indian insurance context\n","    corrections = {\n","        'access max life': 'Axis Max Life',\n","        'axis max life': 'Axis Max Life',\n","        'g pay': 'GPay',\n","        'google pay': 'Google Pay',\n","        'phone pay': 'PhonePe',\n","        'phone pe': 'PhonePe',\n","        'pay tm': 'Paytm',\n","        'net banking': 'netbanking',\n","        'some assured': 'sum assured',\n","        'premium do': 'premium due',\n","        'do date': 'due date',\n","    }\n","\n","    text_lower = text.lower()\n","    for wrong, correct in corrections.items():\n","        text_lower = text_lower.replace(wrong, correct)\n","\n","    # Capitalize first letter of sentences\n","    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n","                       lambda m: m.group(1) + m.group(2).upper(), text_lower)\n","\n","    return text_lower.strip()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["36341d262ac44f118ede0b606ea5b7d9","e43fd1310c20499cac08c4d87d190a9d","31afa311ab5c4b17b76fade2d51abf19","7e0e98adcc5543bd83352404baed3090","dfa37da744254c47bdec1341ef0373f7","3e4759d667c94fd5badb9877992f6fbd","17e2f8eb4ad84f3c897df24bf7295dfa","6e3afb1c82bb433492fbe8593c74de54","8c3c4416891e461186a1ccb7964ca0f0","08e73f41850144a9bc137fa3850f0ec5","2b1e42c944754d638dbf0b633c261a6e","cac0a5870a6442f392e5eb851b6c621e","065c5dd2291a4967947a421efe9f9a60","47cdcacfe3c346f5bfe2b44e728a5291","8f130f81851f439a9744486cd7cb3906","d34950fc824b46969559d37f03ffa5c3","f20cb14594a24b1291bb9ce354e818bd","5cf4a701a2474dc4a05670a7495252a0","5818b18e6e5d47f8ae813ed6ce3c6057","9b9efff2243a420d82d91ba082282449","b0c3b6ea6902438fa94510c8c7a3b760","7c3f85e351404972ae9509f14d457881","e8c3896f84304ab597e0c943d2e94143","a9b91341697b4ad991b99ff446725783","837045c447a64fa5ae923205f7c4a39a","a2d3c677c7444d3fb9d140bd37a98163","7fa27d8ae7284644aaaf941565abbf5f","1d71a2f66fd14275a754325886d9d919","5f2f630a5a04425f8f44bfe70e1b3a63","6811321855814bd79b4a80663cedc37c","866f775bd6164a3985f561ba8ba9df6f","01f410d9b4624242af559bd16205c711","175aeb955ff94a239988c103f066d911","bb05b32c2a9a434ab081d2c2a7e05f5b","47fc39a1d5f14261b79c293542f8c399","8b123d5e3e484e1c882ac1547ebe01bb","8d5dd7508fdf499ba1e988ffdfb43bc3","976d49a2186a4107880fbb4014b52574","855e8c380b0a49888032b1f47e3a0037","f02444eda3fa43bbafa279f8f6c7ad9c","6a43cd43f8c24e87bb16aedb10cd1319","642dcb71bdcf4790aedc203fe48704cb","343f3506225943fdbfaa5cadb6707285","ea01addbbb814699a36f2f8e6829ace3","bcb90921c0d54a74a3a966dc5631fd5a","4df7d13190b34790a9a666e7cca0a843","5c6228fdafed462082630a66d2c2a932","63163991c56c4e6f9b5e851813e865de","44a9c73cb80d47e0800a1041cdf75c13","7d73ee330b1c40efb92761c88ceab0cb","66ac509774014dd39f5af3dbc85e3d1c","afe744bd7f6a4d0895769b836575a672","e9f49f19207f41f48070fe1e8f62aa4c","b06679d804b74e9ebc5dfea980d8db99","dff644b121d04c5b91e9ad11dccc3049"]},"id":"RY9MBqKZO59P","executionInfo":{"status":"ok","timestamp":1753267021452,"user_tz":-330,"elapsed":2038651,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"3f862ae0-63fa-4af4-848d-fd29721f5008"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n","============================================================\n","Original audio duration: 238.30 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 238.28 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n","Detected language: Tamil\n","[00:00.000 --> 00:07.000]  Please press any key to enter the search option.\n","[00:07.000 --> 00:15.000]  Hello Sir, My name is Arookya Mehri. Can I talk to Mr. Subbaraj Sir?\n","[00:15.000 --> 00:17.000]  Yes, I am coming.\n","[00:17.000 --> 00:23.000]  I have received a service call from Maxlife Insurance. Can I talk to you about the policy? Are you free?\n","[00:23.000 --> 00:25.000]  Yes, I am free.\n","[00:25.000 --> 00:28.000]  Thank you. You have taken a policy from your name.\n","[00:28.000 --> 00:33.000]  Max Life Fast Track Super Plan. Policy number is 00603260.\n","[00:33.000 --> 00:37.000]  The due date is 30th September 2021.\n","[00:37.000 --> 00:40.000]  The amount to be paid is 1,20,000.\n","[00:40.000 --> 00:44.000]  The policy status is discontinued. Can you tell me why you are not paying?\n","[00:44.000 --> 00:47.000]  Finance is not good.\n","[00:47.000 --> 00:48.000]  Sorry sir.\n","[00:48.000 --> 00:50.000]  Finance is not good.\n","[00:50.000 --> 00:56.520]  If I ask for 1 lakh rupees, can I take this amount?\n","[00:56.520 --> 00:58.060]  How long can I take?\n","[00:58.060 --> 01:01.420]  So you have a 5 year locking period. You have a policy in 2018.\n","[01:01.420 --> 01:05.820]  You have paid 18, 19, 20 years. You can take this amount after paying the dues.\n","[01:05.820 --> 01:08.180]  You can take the complete locking period.\n","[01:08.180 --> 01:10.980]  How much will I get?\n","[01:10.980 --> 01:13.820]  You can take the amount that will be your fund value.\n","[01:13.820 --> 01:16.340]  Or you can do partial withdrawal.\n","[01:16.340 --> 01:20.340]  No, I am not a fund-granting person.\n","[01:20.340 --> 01:25.340]  Sir, as of now, you have Rs.1,87,551.\n","[01:25.340 --> 01:28.340]  But if you pay now, your fund will participate in the share market.\n","[01:28.340 --> 01:31.340]  So, there are chances of high returns.\n","[01:31.340 --> 01:35.340]  Sir, it has been 3 years and Rs.1,58,000.\n","[01:35.340 --> 01:37.340]  It is fair at any rate.\n","[01:37.340 --> 01:40.340]  Sir, your policy status is in discontinued.\n","[01:40.340 --> 01:42.340]  Your funds are going in discontinued funds.\n","[01:42.340 --> 01:46.140]  only when the policy status is active, your fund will participate\n","[01:46.140 --> 01:51.700]  you have been paying for the fund since 2021, how will your fund participate?\n","[01:51.700 --> 01:54.780]  if I take a loan of Rs. 1,20,000 and pay it\n","[01:54.780 --> 01:57.940]  when will it end?\n","[01:57.940 --> 01:58.940]  5 years\n","[01:58.940 --> 02:00.740]  you have to pay for 5 years\n","[02:00.740 --> 02:05.040]  but the risk is for 10 years, your fund will participate for 10 years\n","[02:05.040 --> 02:09.240]  if you need any emergency amount, you can withdraw it partially\n","[02:09.240 --> 02:14.240]  Yes, we can take the lock period until the lock period is completed.\n","[02:14.240 --> 02:18.240]  When did the lock period start?\n","[02:18.240 --> 02:21.240]  In 2018.\n","[02:21.240 --> 02:23.240]  Which month?\n","[02:23.240 --> 02:26.240]  In the 9th month of the 30th.\n","[02:26.240 --> 02:30.240]  Can we take the lock period until the end of the 30th month?\n","[02:30.240 --> 02:33.240]  Yes, we can take the lock period once the payment is made.\n","[02:33.240 --> 02:38.240]  Yes, your locking period is completed with 2022.\n","[02:38.240 --> 02:42.240]  But if you pay, you can take the amount.\n","[02:42.240 --> 02:45.240]  Where can I pay for 1 lakh?\n","[02:45.240 --> 02:51.240]  You can pay through branch office or you can pay online.\n","[02:51.240 --> 02:55.240]  You can pay easily through Google Pay, Phone Pay, Link, Website.\n","[02:55.240 --> 02:58.240]  Can I pay online through Access Bank?\n","[02:58.240 --> 03:00.240]  Yes, I will do that.\n","[03:00.240 --> 03:05.240]  I will guide you, and you are paying online, so I can help you if you get any error.\n","[03:06.240 --> 03:12.240]  I don't know if I will get any error if I go online and pay.\n","[03:12.240 --> 03:14.240]  I don't understand.\n","[03:14.240 --> 03:18.240]  I don't know if I will get any error if I go online and pay.\n","[03:18.240 --> 03:21.240]  Ok, when are you going to pay?\n","[03:21.240 --> 03:24.240]  I will do that by the deadline.\n","[03:24.240 --> 03:29.240]  Yes we can do that sir, if you do it on your website it will be 100% secure.\n","[03:29.240 --> 03:36.240]  If you give any details, you are giving only your contact number and not asking any other details.\n","[03:36.240 --> 03:43.240]  I will try to do it on my website, but I will pay directly.\n","[03:43.240 --> 03:47.240]  I have to take the amount by 30th September.\n","[03:47.240 --> 03:49.240]  Shall we take it by 1st September?\n","[03:49.240 --> 03:50.240]  Yes we can take it.\n","[03:50.240 --> 03:53.240]  Rajiv has not blocked it.\n","[03:53.240 --> 03:58.240]  If you complete the locking period, you can take the amount.\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","\n","--- Removing Repetitive Segments ---\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting word-dominated segment: Sorry sir.... (dominance: 0.50)\n","🚫 Rejecting near-duplicate: Finance is not good.... (similarity: 1.00)\n","🚫 Rejecting word-dominated segment: 5 years... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: In 2018.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Which month?... (dominance: 0.50)\n","🚫 Rejecting near-duplicate: I don't know if I will get any... (similarity: 1.00)\n","📊 Aggressive cleaning: 65 → 59 segments\n","🗑️  Removed 6 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n"]},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36341d262ac44f118ede0b606ea5b7d9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac0a5870a6442f392e5eb851b6c621e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c3896f84304ab597e0c943d2e94143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb05b32c2a9a434ab081d2c2a7e05f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb90921c0d54a74a3a966dc5631fd5a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_01 [0.0s - 15.0s]:\n","  📝 Please press any key to enter the search option. Hello sir, my name is arookya mehri. Can i talk to mr. Subbaraj sir?\n","\n","SPEAKER_00 [15.0s - 17.0s]:\n","  📝 Yes, i am coming.\n","\n","SPEAKER_01 [17.0s - 23.0s]:\n","  📝 I have received a service call from maxlife insurance. Can i talk to you about the policy? Are you free?\n","\n","SPEAKER_00 [23.0s - 25.0s]:\n","  📝 Yes, i am free.\n","\n","SPEAKER_01 [25.0s - 44.0s]:\n","  📝 Thank you. You have taken a policy from your name. Max life fast track super plan. Policy number is 00603260. The due date is 30th september 2021. The amount to be paid is 1,20,000. The policy status is discontinued. Can you tell me why you are not paying?\n","\n","SPEAKER_00 [44.0s - 47.0s]:\n","  📝 Finance is not good.\n","\n","SPEAKER_00 [50.0s - 58.1s]:\n","  📝 If i ask for 1 lakh rupees, can i take this amount? How long can i take?\n","\n","SPEAKER_01 [58.1s - 68.2s]:\n","  📝 So you have a 5 year locking period. You have a policy in 2018. You have paid 18, 19, 20 years. You can take this amount after paying the dues. You can take the complete locking period.\n","\n","SPEAKER_00 [68.2s - 71.0s]:\n","  📝 How much will i get?\n","\n","SPEAKER_01 [71.0s - 76.3s]:\n","  📝 You can take the amount that will be your fund value. Or you can do partial withdrawal.\n","\n","SPEAKER_00 [76.3s - 80.3s]:\n","  📝 No, i am not a fund-granting person.\n","\n","SPEAKER_01 [80.3s - 91.3s]:\n","  📝 Sir, as of now, you have rs.1,87,551. But if you pay now, your fund will participate in the share market. So, there are chances of high returns.\n","\n","SPEAKER_00 [91.3s - 97.3s]:\n","  📝 Sir, it has been 3 years and rs.1,58,000. It is fair at any rate.\n","\n","SPEAKER_01 [97.3s - 111.7s]:\n","  📝 Sir, your policy status is in discontinued. Your funds are going in discontinued funds. Only when the policy status is active, your fund will participate You have been paying for the fund since 2021, how will your fund participate?\n","\n","SPEAKER_00 [111.7s - 117.9s]:\n","  📝 If i take a loan of rs. 1,20,000 and pay it When will it end?\n","\n","SPEAKER_01 [118.9s - 134.2s]:\n","  📝 You have to pay for 5 years But the risk is for 10 years, your fund will participate for 10 years If you need any emergency amount, you can withdraw it partially Yes, we can take the lock period until the lock period is completed.\n","\n","SPEAKER_00 [134.2s - 138.2s]:\n","  📝 When did the lock period start?\n","\n","SPEAKER_01 [143.2s - 146.2s]:\n","  📝 In the 9th month of the 30th.\n","\n","SPEAKER_00 [146.2s - 150.2s]:\n","  📝 Can we take the lock period until the end of the 30th month?\n","\n","SPEAKER_01 [150.2s - 162.2s]:\n","  📝 Yes, we can take the lock period once the payment is made. Yes, your locking period is completed with 2022. But if you pay, you can take the amount.\n","\n","SPEAKER_00 [162.2s - 165.2s]:\n","  📝 Where can i pay for 1 lakh?\n","\n","SPEAKER_01 [165.2s - 175.2s]:\n","  📝 You can pay through branch office or you can pay online. You can pay easily through Google Pay, PhonePe, link, website.\n","\n","SPEAKER_00 [175.2s - 178.2s]:\n","  📝 Can i pay online through access bank?\n","\n","SPEAKER_01 [178.2s - 185.2s]:\n","  📝 Yes, i will do that. I will guide you, and you are paying online, so i can help you if you get any error.\n","\n","SPEAKER_00 [186.2s - 192.2s]:\n","  📝 I don't know if i will get any error if i go online and pay.\n","\n","SPEAKER_01 [192.2s - 194.2s]:\n","  📝 I don't understand.\n","\n","SPEAKER_01 [198.2s - 201.2s]:\n","  📝 Ok, when are you going to pay?\n","\n","SPEAKER_00 [201.2s - 204.2s]:\n","  📝 I will do that by the deadline.\n","\n","SPEAKER_01 [204.2s - 216.2s]:\n","  📝 Yes we can do that sir, if you do it on your website it will be 100% secure. If you give any details, you are giving only your contact number and not asking any other details.\n","\n","SPEAKER_00 [216.2s - 233.2s]:\n","  📝 I will try to do it on my website, but i will pay directly. I have to take the amount by 30th september. Shall we take it by 1st september? Yes we can take it. Rajiv has not blocked it.\n","\n","SPEAKER_01 [233.2s - 238.2s]:\n","  📝 If you complete the locking period, you can take the amount.\n","\n","💾 Results saved to: enhanced_transcription_results.json\n","✅ Processing completed successfully!\n"]}],"source":["def main():\n","    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Smart Audio Preprocessing\n","    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","        return\n","\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 2: Enhanced Whisper Transcription with anti-repetition\n","    try:\n","        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","        print(\"✅ Whisper transcription completed\")\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    # Step 3: Remove repetitive segments BEFORE post-processing\n","    print(\"\\n--- Removing Repetitive Segments ---\")\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    # Step 4: Post-process remaining transcription\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = post_process_text(segment['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    # Step 5: Speaker Diarization\n","    print(\"\\n--- Speaker Diarization ---\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU for diarization\")\n","\n","        diarization = pipeline(CLEAN_AUDIO_PATH)\n","        print(\"✅ Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"⚠️  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 6: Generate Enhanced Dialogue\n","    print(\"\\n--- Generating Dialogue ---\")\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Combine segments by speaker\n","    dialogue = []\n","    current_speaker = None\n","    current_texts = []\n","    current_start = 0\n","    current_end = 0\n","\n","    for segment in processed_segments:\n","        start = segment['start']\n","        end = segment['end']\n","        text = segment['text'].strip()\n","\n","        speaker = get_dominant_speaker(start, end, diarization)\n","\n","        # Merge consecutive segments from same speaker (within 3 seconds)\n","        if (speaker == current_speaker and\n","            current_speaker and\n","            (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            # Save previous speaker's dialogue\n","            if current_speaker and current_texts:\n","                combined_text = ' '.join(current_texts)\n","                # Final check for repetition in combined text\n","                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined_text,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","\n","            # Start new speaker segment\n","            current_speaker = speaker\n","            current_texts = [text]\n","            current_start = start\n","            current_end = end\n","\n","    # Add final segment\n","    if current_speaker and current_texts:\n","        combined_text = ' '.join(current_texts)\n","        if len(combined_text.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined_text,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Step 7: Display Results\n","    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  📝 {entry['text']}\")\n","\n","    # Step 8: Save Results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open('enhanced_transcription_results.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n","    print(\"✅ Processing completed successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"pQoq4Py49A32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mHxSqdds9BDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cp908DtL9BF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuration\n","INPUT_AUDIO_PATH = \"/content/001_t1.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization_new.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""],"metadata":{"id":"_XJe7ffA9BI5","executionInfo":{"status":"ok","timestamp":1753268678144,"user_tz":-330,"elapsed":5,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"miSHQ7pOoNVM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753270774955,"user_tz":-330,"elapsed":2096632,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"45b71974-aad7-483c-a6f1-bcc40236f3f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n","============================================================\n","Original audio duration: 258.16 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 258.12 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n","Detected language: Tamil\n","[00:00.000 --> 00:14.000]  Hello, Hello, Hello.\n","[00:14.000 --> 00:15.000]  Hello.\n","[00:15.000 --> 00:18.000]  Hello sir, my name is Aruke. Can you speak to Mrs. Vijaya?\n","[00:18.000 --> 00:23.000]  Yes, she is at home. I am at home.\n","[00:23.000 --> 00:25.000]  What is your relation with her?\n","[00:25.000 --> 00:28.000]  Yes, I am her husband.\n","[00:28.000 --> 00:31.000]  We have called from Maxlife Insurance. This is a service call.\n","[00:31.000 --> 00:33.000]  Can you talk about the policy for 2 minutes?\n","[00:33.000 --> 00:36.000]  Sir, do you have any details about it?\n","[00:36.000 --> 00:41.000]  Nothing. I am applying for it for 8 days.\n","[00:41.000 --> 00:48.000]  Sir, you have applied for the policy for 8600 rupees in 2018.\n","[00:48.000 --> 00:51.000]  Now, you have not paid 3 rupees and it is pending.\n","[00:51.000 --> 00:54.000]  Sir, you have applied for the Maxlife Gain Premier Plan.\n","[00:54.000 --> 01:07.000]  The policy number is 31, 5, 53, 11. The due date is 23rd November 2020. The amount is 29,162.36 paise.\n","[01:07.000 --> 01:14.000]  You have taken a lot of time for the due date. The policy status is paid. The day by day you are getting paid.\n","[01:14.000 --> 01:18.000]  Can you tell us the reason why you are not paying?\n","[01:18.000 --> 01:20.000]  Yes, I told you.\n","[01:20.000 --> 01:24.000]  A guy named Mahesh has been fired.\n","[01:24.000 --> 01:30.000]  Another guy named Payam has been fired.\n","[01:30.000 --> 01:34.000]  Both of them have been fined Rs. 40,000-50,000.\n","[01:34.000 --> 01:39.000]  That's why the pending is not going well.\n","[01:39.000 --> 01:43.000]  The next place is being spent on another place, so I can't manage.\n","[01:43.000 --> 01:47.000]  That's why I can't pay the rent.\n","[01:47.000 --> 01:55.000]  I understand what you are saying. But you have taken a very beneficial policy.\n","[01:55.000 --> 01:58.000]  Your benefits are added according to how you pay.\n","[01:58.000 --> 02:04.000]  You have paid for 6 years. You have paid for 3 years.\n","[02:04.000 --> 02:08.000]  You have only 3 years to pay and claim good benefits.\n","[02:08.000 --> 02:14.000]  If you pay for 3 years, you don't have to pay insecure.\n","[02:14.000 --> 02:18.000]  I will see how much I can manage.\n","[02:18.000 --> 02:23.000]  If I get a loan, I will take another loan. I will not be ready.\n","[02:23.000 --> 02:27.000]  I cannot manage. Otherwise, I will not stop.\n","[02:27.000 --> 02:31.000]  I will see how much I can manage.\n","[02:31.000 --> 02:34.000]  When will you pay the rent?\n","[02:34.000 --> 02:39.000]  It will take about a month. I have some issues.\n","[02:39.000 --> 02:42.000]  I cannot manage.\n","[02:42.000 --> 02:49.000]  So, if you close the loan, you will not face any loss.\n","[02:49.000 --> 02:53.000]  The amount you have paid will be given to you in the percentage amount for Surrender Valiance.\n","[02:53.000 --> 02:55.000]  You will not face any loss.\n","[02:55.000 --> 02:59.000]  So, you can claim the loan in the maturity period with a good benefit.\n","[02:59.000 --> 03:04.000]  Since you have paid the loan in due date, the bonus amount has been increased.\n","[03:04.000 --> 03:08.000]  The amount is Rs.2001.50.\n","[03:08.000 --> 03:13.000]  If you pay in delay, the amount will increase year by year.\n","[03:13.000 --> 03:17.000]  You can claim it with a good benefit when you claim it in maturity time.\n","[03:17.000 --> 03:20.000]  Ok. I will see.\n","[03:20.000 --> 03:24.000]  Where do you pay? Online or branch office?\n","[03:24.000 --> 03:27.000]  I pay at the office.\n","[03:27.000 --> 03:31.000]  Ok. Don't delay. Pay as soon as possible.\n","[03:31.000 --> 03:33.000]  Because you have been paid late fees waiver off this month.\n","[03:33.000 --> 03:35.000]  You can pay without a bud.\n","[03:35.000 --> 03:37.000]  We'll try to build that in the future.\n","[03:37.000 --> 03:39.000]  Ok Madam. We'llophrenge.\n","[03:39.000 --> 03:41.000]  We'll call back to you again.\n","[03:41.000 --> 03:43.000]  Ok Madam.\n","[03:43.000 --> 03:45.000]  Your health declaration form is pending.\n","[03:45.000 --> 03:47.000]  Since you took time to get the DVD,\n","[03:47.000 --> 03:49.000]  we're asking for your form\n","[03:49.000 --> 03:51.000]  to know about your health condition.\n","[03:51.000 --> 03:53.000]  You can fill it in the branch office\n","[03:53.000 --> 03:55.000]  or we can fill it and submit for you.\n","[03:55.000 --> 03:57.000]  Ok?\n","[03:57.000 --> 03:59.000]  Ok Madam.\n","[03:59.000 --> 04:01.000]  Ok. Plus your watch up number\n","[04:01.000 --> 04:03.000]  is not linked to the policy.\n","[04:03.000 --> 04:07.000]  We will share a link. Click on it and register your WhatsApp number.\n","[04:07.000 --> 04:11.000]  If there is an update in the MAC Flash Incidence, you can check it through WhatsApp.\n","[04:11.000 --> 04:14.000]  I don't have a WhatsApp number.\n","[04:14.000 --> 04:15.000]  No, you don't have.\n","[04:15.000 --> 04:16.000]  I will give you my number.\n","[04:16.000 --> 04:17.000]  Okay, sir.\n","[04:17.000 --> 04:18.000]  Thank you.\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","\n","--- Removing Repetitive Segments ---\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting word-dominated segment: Hello, Hello, Hello.... (dominance: 1.00)\n","🚫 Rejecting near-duplicate: I will see how much I can mana... (similarity: 1.00)\n","🚫 Rejecting word-dominated segment: Ok Madam.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Ok Madam.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Okay, sir.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Thank you.... (dominance: 0.50)\n","📊 Aggressive cleaning: 70 → 62 segments\n","🗑️  Removed 8 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_01 [15.0s - 18.0s]:\n","  📝 Hello sir, my name is aruke. Can you speak to mrs. Vijaya?\n","\n","SPEAKER_00 [18.0s - 23.0s]:\n","  📝 Yes, she is at home. I am at home.\n","\n","SPEAKER_01 [23.0s - 25.0s]:\n","  📝 What is your relation with her?\n","\n","SPEAKER_00 [25.0s - 28.0s]:\n","  📝 Yes, i am her husband.\n","\n","SPEAKER_01 [28.0s - 36.0s]:\n","  📝 We have called from maxlife insurance. This is a service call. Can you talk about the policy for 2 minutes? Sir, do you have any details about it?\n","\n","SPEAKER_00 [36.0s - 41.0s]:\n","  📝 Nothing. I am applying for it for 8 days.\n","\n","SPEAKER_01 [41.0s - 78.0s]:\n","  📝 Sir, you have applied for the policy for 8600 rupees in 2018. Now, you have not paid 3 rupees and it is pending. Sir, you have applied for the maxlife gain premier plan. The policy number is 31, 5, 53, 11. The due date is 23rd november 2020. The amount is 29,162.36 paise. You have taken a lot of time for the due date. The policy status is paid. The day by day you are getting paid. Can you tell us the reason why you are not paying?\n","\n","SPEAKER_00 [78.0s - 107.0s]:\n","  📝 Yes, i told you. A guy named mahesh has been fired. Another guy named payam has been fired. Both of them have been fined rs. 40,000-50,000. That's why the pending is not going well. The next place is being spent on another place, so i can't manage. That's why i can't pay the rent.\n","\n","SPEAKER_01 [107.0s - 134.0s]:\n","  📝 I understand what you are saying. But you have taken a very beneficial policy. Your benefits are added according to how you pay. You have paid for 6 years. You have paid for 3 years. You have only 3 years to pay and claim good benefits. If you pay for 3 years, you don't have to pay insecure.\n","\n","SPEAKER_00 [134.0s - 147.0s]:\n","  📝 I will see how much i can manage. If i get a loan, i will take another loan. I will not be ready. I cannot manage. Otherwise, i will not stop.\n","\n","SPEAKER_01 [151.0s - 154.0s]:\n","  📝 When will you pay the rent?\n","\n","SPEAKER_00 [154.0s - 169.0s]:\n","  📝 It will take about a month. I have some issues. I cannot manage. So, if you close the loan, you will not face any loss.\n","\n","SPEAKER_01 [169.0s - 197.0s]:\n","  📝 The amount you have paid will be given to you in the percentage amount for surrender valiance. You will not face any loss. So, you can claim the loan in the maturity period with a good benefit. Since you have paid the loan in due date, the bonus amount has been increased. The amount is rs.2001.50. If you pay in delay, the amount will increase year by year. You can claim it with a good benefit when you claim it in maturity time.\n","\n","SPEAKER_00 [197.0s - 200.0s]:\n","  📝 Ok. I will see.\n","\n","SPEAKER_01 [200.0s - 204.0s]:\n","  📝 Where do you pay? Online or branch office?\n","\n","SPEAKER_00 [204.0s - 207.0s]:\n","  📝 I pay at the office.\n","\n","SPEAKER_01 [207.0s - 217.0s]:\n","  📝 Ok. Don't delay. Pay as soon as possible. Because you have been paid late fees waiver off this month. You can pay without a bud. We'll try to build that in the future.\n","\n","SPEAKER_00 [217.0s - 219.0s]:\n","  📝 Ok madam. We'llophrenge.\n","\n","SPEAKER_01 [219.0s - 235.0s]:\n","  📝 We'll call back to you again. Your health declaration form is pending. Since you took time to get the dvd, We're asking for your form To know about your health condition. You can fill it in the branch office Or we can fill it and submit for you.\n","\n","SPEAKER_01 [239.0s - 251.0s]:\n","  📝 Ok. Plus your watch up number Is not linked to the policy. We will share a link. Click on it and register your whatsapp number. If there is an update in the mac flash incidence, you can check it through whatsapp.\n","\n","SPEAKER_00 [251.0s - 254.0s]:\n","  📝 I don't have a whatsapp number.\n","\n","SPEAKER_01 [254.0s - 255.0s]:\n","  📝 No, you don't have.\n","\n","SPEAKER_00 [255.0s - 256.0s]:\n","  📝 I will give you my number.\n","\n","💾 Results saved to: enhanced_transcription_results.json\n","✅ Processing completed successfully!\n"]}],"source":["def main():\n","    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Smart Audio Preprocessing\n","    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","        return\n","\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 2: Enhanced Whisper Transcription with anti-repetition\n","    try:\n","        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","        print(\"✅ Whisper transcription completed\")\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    # Step 3: Remove repetitive segments BEFORE post-processing\n","    print(\"\\n--- Removing Repetitive Segments ---\")\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    # Step 4: Post-process remaining transcription\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = post_process_text(segment['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    # Step 5: Speaker Diarization\n","    print(\"\\n--- Speaker Diarization ---\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU for diarization\")\n","\n","        diarization = pipeline(CLEAN_AUDIO_PATH)\n","        print(\"✅ Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"⚠️  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 6: Generate Enhanced Dialogue\n","    print(\"\\n--- Generating Dialogue ---\")\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Combine segments by speaker\n","    dialogue = []\n","    current_speaker = None\n","    current_texts = []\n","    current_start = 0\n","    current_end = 0\n","\n","    for segment in processed_segments:\n","        start = segment['start']\n","        end = segment['end']\n","        text = segment['text'].strip()\n","\n","        speaker = get_dominant_speaker(start, end, diarization)\n","\n","        # Merge consecutive segments from same speaker (within 3 seconds)\n","        if (speaker == current_speaker and\n","            current_speaker and\n","            (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            # Save previous speaker's dialogue\n","            if current_speaker and current_texts:\n","                combined_text = ' '.join(current_texts)\n","                # Final check for repetition in combined text\n","                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined_text,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","\n","            # Start new speaker segment\n","            current_speaker = speaker\n","            current_texts = [text]\n","            current_start = start\n","            current_end = end\n","\n","    # Add final segment\n","    if current_speaker and current_texts:\n","        combined_text = ' '.join(current_texts)\n","        if len(combined_text.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined_text,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Step 7: Display Results\n","    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  📝 {entry['text']}\")\n","\n","    # Step 8: Save Results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open('enhanced_transcription_results-t2.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n","    print(\"✅ Processing completed successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"RNePxaNd9O1I","executionInfo":{"status":"ok","timestamp":1753270775096,"user_tz":-330,"elapsed":5,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":14,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM/toRRE4o4FLw7ag7LRqA3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"36341d262ac44f118ede0b606ea5b7d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e43fd1310c20499cac08c4d87d190a9d","IPY_MODEL_31afa311ab5c4b17b76fade2d51abf19","IPY_MODEL_7e0e98adcc5543bd83352404baed3090"],"layout":"IPY_MODEL_dfa37da744254c47bdec1341ef0373f7"}},"e43fd1310c20499cac08c4d87d190a9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e4759d667c94fd5badb9877992f6fbd","placeholder":"​","style":"IPY_MODEL_17e2f8eb4ad84f3c897df24bf7295dfa","value":"config.yaml: 100%"}},"31afa311ab5c4b17b76fade2d51abf19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e3afb1c82bb433492fbe8593c74de54","max":469,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c3c4416891e461186a1ccb7964ca0f0","value":469}},"7e0e98adcc5543bd83352404baed3090":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08e73f41850144a9bc137fa3850f0ec5","placeholder":"​","style":"IPY_MODEL_2b1e42c944754d638dbf0b633c261a6e","value":" 469/469 [00:00&lt;00:00, 11.5kB/s]"}},"dfa37da744254c47bdec1341ef0373f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e4759d667c94fd5badb9877992f6fbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e2f8eb4ad84f3c897df24bf7295dfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e3afb1c82bb433492fbe8593c74de54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c3c4416891e461186a1ccb7964ca0f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08e73f41850144a9bc137fa3850f0ec5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b1e42c944754d638dbf0b633c261a6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cac0a5870a6442f392e5eb851b6c621e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_065c5dd2291a4967947a421efe9f9a60","IPY_MODEL_47cdcacfe3c346f5bfe2b44e728a5291","IPY_MODEL_8f130f81851f439a9744486cd7cb3906"],"layout":"IPY_MODEL_d34950fc824b46969559d37f03ffa5c3"}},"065c5dd2291a4967947a421efe9f9a60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f20cb14594a24b1291bb9ce354e818bd","placeholder":"​","style":"IPY_MODEL_5cf4a701a2474dc4a05670a7495252a0","value":"pytorch_model.bin: 100%"}},"47cdcacfe3c346f5bfe2b44e728a5291":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5818b18e6e5d47f8ae813ed6ce3c6057","max":5905440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b9efff2243a420d82d91ba082282449","value":5905440}},"8f130f81851f439a9744486cd7cb3906":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c3b6ea6902438fa94510c8c7a3b760","placeholder":"​","style":"IPY_MODEL_7c3f85e351404972ae9509f14d457881","value":" 5.91M/5.91M [00:00&lt;00:00, 8.93MB/s]"}},"d34950fc824b46969559d37f03ffa5c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f20cb14594a24b1291bb9ce354e818bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cf4a701a2474dc4a05670a7495252a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5818b18e6e5d47f8ae813ed6ce3c6057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b9efff2243a420d82d91ba082282449":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0c3b6ea6902438fa94510c8c7a3b760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c3f85e351404972ae9509f14d457881":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8c3896f84304ab597e0c943d2e94143":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9b91341697b4ad991b99ff446725783","IPY_MODEL_837045c447a64fa5ae923205f7c4a39a","IPY_MODEL_a2d3c677c7444d3fb9d140bd37a98163"],"layout":"IPY_MODEL_7fa27d8ae7284644aaaf941565abbf5f"}},"a9b91341697b4ad991b99ff446725783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d71a2f66fd14275a754325886d9d919","placeholder":"​","style":"IPY_MODEL_5f2f630a5a04425f8f44bfe70e1b3a63","value":"config.yaml: 100%"}},"837045c447a64fa5ae923205f7c4a39a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6811321855814bd79b4a80663cedc37c","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_866f775bd6164a3985f561ba8ba9df6f","value":399}},"a2d3c677c7444d3fb9d140bd37a98163":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f410d9b4624242af559bd16205c711","placeholder":"​","style":"IPY_MODEL_175aeb955ff94a239988c103f066d911","value":" 399/399 [00:00&lt;00:00, 27.0kB/s]"}},"7fa27d8ae7284644aaaf941565abbf5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d71a2f66fd14275a754325886d9d919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f2f630a5a04425f8f44bfe70e1b3a63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6811321855814bd79b4a80663cedc37c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"866f775bd6164a3985f561ba8ba9df6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01f410d9b4624242af559bd16205c711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"175aeb955ff94a239988c103f066d911":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb05b32c2a9a434ab081d2c2a7e05f5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47fc39a1d5f14261b79c293542f8c399","IPY_MODEL_8b123d5e3e484e1c882ac1547ebe01bb","IPY_MODEL_8d5dd7508fdf499ba1e988ffdfb43bc3"],"layout":"IPY_MODEL_976d49a2186a4107880fbb4014b52574"}},"47fc39a1d5f14261b79c293542f8c399":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855e8c380b0a49888032b1f47e3a0037","placeholder":"​","style":"IPY_MODEL_f02444eda3fa43bbafa279f8f6c7ad9c","value":"pytorch_model.bin: 100%"}},"8b123d5e3e484e1c882ac1547ebe01bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a43cd43f8c24e87bb16aedb10cd1319","max":26645418,"min":0,"orientation":"horizontal","style":"IPY_MODEL_642dcb71bdcf4790aedc203fe48704cb","value":26645418}},"8d5dd7508fdf499ba1e988ffdfb43bc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_343f3506225943fdbfaa5cadb6707285","placeholder":"​","style":"IPY_MODEL_ea01addbbb814699a36f2f8e6829ace3","value":" 26.6M/26.6M [00:00&lt;00:00, 31.4MB/s]"}},"976d49a2186a4107880fbb4014b52574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855e8c380b0a49888032b1f47e3a0037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f02444eda3fa43bbafa279f8f6c7ad9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a43cd43f8c24e87bb16aedb10cd1319":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642dcb71bdcf4790aedc203fe48704cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"343f3506225943fdbfaa5cadb6707285":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea01addbbb814699a36f2f8e6829ace3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcb90921c0d54a74a3a966dc5631fd5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4df7d13190b34790a9a666e7cca0a843","IPY_MODEL_5c6228fdafed462082630a66d2c2a932","IPY_MODEL_63163991c56c4e6f9b5e851813e865de"],"layout":"IPY_MODEL_44a9c73cb80d47e0800a1041cdf75c13"}},"4df7d13190b34790a9a666e7cca0a843":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d73ee330b1c40efb92761c88ceab0cb","placeholder":"​","style":"IPY_MODEL_66ac509774014dd39f5af3dbc85e3d1c","value":"config.yaml: 100%"}},"5c6228fdafed462082630a66d2c2a932":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afe744bd7f6a4d0895769b836575a672","max":221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9f49f19207f41f48070fe1e8f62aa4c","value":221}},"63163991c56c4e6f9b5e851813e865de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b06679d804b74e9ebc5dfea980d8db99","placeholder":"​","style":"IPY_MODEL_dff644b121d04c5b91e9ad11dccc3049","value":" 221/221 [00:00&lt;00:00, 8.68kB/s]"}},"44a9c73cb80d47e0800a1041cdf75c13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d73ee330b1c40efb92761c88ceab0cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ac509774014dd39f5af3dbc85e3d1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afe744bd7f6a4d0895769b836575a672":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9f49f19207f41f48070fe1e8f62aa4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b06679d804b74e9ebc5dfea980d8db99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dff644b121d04c5b91e9ad11dccc3049":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}