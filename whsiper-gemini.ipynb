{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":19249,"status":"ok","timestamp":1756964228327,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"APEFy1kWOh5S","outputId":"e5f7601a-4a58-4109-9b50-7fba68d08c5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-_b59tud2\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-_b59tud2\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.11.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton\u003e=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n","Requirement already satisfied: setuptools\u003e=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton\u003e=2-\u003eopenai-whisper==20250625) (75.2.0)\n","Requirement already satisfied: llvmlite\u003c0.44,\u003e=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba-\u003eopenai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex\u003e=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken-\u003eopenai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests\u003e=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken-\u003eopenai-whisper==20250625) (2.32.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (3.19.1)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (4.15.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch-\u003eopenai-whisper==20250625) (1.11.1.6)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.26.0-\u003etiktoken-\u003eopenai-whisper==20250625) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.26.0-\u003etiktoken-\u003eopenai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.26.0-\u003etiktoken-\u003eopenai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.26.0-\u003etiktoken-\u003eopenai-whisper==20250625) (2025.8.3)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch-\u003eopenai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch-\u003eopenai-whisper==20250625) (3.0.2)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=33c3bf938505926688b1e3da92b1b9591b2cd4e3e1f525389230f49ee1664369\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kwbusxpk/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n","Successfully built openai-whisper\n","Installing collected packages: openai-whisper\n","Successfully installed openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":25897,"status":"ok","timestamp":1756964254156,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ZZ-jXf6mOqp0","outputId":"6784d675-6fb1-4fd9-fe32-2824d87338bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting asteroid-filterbanks\u003e=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops\u003e=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub\u003e=0.13.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.34.4)\n","Collecting lightning\u003e=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.4-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: omegaconf\u003c3.0,\u003e=2.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core\u003e=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database\u003e=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics\u003e=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline\u003e=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning\u003e=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich\u003e=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver\u003e=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile\u003e=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain\u003e=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX\u003e=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch\u003e=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio) (2.8.0+cu126)\n","Collecting torch-audiomentations\u003e=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics\u003e=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (3.19.1)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=2.0.0-\u003epyannote.audio) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from asteroid-filterbanks\u003e=0.4-\u003epyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003e=0.13.0-\u003epyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003e=0.13.0-\u003epyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003e=0.13.0-\u003epyannote.audio) (2.32.4)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003e=0.13.0-\u003epyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003e=0.13.0-\u003epyannote.audio) (1.1.8)\n","Collecting lightning-utilities\u003c2.0,\u003e=0.10.0 (from lightning\u003e=2.0.1-\u003epyannote.audio)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Collecting pytorch-lightning (from lightning\u003e=2.0.1-\u003epyannote.audio)\n","  Downloading pytorch_lightning-2.5.4-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf\u003c3.0,\u003e=2.1-\u003epyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers\u003e=2.0.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.core\u003e=5.0.0-\u003epyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy\u003e=1.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.core\u003e=5.0.0-\u003epyannote.audio) (1.16.1)\n","Requirement already satisfied: pandas\u003e=0.19 in /usr/local/lib/python3.12/dist-packages (from pyannote.database\u003e=5.0.1-\u003epyannote.audio) (2.2.2)\n","Requirement already satisfied: typer\u003e=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.database\u003e=5.0.1-\u003epyannote.audio) (0.16.1)\n","Requirement already satisfied: scikit-learn\u003e=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics\u003e=3.2-\u003epyannote.audio) (1.6.1)\n","Collecting docopt\u003e=0.6.2 (from pyannote.metrics\u003e=3.2-\u003epyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate\u003e=0.7.7 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics\u003e=3.2-\u003epyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib\u003e=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics\u003e=3.2-\u003epyannote.audio) (3.10.0)\n","Collecting optuna\u003e=3.1 (from pyannote.pipeline\u003e=3.0.1-\u003epyannote.audio)\n","  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich\u003e=12.0.0-\u003epyannote.audio) (4.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich\u003e=12.0.0-\u003epyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi\u003e=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile\u003e=0.12.1-\u003epyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain\u003e=1.0.0-\u003epyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from speechbrain\u003e=1.0.0-\u003epyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain\u003e=1.0.0-\u003epyannote.audio) (0.2.1)\n","Requirement already satisfied: protobuf\u003e=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX\u003e=2.6-\u003epyannote.audio) (5.29.5)\n","Collecting julius\u003c0.3,\u003e=0.2.3 (from torch-audiomentations\u003e=0.11.0-\u003epyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift\u003e=1.2.2 (from torch-audiomentations\u003e=0.11.0-\u003epyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi\u003e=1.0-\u003esoundfile\u003e=0.12.1-\u003epyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (3.12.15)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=12.0.0-\u003epyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (4.59.1)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (1.4.9)\n","Requirement already satisfied: pillow\u003e=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (2.9.0.post0)\n","Collecting alembic\u003e=1.5.0 (from optuna\u003e=3.1-\u003epyannote.pipeline\u003e=3.0.1-\u003epyannote.audio)\n","  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna\u003e=3.1-\u003epyannote.pipeline\u003e=3.0.1-\u003epyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy\u003e=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna\u003e=3.1-\u003epyannote.pipeline\u003e=3.0.1-\u003epyannote.audio) (2.0.43)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=0.19-\u003epyannote.database\u003e=5.0.1-\u003epyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=0.19-\u003epyannote.database\u003e=5.0.1-\u003epyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=0.17.1-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (3.6.0)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch\u003e=2.0.0-\u003epyannote.audio) (1.3.0)\n","Collecting primePy\u003e=1.3 (from torch-pitch-shift\u003e=1.2.2-\u003etorch-audiomentations\u003e=0.11.0-\u003epyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click\u003e=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer\u003e=0.12.1-\u003epyannote.database\u003e=5.0.1-\u003epyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer\u003e=0.12.1-\u003epyannote.database\u003e=5.0.1-\u003epyannote.audio) (1.5.4)\n","Collecting ruamel.yaml\u003e=0.17.28 (from hyperpyyaml-\u003espeechbrain\u003e=1.0.0-\u003epyannote.audio)\n","  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch\u003e=2.0.0-\u003epyannote.audio) (3.0.2)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.13.0-\u003epyannote.audio) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.13.0-\u003epyannote.audio) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.13.0-\u003epyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface-hub\u003e=0.13.0-\u003epyannote.audio) (2025.8.3)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (6.6.4)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c2027.0,\u003e=2022.5.0-\u003elightning\u003e=2.0.1-\u003epyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna\u003e=3.1-\u003epyannote.pipeline\u003e=3.0.1-\u003epyannote.audio) (1.1.3)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=2.0.0-\u003epyannote.metrics\u003e=3.2-\u003epyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib\u003e=0.2.7 (from ruamel.yaml\u003e=0.17.28-\u003ehyperpyyaml-\u003espeechbrain\u003e=1.0.0-\u003epyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet\u003e=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy\u003e=1.4.2-\u003eoptuna\u003e=3.1-\u003epyannote.pipeline\u003e=3.0.1-\u003epyannote.audio) (3.2.4)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.4-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.2/825.2 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.4-py3-none-any.whl (829 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=1c685035ff6cf8383bbf64cab779c798d229e1a354917111ca87efda3e50385c\n","  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=105164bbc28cc1a855241233b66bccdb75084e6bdc1b7b6ac87a1f3b386b33d7\n","  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.5 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.4 lightning-utilities-0.15.2 optuna-4.5.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.4 pytorch-metric-learning-2.9.0 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.2\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28472,"status":"ok","timestamp":1756964282651,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"RYWjS4mkN3vD","outputId":"aa122a07-b044-4c36-91a1-43e291c62c31"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  torchaudio.list_audio_backends()\n"]}],"source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocess\n","import json\n","from datetime import datetime\n","from typing import Dict, List, Optional, Any"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1756964282707,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"dGOsrwY7N_jM"},"outputs":[],"source":["# Configuration\n","INPUT_AUDIO_PATH = \"call2.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":194,"status":"ok","timestamp":1756964289123,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"hRYwyQVAOCPw"},"outputs":[],"source":["class EnhancedInsuranceASR:\n","    def __init__(self):\n","        self.model = whisper.load_model(\"large-v3\")\n","\n","        # Enhanced domain-specific vocabulary from Gemini prompt\n","        self.domain_terms = {\n","            # Insurance companies\n","            'axis maxlife': 'Axis Maxlife Insurance',\n","            'axis max life': 'Axis Maxlife Insurance',\n","            'access max life': 'Axis Maxlife Insurance',\n","\n","            # Payment methods\n","            'g pay': 'Google Pay',\n","            'google pay': 'Google Pay',\n","            'phone pay': 'PhonePe',\n","            'phone pe': 'PhonePe',\n","            'pay tm': 'Paytm',\n","            'net banking': 'net banking',\n","\n","            # Insurance terms\n","            'some assured': 'sum assured',\n","            'fund value': 'fund values',\n","            'premium do': 'premium due',\n","            'do date': 'due date',\n","            'late fee': 'late fee',\n","            'surrender value': 'surrender value',\n","            'maturity value': 'maturity value',\n","            'health declaration form': 'health declaration form',\n","\n","            # Common names (only add if actually heard in audio)\n","            'jaya parkash': 'Jaya Parkash',\n","            'chandru': 'Chandru',\n","            'sneha': 'Sneha',\n","            'kowsalya': 'Kowsalya',\n","            'swathi': 'Swathi',\n","            'delphina': 'Delphina'\n","        }\n","\n","        # Enhanced initial prompt based on Gemini approach\n","        self.enhanced_prompt = (\n","            \"This is a customer support call for Axis Maxlife Insurance about policy renewal. \"\n","            \"The conversation includes policy numbers, due dates, fund values, sum assured amounts, \"\n","            \"late fees, and payment methods like Google Pay, PhonePe, Paytm, net banking, UPI, and cards. \"\n","            \"Speakers discuss premium due amounts, surrender values, maturity values, alternative mobile numbers, \"\n","            \"email IDs, and health declaration forms. Common outcomes include payment confirmations, \"\n","            \"callback requests, or providing policy information.\"\n","        )\n","\n","        # Entity extraction patterns\n","        self.entity_patterns = {\n","            'policy_number': [\n","                r'\\b(?:policy|policy number|pol no)[:\\s]*([A-Z0-9]{8,15})\\b',\n","                r'\\b([A-Z]{2,4}\\d{8,12})\\b'  # Common policy number formats\n","            ],\n","            'due_date': [\n","                r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b',\n","                r'\\b(\\d{1,2}\\s+(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\w*\\s+\\d{2,4})\\b'\n","            ],\n","            'premium_due_amount': [\n","                r'\\b(?:premium|amount|rupees?|rs\\.?|₹)\\s*(?:due|payable)?\\s*(?:is|of)?\\s*([₹]?\\s*\\d{1,6}(?:,\\d{3})*(?:\\.\\d{2})?)\\b',\n","                r'\\b([₹]\\s*\\d{1,6}(?:,\\d{3})*(?:\\.\\d{2})?)\\s*(?:rupees?|due|premium)\\b'\n","            ],\n","            'late_fee': [\n","                r'\\b(?:late fee|penalty|additional charge)\\s*(?:is|of)?\\s*([₹]?\\s*\\d{1,6}(?:,\\d{3})*(?:\\.\\d{2})?)\\b'\n","            ],\n","            'mobile_number': [\n","                r'\\b(?:mobile|phone|number)\\s*(?:is|:)?\\s*([6-9]\\d{9})\\b',\n","                r'\\b([6-9]\\d{9})\\b'\n","            ],\n","            'email': [\n","                r'\\b([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\b'\n","            ]\n","        }\n","\n","    def smart_audio_preprocessing(self, input_path: str, output_path: str) -\u003e bool:\n","        \"\"\"Enhanced audio preprocessing with insurance call optimization\"\"\"\n","        print(\"--- Optimized Insurance Call Audio Preprocessing ---\")\n","\n","        # Optimized for Indian English and regional accents in insurance calls\n","        ffmpeg_command = [\n","            \"ffmpeg\", \"-i\", input_path,\n","            \"-acodec\", \"pcm_s16le\",\n","            \"-ac\", \"1\",  # Mono\n","            \"-ar\", \"16000\",  # Optimal for Whisper\n","            \"-af\", \"loudnorm=I=-16:TP=-1.5:LRA=11,highpass=f=85,lowpass=f=7500,afftdn=nr=8\",\n","            \"-y\", output_path\n","        ]\n","\n","        try:\n","            result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","            print(\"✅ Insurance call preprocessing successful\")\n","            return True\n","        except subprocess.CalledProcessError as e:\n","            print(f\"❌ Preprocessing failed: {e}\")\n","            return False\n","\n","    def detect_language(self, audio_path: str) -\u003e str:\n","        \"\"\"Detect language from predefined set\"\"\"\n","        # Quick detection using Whisper\n","        result = self.model.transcribe(audio_path, language=None, fp16=False, task=\"transcribe\")\n","        detected_lang = result.get('language', 'en')\n","\n","        # Map to Gemini's expected language format\n","        lang_mapping = {\n","            'ta': 'Tamil',\n","            'te': 'Telugu',\n","            'hi': 'Hindi',\n","            'ml': 'Malayalam',\n","            'kn': 'Kannada',\n","            'en': 'English'  # Fallback\n","        }\n","\n","        return lang_mapping.get(detected_lang, 'Hindi')  # Default to Hindi if uncertain\n","\n","    def enhanced_whisper_transcription(self, audio_path: str, detected_language: str) -\u003e Dict[str, Any]:\n","        \"\"\"Enhanced transcription with language-specific optimization\"\"\"\n","        print(f\"--- Enhanced Whisper Transcription (Language: {detected_language}) ---\")\n","\n","        # Map language back to Whisper format\n","        whisper_lang_map = {\n","            'Tamil': 'ta',\n","            'Telugu': 'te',\n","            'Hindi': 'hi',\n","            'Malayalam': 'ml',\n","            'Kannada': 'kn',\n","            'English': 'en'\n","        }\n","\n","        whisper_lang = whisper_lang_map.get(detected_language, 'hi')\n","\n","        # Enhanced parameters for insurance calls\n","        result = self.model.transcribe(\n","            audio_path,\n","            language=whisper_lang,\n","            task=\"transcribe\",  # Keep in original language first\n","            temperature=0.0,\n","            beam_size=5,\n","            patience=1.5,\n","            condition_on_previous_text=False,\n","            no_speech_threshold=0.6,  # More sensitive for quiet speakers\n","            compression_ratio_threshold=1.8,\n","            logprob_threshold=-0.4,\n","            word_timestamps=True,  # Enable for better segmentation\n","            initial_prompt=self.enhanced_prompt,\n","            verbose=True,\n","        )\n","\n","        # If not English, also get English translation\n","        english_result = None\n","        if detected_language != 'English':\n","            english_result = self.model.transcribe(\n","                audio_path,\n","                language=whisper_lang,\n","                task=\"translate\",  # Translate to English\n","                temperature=0.0,\n","                beam_size=5,\n","                patience=1.5,\n","                condition_on_previous_text=False,\n","                no_speech_threshold=0.6,\n","                compression_ratio_threshold=1.8,\n","                logprob_threshold=-0.4,\n","                initial_prompt=self.enhanced_prompt,\n","                verbose=True,\n","            )\n","\n","        return {\n","            'source_result': result,\n","            'english_result': english_result,\n","            'detected_language': detected_language\n","        }\n","\n","    def extract_entities(self, text: str) -\u003e Dict[str, Any]:\n","        \"\"\"Extract insurance-specific entities from transcription\"\"\"\n","        entities = {\n","            \"policy_number\": None,\n","            \"due_date\": None,\n","            \"premium_due_amount\": None,\n","            \"late_fee\": None,\n","            \"payment_method\": None,\n","            \"fund_values\": None,\n","            \"sum_assured\": None,\n","            \"surrender_value\": None,\n","            \"maturity_value\": None,\n","            \"alternative_mobile\": None,\n","            \"email_id\": None,\n","            \"needs_health_declaration_form\": False,\n","            \"outcome\": None\n","        }\n","\n","        text_lower = text.lower()\n","\n","        # Extract structured entities using patterns\n","        for entity, patterns in self.entity_patterns.items():\n","            for pattern in patterns:\n","                match = re.search(pattern, text_lower, re.IGNORECASE)\n","                if match:\n","                    if entity == 'policy_number':\n","                        entities['policy_number'] = match.group(1).upper()\n","                    elif entity == 'due_date':\n","                        entities['due_date'] = match.group(1)\n","                    elif entity in ['premium_due_amount', 'late_fee']:\n","                        amount = match.group(1).replace('₹', '').replace(',', '').strip()\n","                        entities[entity] = amount\n","                    elif entity == 'mobile_number':\n","                        entities['alternative_mobile'] = match.group(1)\n","                    elif entity == 'email':\n","                        entities['email_id'] = match.group(1)\n","                    break\n","\n","        # Detect payment methods\n","        payment_methods = {\n","            'google pay': 'Google Pay',\n","            'g pay': 'Google Pay',\n","            'phonepe': 'PhonePe',\n","            'phone pe': 'PhonePe',\n","            'paytm': 'Paytm',\n","            'pay tm': 'Paytm',\n","            'net banking': 'net banking',\n","            'netbanking': 'net banking',\n","            'upi': 'UPI',\n","            'card': 'card',\n","            'cash': 'cash'\n","        }\n","\n","        for method_key, method_value in payment_methods.items():\n","            if method_key in text_lower:\n","                entities['payment_method'] = method_value\n","                break\n","\n","        # Detect outcome\n","        if any(word in text_lower for word in ['paid', 'payment done', 'successful']):\n","            entities['outcome'] = 'paid_now'\n","        elif any(word in text_lower for word in ['will pay', 'pay later', 'tomorrow']):\n","            entities['outcome'] = 'will_pay_later'\n","        elif any(word in text_lower for word in ['callback', 'call back', 'call later']):\n","            entities['outcome'] = 'callback_required'\n","        elif any(word in text_lower for word in ['declined', 'cannot pay', 'not possible']):\n","            entities['outcome'] = 'declined'\n","        else:\n","            entities['outcome'] = 'info_given'\n","\n","        # Check for health declaration form\n","        if 'health declaration' in text_lower or 'medical form' in text_lower:\n","            entities['needs_health_declaration_form'] = True\n","\n","        return entities\n","\n","    def post_process_insurance_text(self, text: str) -\u003e str:\n","        \"\"\"Enhanced post-processing for insurance domain\"\"\"\n","        if not text:\n","            return \"\"\n","\n","        # Apply domain-specific corrections\n","        text_lower = text.lower()\n","        for wrong, correct in self.domain_terms.items():\n","            text_lower = text_lower.replace(wrong, correct)\n","\n","        # Enhanced currency formatting\n","        text_lower = re.sub(r'\\brs[.]?\\s*', '₹', text_lower)\n","        text_lower = re.sub(r'\\brupees?\\s*(\\d+)', r'₹\\1', text_lower)\n","\n","        # Remove excessive repetitions (enhanced)\n","        words = text_lower.split()\n","        cleaned_words = []\n","        i = 0\n","        while i \u003c len(words):\n","            current_word = words[i].lower()\n","            repetition_count = 1\n","            j = i + 1\n","            while j \u003c len(words) and words[j].lower() == current_word:\n","                repetition_count += 1\n","                j += 1\n","            # Keep max 2 repetitions for emphasis, 1 for excessive repetition\n","            keep_count = min(repetition_count, 2) if repetition_count \u003c= 3 else 1\n","            for _ in range(keep_count):\n","                cleaned_words.append(words[i])\n","            i += repetition_count\n","\n","        text_lower = ' '.join(cleaned_words)\n","\n","        # Clean up spacing and punctuation\n","        text_lower = re.sub(r'\\s{2,}', ' ', text_lower)\n","        text_lower = re.sub(r'\\s+([,.!?])', r'\\1', text_lower)\n","\n","        # Capitalize sentences\n","        text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n","                           lambda m: m.group(1) + m.group(2).upper(),\n","                           text_lower)\n","\n","        return text_lower.strip()\n","\n","    def create_gemini_style_output(self, transcription_result: Dict[str, Any],\n","                                 dialogue: List[Dict[str, Any]]) -\u003e Dict[str, Any]:\n","        \"\"\"Create output matching Gemini's JSON structure\"\"\"\n","\n","        source_result = transcription_result['source_result']\n","        english_result = transcription_result['english_result']\n","        detected_language = transcription_result['detected_language']\n","\n","        # Combine all text for entity extraction\n","        full_source_text = ' '.join([seg['text'] for seg in source_result['segments']])\n","        full_english_text = ' '.join([seg['text'] for seg in english_result['segments']]) if english_result else full_source_text\n","\n","        # Extract entities from English text (more reliable)\n","        entities = self.extract_entities(full_english_text)\n","\n","        # Create dialogue with timestamps\n","        formatted_dialogue = []\n","        for entry in dialogue:\n","            start_min = int(entry['start_time'] // 60)\n","            start_sec = int(entry['start_time'] % 60)\n","            timestamp = f\"{start_min:02d}:{start_sec:02d}\"\n","\n","            # Map speaker to Agent/Customer (simple heuristic)\n","            speaker_name = entry['speaker']\n","            if 'speaker_00' in speaker_name.lower() or '0' in speaker_name:\n","                speaker_label = \"Agent\"\n","            elif 'speaker_01' in speaker_name.lower() or '1' in speaker_name:\n","                speaker_label = \"Customer\"\n","            else:\n","                speaker_label = speaker_name\n","\n","            formatted_dialogue.append({\n","                \"ts\": timestamp,\n","                \"speaker\": speaker_label,\n","                \"text\": entry['text']\n","            })\n","\n","        # Quality flags\n","        quality_flags = {\n","            \"noisy_audio\": len([seg for seg in source_result['segments'] if seg.get('no_speech_prob', 0) \u003e 0.8]) \u003e 0,\n","            \"mixed_language\": detected_language == 'English' and any(word in full_source_text.lower()\n","                                                                   for word in ['tamil', 'hindi', 'telugu']),\n","            \"missing_key_info\": []\n","        }\n","\n","        # Check for missing key information\n","        if not entities['policy_number']:\n","            quality_flags['missing_key_info'].append('policy_number')\n","        if not entities['premium_due_amount']:\n","            quality_flags['missing_key_info'].append('premium_due_amount')\n","        if not entities['payment_method'] and entities['outcome'] == 'paid_now':\n","            quality_flags['missing_key_info'].append('payment_method')\n","\n","        return {\n","            \"source_language\": detected_language,\n","            \"transcript_source\": full_source_text,\n","            \"transcript_english\": full_english_text,\n","            \"dialogue\": formatted_dialogue,\n","            \"entities\": entities,\n","            \"quality_flags\": quality_flags,\n","            \"processing_metadata\": {\n","                \"model_used\": \"whisper-large-v3\",\n","                \"processing_time\": datetime.now().isoformat(),\n","                \"total_duration\": source_result.get('duration', 0),\n","                \"total_speakers\": len(set(d['speaker'] for d in dialogue))\n","            }\n","        }\n","\n","    def process_audio(self, audio_path: str) -\u003e Dict[str, Any]:\n","        \"\"\"Main processing pipeline\"\"\"\n","        print(\"🎯 Starting Enhanced Insurance ASR Pipeline\")\n","        print(\"=\" * 60)\n","\n","        # Step 1: Audio preprocessing\n","        if not self.smart_audio_preprocessing(audio_path, CLEAN_AUDIO_PATH):\n","            raise Exception(\"Audio preprocessing failed\")\n","\n","        # Step 2: Language detection\n","        detected_language = self.detect_language(CLEAN_AUDIO_PATH)\n","        print(f\"🌐 Detected language: {detected_language}\")\n","\n","        # Step 3: Enhanced transcription\n","        transcription_result = self.enhanced_whisper_transcription(CLEAN_AUDIO_PATH, detected_language)\n","\n","        # Step 4: Process segments and remove repetitions\n","        source_segments = transcription_result['source_result']['segments']\n","        cleaned_segments = self.detect_and_remove_repetitions(source_segments)\n","\n","        # Step 5: Post-process text\n","        processed_segments = []\n","        for segment in cleaned_segments:\n","            processed_text = self.post_process_insurance_text(segment['text'])\n","            if processed_text.strip() and len(processed_text.strip()) \u003e 5:\n","                segment_copy = segment.copy()\n","                segment_copy['text'] = processed_text\n","                processed_segments.append(segment_copy)\n","\n","        # Step 6: Speaker diarization\n","        dialogue = self.create_dialogue_from_segments(processed_segments)\n","\n","        # Step 7: Create Gemini-style output\n","        final_output = self.create_gemini_style_output(transcription_result, dialogue)\n","\n","        return final_output\n","\n","    def detect_and_remove_repetitions(self, segments: List[Dict[str, Any]]) -\u003e List[Dict[str, Any]]:\n","        \"\"\"Enhanced repetition detection for insurance calls\"\"\"\n","        # Your existing repetition detection logic here\n","        # (I'll keep your implementation as it's already quite good)\n","        print(\"🔍 Starting enhanced repetition detection...\")\n","        cleaned_segments = []\n","\n","        for segment in segments:\n","            text = segment['text'].strip()\n","            words = text.split()\n","\n","            if len(words) \u003c 2:\n","                continue\n","\n","            # Check for word dominance\n","            word_counts = {}\n","            for word in words:\n","                word_lower = word.lower().strip('.,!?')\n","                word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","            max_word_count = max(word_counts.values()) if word_counts else 0\n","            word_dominance = max_word_count / len(words) if words else 0\n","\n","            if word_dominance \u003e 0.4:\n","                continue\n","\n","            # Check consecutive repetitions\n","            max_consecutive = 0\n","            consecutive_repeats = 0\n","            for j in range(1, len(words)):\n","                if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                    consecutive_repeats += 1\n","                    max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","                else:\n","                    consecutive_repeats = 0\n","\n","            if max_consecutive \u003e 3:\n","                continue\n","\n","            cleaned_segments.append(segment)\n","\n","        print(f\"📊 Cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","        return cleaned_segments\n","\n","    def create_dialogue_from_segments(self, segments: List[Dict[str, Any]]) -\u003e List[Dict[str, Any]]:\n","        \"\"\"Create dialogue structure from processed segments\"\"\"\n","        # Simple implementation - you can enhance with your diarization logic\n","        dialogue = []\n","\n","        for i, segment in enumerate(segments):\n","            # Simple speaker alternation (enhance with actual diarization)\n","            speaker = f\"Speaker_{i % 2}\"\n","\n","            dialogue.append({\n","                'speaker': speaker,\n","                'text': segment['text'],\n","                'start_time': segment.get('start', i * 3),  # Approximate timing\n","                'end_time': segment.get('end', (i + 1) * 3)\n","            })\n","\n","        return dialogue"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Tv5LUXbiOVP8"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|█████████████████████████████████████| 2.88G/2.88G [01:20\u003c00:00, 38.3MiB/s]\n"]},{"name":"stdout","output_type":"stream","text":["🎯 Starting Enhanced Insurance ASR Pipeline\n","============================================================\n","--- Optimized Insurance Call Audio Preprocessing ---\n","✅ Insurance call preprocessing successful\n","🌐 Detected language: Tamil\n","--- Enhanced Whisper Transcription (Language: Tamil) ---\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]}],"source":["def main():\n","    \"\"\"Main execution function\"\"\"\n","    try:\n","        asr = EnhancedInsuranceASR()\n","        result = asr.process_audio(INPUT_AUDIO_PATH)\n","\n","        # Save results in Gemini-compatible format\n","        with open('insurance_call_analysis.json', 'w', encoding='utf-8') as f:\n","            json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"📋 EXTRACTED INFORMATION\")\n","        print(\"=\" * 60)\n","        print(f\"Language: {result['source_language']}\")\n","        print(f\"Policy Number: {result['entities']['policy_number']}\")\n","        print(f\"Premium Due: {result['entities']['premium_due_amount']}\")\n","        print(f\"Payment Method: {result['entities']['payment_method']}\")\n","        print(f\"Outcome: {result['entities']['outcome']}\")\n","\n","        print(\"\\n\" + \"🎭 DIALOGUE PREVIEW\" + \"=\" * 40)\n","        for entry in result['dialogue'][:10]:  # Show first 10 exchanges\n","            print(f\"\\n[{entry['ts']}] {entry['speaker']}:\")\n","            print(f\"  📝 {entry['text']}\")\n","\n","        print(f\"\\n💾 Complete results saved to: insurance_call_analysis.json\")\n","        print(\"✅ Enhanced processing completed successfully!\")\n","\n","    except Exception as e:\n","        print(f\"❌ Processing failed: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNPNFLHP50AmMkofbB8xylk","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}