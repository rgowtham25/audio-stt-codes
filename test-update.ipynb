{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPXgRKVtc6/rZAGb2KZVd5n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":107023,"status":"ok","timestamp":1753766341925,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"APEFy1kWOh5S","outputId":"147bd402-c692-4542-8fb6-e8968dc8b7d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-fongbyha\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-fongbyha\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=61ff886644ec82898e6da89b58d9dcf0ded4461bcb7744efccbfa7d44a1ce2bf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fc9co9gq/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":17748,"status":"ok","timestamp":1753766359637,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ZZ-jXf6mOqp0","outputId":"13d1d224-3407-4a46-e6b0-f4a01857cdc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.33.5)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.15.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.12.14)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.0-py3-none-any.whl (29 kB)\n","Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=210e4b1894c30955ab59c2774b7fd0ebf4c8f0af896d4b10dfeaaa465bf688eb\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=1a5419646d741c2fb997bde77c498a5c99d802544233ad7704f15b2c321183f7\n","  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.15.0 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.0\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocess\n","import json\n","import librosa\n","import numpy as np"],"metadata":{"id":"FNJ6YYlogD98","executionInfo":{"status":"ok","timestamp":1753766384228,"user_tz":-330,"elapsed":24574,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class Config:\n","    INPUT_AUDIO_PATH = \"call4.wav\"\n","    CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","    HUGGING_FACE_ACCESS_TOKEN = \"hf_\"\n","\n","    # Single optimized prompt (optional - you can remove this)\n","    INSURANCE_PROMPT = (\n","        \"This is customer support for Axis Max Life Insurance. \"\n","        \"Keywords: policy number, due date, fund value, sum assured, \"\n","        \"Google Pay, PhonePe, Paytm, netbanking, premium, late fee.\"\n","    )"],"metadata":{"id":"oGyAGSxym5V7","executionInfo":{"status":"ok","timestamp":1753766384265,"user_tz":-330,"elapsed":21,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model = whisper.load_model(\"large\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3Lk0UUCnXcy","executionInfo":{"status":"ok","timestamp":1753766478695,"user_tz":-330,"elapsed":94422,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"39f45f3c-fb71-46ce-fb5f-38b28018579f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [01:01<00:00, 49.9MiB/s]\n"]}]},{"cell_type":"code","source":["def get_audio_duration(audio_path):\n","    \"\"\"Get audio duration using ffprobe\"\"\"\n","    try:\n","        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","        return float(result.stdout.strip())\n","    except Exception as e:\n","        print(f\"Could not get duration: {e}\")\n","        return 0\n","\n","def quick_audio_check(audio_path):\n","    \"\"\"Quick audio quality check without heavy processing\"\"\"\n","    try:\n","        # Just check basic properties\n","        duration = get_audio_duration(audio_path)\n","\n","        # Simple file size vs duration ratio (rough quality indicator)\n","        file_size = os.path.getsize(audio_path)\n","        size_per_second = file_size / duration if duration > 0 else 0\n","\n","        # Very rough heuristic: smaller files might be lower quality\n","        if size_per_second < 8000:  # bytes per second\n","            return \"low_quality\"\n","        elif size_per_second < 16000:\n","            return \"medium_quality\"\n","        else:\n","            return \"good_quality\"\n","\n","    except Exception as e:\n","        print(f\"Quick audio check failed: {e}\")\n","        return \"unknown_quality\"\n","\n","def fast_preprocessing(input_path, output_path, quality_hint=\"unknown_quality\"):\n","    \"\"\"Fast, single-pass preprocessing\"\"\"\n","    print(f\"ğŸ”§ Fast preprocessing for {quality_hint} audio\")\n","\n","    # Choose preprocessing based on simple quality hint\n","    if quality_hint == \"low_quality\":\n","        # More aggressive for poor quality\n","        af_filter = (\n","            \"highpass=f=100,\"\n","            \"lowpass=f=7500,\"\n","            \"loudnorm=I=-20:TP=-2,\"\n","            \"afftdn=nr=15\"\n","        )\n","    elif quality_hint == \"medium_quality\":\n","        # Moderate processing\n","        af_filter = (\n","            \"highpass=f=80,\"\n","            \"lowpass=f=8000,\"\n","            \"loudnorm=I=-23:TP=-2,\"\n","            \"afftdn=nr=10\"\n","        )\n","    else:\n","        # Minimal processing for good quality\n","        af_filter = \"loudnorm=I=-23:TP=-2\"\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", af_filter,\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"âœ… Fast preprocessing completed\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"âŒ Fast preprocessing failed: {e}\")\n","        return False"],"metadata":{"id":"fCaEIiQfm88g","executionInfo":{"status":"ok","timestamp":1753766384269,"user_tz":-330,"elapsed":21,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def optimized_whisper_transcription(model, audio_path):\n","    \"\"\"Single-pass optimized Whisper transcription\"\"\"\n","    print(\"ğŸ¯ Running optimized Whisper transcription\")\n","\n","    # Single optimal parameter set (no multiple strategies)\n","    result = model.transcribe(\n","        audio_path,\n","        language=\"ta\",  # Tamil with English translation\n","        task=\"translate\",\n","        temperature=0.0,  # Deterministic\n","        beam_size=5,     # Good balance of speed vs accuracy\n","        patience=1.0,    # Standard patience\n","        condition_on_previous_text=True,  # Use context\n","        no_speech_threshold=0.6,\n","        compression_ratio_threshold=2.4,\n","        logprob_threshold=-1.0,\n","        word_timestamps=False,  # Disable to save time\n","        initial_prompt=Config.INSURANCE_PROMPT,  # Optional domain hint\n","        verbose=True,   # Reduce output\n","    )\n","\n","    print(\"âœ… Whisper transcription completed\")\n","    return result\n"],"metadata":{"id":"AA6RxnEEm9Zf","executionInfo":{"status":"ok","timestamp":1753766576036,"user_tz":-330,"elapsed":6,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def smart_chunking_for_long_audio(model, audio_path, max_duration=300):\n","    \"\"\"Only use chunking for very long audio (>5 minutes)\"\"\"\n","    duration = get_audio_duration(audio_path)\n","\n","    if duration <= max_duration:\n","        # Short audio - process normally\n","        return optimized_whisper_transcription(model, audio_path)\n","\n","    print(f\"ğŸ”„ Long audio detected ({duration:.1f}s). Using smart chunking...\")\n","\n","    chunk_duration = 120  # 2-minute chunks\n","    overlap = 10  # 10-second overlap\n","    all_segments = []\n","\n","    for start_time in range(0, int(duration), chunk_duration - overlap):\n","        end_time = min(start_time + chunk_duration, duration)\n","\n","        print(f\"Processing chunk: {start_time}s - {end_time}s\")\n","\n","        # Extract chunk\n","        chunk_path = f\"temp_chunk_{start_time}.wav\"\n","        extract_command = [\n","            \"ffmpeg\", \"-i\", audio_path,\n","            \"-ss\", str(start_time),\n","            \"-t\", str(end_time - start_time),\n","            \"-acodec\", \"pcm_s16le\",\n","            \"-ar\", \"16000\",\n","            \"-ac\", \"1\",\n","            \"-y\", chunk_path\n","        ]\n","\n","        try:\n","            subprocess.run(extract_command, check=True, capture_output=True)\n","\n","            # Transcribe chunk\n","            chunk_result = optimized_whisper_transcription(model, chunk_path)\n","\n","            # Adjust timestamps and add to all segments\n","            for segment in chunk_result.get('segments', []):\n","                segment['start'] += start_time\n","                segment['end'] += start_time\n","                all_segments.append(segment)\n","\n","            # Clean up\n","            os.remove(chunk_path)\n","\n","        except Exception as e:\n","            print(f\"Chunk processing failed for {start_time}-{end_time}: {e}\")\n","            continue\n","\n","    return {'segments': all_segments}"],"metadata":{"id":"hn9M6QC1nBB9","executionInfo":{"status":"ok","timestamp":1753766576055,"user_tz":-330,"elapsed":5,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def lightweight_repetition_removal(segments):\n","    \"\"\"Lightweight repetition removal (faster than aggressive version)\"\"\"\n","    print(\"ğŸ” Quick repetition removal...\")\n","\n","    cleaned_segments = []\n","\n","    for segment in segments:\n","        text = segment['text'].strip()\n","        words = text.split()\n","\n","        # Skip very short segments\n","        if len(words) < 2:\n","            continue\n","\n","        # Simple checks only:\n","\n","        # 1. Check for excessive same-word repetition\n","        word_counts = {}\n","        for word in words:\n","            word_lower = word.lower().strip('.,!?')\n","            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","        max_word_count = max(word_counts.values()) if word_counts else 0\n","        word_dominance = max_word_count / len(words) if words else 0\n","\n","        if word_dominance > 0.5:  # If any word is >50% of segment\n","            continue\n","\n","        # 2. Check for immediate consecutive repetitions\n","        max_consecutive = 0\n","        consecutive = 0\n","\n","        for i in range(1, len(words)):\n","            if words[i].lower() == words[i-1].lower():\n","                consecutive += 1\n","                max_consecutive = max(max_consecutive, consecutive + 1)\n","            else:\n","                consecutive = 0\n","\n","        if max_consecutive > 4:  # More than 4 consecutive same words\n","            continue\n","\n","        # Segment passed basic checks\n","        cleaned_segments.append(segment)\n","\n","    removed_count = len(segments) - len(cleaned_segments)\n","    print(f\"ğŸ“Š Quick cleaning: {len(segments)} â†’ {len(cleaned_segments)} segments\")\n","    print(f\"ğŸ—‘ï¸  Removed {removed_count} problematic segments\")\n","\n","    return cleaned_segments\n"],"metadata":{"id":"zZ6YUqXXnDjh","executionInfo":{"status":"ok","timestamp":1753766576200,"user_tz":-330,"elapsed":140,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def simple_post_process_text(text):\n","    \"\"\"Simplified text post-processing (faster)\"\"\"\n","    if not text:\n","        return \"\"\n","\n","    # Basic cleanup only\n","    text = text.strip()\n","\n","    # Remove excessive spaces\n","    text = re.sub(r'\\s{2,}', ' ', text)\n","\n","    # Simple domain corrections (most common ones only)\n","    corrections = {\n","        'axis max life': 'Axis Max Life',\n","        'g pay': 'GPay',\n","        'phone pe': 'PhonePe',\n","        'pay tm': 'Paytm',\n","        'some assured': 'sum assured',\n","        'do date': 'due date',\n","    }\n","\n","    text_lower = text.lower()\n","    for wrong, correct in corrections.items():\n","        text_lower = text_lower.replace(wrong, correct)\n","\n","    # Capitalize first letter\n","    if text_lower:\n","        text_lower = text_lower[0].upper() + text_lower[1:]\n","\n","    return text_lower"],"metadata":{"id":"PCkiWG-ynFo7","executionInfo":{"status":"ok","timestamp":1753766576216,"user_tz":-330,"elapsed":123,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def fast_main():\n","    \"\"\"Fast, optimized main pipeline\"\"\"\n","    print(\"ğŸš€ Starting FAST Audio Processing Pipeline\")\n","    print(\"=\" * 50)\n","\n","    # Step 1: Quick audio quality check (optional)\n","    quality_hint = quick_audio_check(Config.INPUT_AUDIO_PATH)\n","    print(f\"Audio quality hint: {quality_hint}\")\n","\n","    # Step 2: Fast preprocessing\n","    if not fast_preprocessing(Config.INPUT_AUDIO_PATH, Config.CLEAN_AUDIO_PATH, quality_hint):\n","        print(\"âŒ Audio preprocessing failed. Exiting.\")\n","        return\n","\n","    # Step 3: Load model once\n","    print(\"ğŸ“‚ Loading Whisper model...\")\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 4: Smart transcription (chunking only for very long audio)\n","    print(\"ğŸ¤ Starting transcription...\")\n","    whisper_result = smart_chunking_for_long_audio(model, Config.CLEAN_AUDIO_PATH)\n","\n","    # Step 5: Quick repetition removal\n","    if 'segments' in whisper_result:\n","        cleaned_segments = lightweight_repetition_removal(whisper_result['segments'])\n","    else:\n","        cleaned_segments = []\n","\n","    # Step 6: Simple post-processing\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = simple_post_process_text(segment['text'])\n","        if processed_text and len(processed_text) > 3:\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    # Step 7: Speaker diarization (keep your existing code)\n","    print(\"ğŸ‘¥ Starting speaker diarization...\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=Config.HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","\n","        diarization = pipeline(Config.CLEAN_AUDIO_PATH)\n","        print(\"âœ… Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"âš ï¸  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 8: Generate dialogue (simplified)\n","    dialogue = []\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Simple dialogue generation\n","    for segment in processed_segments:\n","        speaker = get_dominant_speaker(segment['start'], segment['end'], diarization)\n","        dialogue.append({\n","            'speaker': speaker,\n","            'text': segment['text'],\n","            'start_time': segment['start'],\n","            'end_time': segment['end']\n","        })\n","\n","    # Step 9: Display results\n","    print(\"\\n\" + \"ğŸ­ DIALOGUE OUTPUT\" + \"=\" * 30)\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  ğŸ“ {entry['text']}\")\n","\n","    # Step 10: Save results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': get_audio_duration(Config.INPUT_AUDIO_PATH),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'processing_mode': 'fast_optimized'\n","        },\n","        'dialogue': dialogue\n","    }\n","\n","    with open('fast_transcription_results.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\nğŸ’¾ Results saved to: fast_transcription_results.json\")\n","    print(\"âœ… Fast processing completed!\")\n","\n","if __name__ == \"__main__\":\n","    fast_main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5QobMEAnJlp","executionInfo":{"status":"ok","timestamp":1753766627342,"user_tz":-330,"elapsed":51123,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"96eae224-7601-466d-edb0-345ec4452e18"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Starting FAST Audio Processing Pipeline\n","==================================================\n","Audio quality hint: good_quality\n","ğŸ”§ Fast preprocessing for good_quality audio\n","âœ… Fast preprocessing completed\n","ğŸ“‚ Loading Whisper model...\n","ğŸ¤ Starting transcription...\n","ğŸ¯ Running optimized Whisper transcription\n","[00:00.000 --> 00:30.000]  Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello!\n","[00:30.000 --> 00:37.000]  Hello! Hello! Hello! Hello! Hello!\n","[01:00.000 --> 01:02.000]  Hello! Hello! Hello! Hello!\n","[01:02.000 --> 01:04.000]  Hello! Hello! Hello! Hello!\n","[01:04.000 --> 01:06.000]  Hello! Hello! Hello!\n","[01:06.000 --> 01:08.000]  Hello! Hello! Hello!\n","[01:08.000 --> 01:10.000]  Hello! Hello! Hello!\n","[01:10.000 --> 01:12.000]  Hello! Hello! Hello!\n","[01:12.000 --> 01:14.000]  Hello! Hello! Hello!\n","[01:14.000 --> 01:16.000]  Hello! Hello! Hello!\n","[01:16.000 --> 01:18.000]  Hello! Hello! Hello!\n","[01:18.000 --> 01:20.000]  Hello! Hello! Hello!\n","[01:20.000 --> 01:22.000]  Hello! Hello! Hello!\n","[01:22.000 --> 01:24.000]  Hello! Hello! Hello!\n","[01:24.000 --> 01:26.000]  Hello! Hello! Hello!\n","[01:26.000 --> 01:28.000]  Hello! Hello! Hello!\n","[01:28.000 --> 01:30.000]  Hello! Hello! Hello!\n","[01:30.000 --> 01:32.000]  Hello! Hello! Hello!\n","[01:32.000 --> 01:34.000]  Hello! Hello! Hello!\n","[01:34.000 --> 01:36.000]  Hello! Hello! Hello!\n","[01:36.000 --> 01:38.000]  Hello! Hello!\n","[01:38.000 --> 01:40.000]  Hello! Hello!\n","[01:40.000 --> 01:42.000]  Hello! Hello! Hello!\n","[01:42.000 --> 01:44.000]  Hello! Hello!\n","[01:44.000 --> 01:46.000]  Hello! Hello!\n","[01:46.000 --> 01:48.000]  Hello! Hello!\n","[01:48.000 --> 01:50.000]  Hello! Hello!\n","[01:50.000 --> 01:52.000]  Hello! Hello!\n","[01:52.000 --> 01:54.000]  Hello! Hello!\n","[01:54.000 --> 01:56.000]  Hello! Hello!\n","[01:56.000 --> 01:58.000]  Hello!\n","[01:58.000 --> 02:00.000]  Hello!\n","[02:00.000 --> 02:02.000]  Hello!\n","[02:02.000 --> 02:04.000]  Hello!\n","[02:04.000 --> 02:06.000]  Hello!\n","[02:06.000 --> 02:08.000]  Hello!\n","[02:08.000 --> 02:10.000]  Hello!\n","[02:10.000 --> 02:12.000]  Hello!\n","[02:12.000 --> 02:14.000]  Hello!\n","[02:14.000 --> 02:16.000]  Hello!\n","[02:16.000 --> 02:18.000]  Hello!\n","[02:18.000 --> 02:20.000]  Hello!\n","[02:20.000 --> 02:22.000]  Hello!\n","[02:22.000 --> 02:24.000]  Hello!\n","[02:24.000 --> 02:26.000]  Hello!\n","[02:26.000 --> 02:28.000]  Hello!\n","[02:28.000 --> 02:36.000]  Hello!\n","[02:36.000 --> 02:38.000]  Hello!\n","[02:38.000 --> 02:40.000]  Hello!\n","[02:40.000 --> 02:42.000]  Hello!\n","[02:42.000 --> 02:44.000]  Hello!\n","[02:44.000 --> 02:46.000]  Hello!\n","[02:46.000 --> 02:48.000]  Hello!\n","[02:48.000 --> 02:50.000]  Hello!\n","[02:50.000 --> 02:52.000]  Hello!\n","[02:52.000 --> 02:54.000]  Hello!\n","[02:54.000 --> 02:56.000]  Hello!\n","[02:56.000 --> 02:58.000]  Hello!\n","âœ… Whisper transcription completed\n","ğŸ” Quick repetition removal...\n","ğŸ“Š Quick cleaning: 58 â†’ 0 segments\n","ğŸ—‘ï¸  Removed 58 problematic segments\n","ğŸ‘¥ Starting speaker diarization...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Speaker diarization completed\n","\n","ğŸ­ DIALOGUE OUTPUT==============================\n","\n","ğŸ’¾ Results saved to: fast_transcription_results.json\n","âœ… Fast processing completed!\n"]}]}]}