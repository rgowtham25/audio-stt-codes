{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOJ0Q/kx/zvYwc4rP21pFyj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4e4cc1819cec415daa23c6aa54fd82e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1c66139d40c4975ba9d530e48a471d3","IPY_MODEL_ffd54730d4c742f8b7aa7c0e53e35147","IPY_MODEL_08c29cd7fc5e42bd81e431852ee2bb2d"],"layout":"IPY_MODEL_6c80b31b4d9446ed85931127649eda63"}},"f1c66139d40c4975ba9d530e48a471d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8783b0e70840d0ad1250a22f3da008","placeholder":"​","style":"IPY_MODEL_fdee29c9308d404eb3e8ca2b2c661e50","value":"config.yaml: 100%"}},"ffd54730d4c742f8b7aa7c0e53e35147":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44ef5ab7e3454880b5a43f39e1ba602c","max":469,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fac81451ea64b7db2f6046430a85c3d","value":469}},"08c29cd7fc5e42bd81e431852ee2bb2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18f02a17470f4c31ae75b52a15df4147","placeholder":"​","style":"IPY_MODEL_ac4bd85bbe9c49fda755a48d65baed9a","value":" 469/469 [00:00&lt;00:00, 47.2kB/s]"}},"6c80b31b4d9446ed85931127649eda63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d8783b0e70840d0ad1250a22f3da008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdee29c9308d404eb3e8ca2b2c661e50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44ef5ab7e3454880b5a43f39e1ba602c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fac81451ea64b7db2f6046430a85c3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18f02a17470f4c31ae75b52a15df4147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac4bd85bbe9c49fda755a48d65baed9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"694231200353438a9068675e2dc3cc86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c40863d1ee024c7cb89e9c2fcbb5823b","IPY_MODEL_73b51827465c48e5a09434f80d267424","IPY_MODEL_7408028f23404a85bf61beb109532978"],"layout":"IPY_MODEL_1268456cd3eb492c860a589ef63ff2bb"}},"c40863d1ee024c7cb89e9c2fcbb5823b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b587e4af60dc4592bf97c541a40a9ff1","placeholder":"​","style":"IPY_MODEL_3c3a1c2e5fd14b609d95c6d1e81304d3","value":"pytorch_model.bin: 100%"}},"73b51827465c48e5a09434f80d267424":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbde3b57dea04aba8cfae3e6662b3bf0","max":5905440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_565573a5e2ba4858b84250f1296b5444","value":5905440}},"7408028f23404a85bf61beb109532978":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cbded7c221c41fbba123f53064d6135","placeholder":"​","style":"IPY_MODEL_43104ff694c94ecf8b644bf5211f9986","value":" 5.91M/5.91M [00:00&lt;00:00, 8.44MB/s]"}},"1268456cd3eb492c860a589ef63ff2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b587e4af60dc4592bf97c541a40a9ff1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c3a1c2e5fd14b609d95c6d1e81304d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbde3b57dea04aba8cfae3e6662b3bf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"565573a5e2ba4858b84250f1296b5444":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9cbded7c221c41fbba123f53064d6135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43104ff694c94ecf8b644bf5211f9986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd8a5f47f8444d2eb0e32f9e4d64dbe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62d0e61b087c4e5390c78f138cc313ec","IPY_MODEL_a24a6d5e7e1a48b7b507700a21366ac4","IPY_MODEL_fbe3b0d7a83c4d7e98d8c6dd31d4691b"],"layout":"IPY_MODEL_3e7b58859da54a789fbff8416ffd13a7"}},"62d0e61b087c4e5390c78f138cc313ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dbf64f1bf234985831579d4441543e6","placeholder":"​","style":"IPY_MODEL_9e2bb978ce314132b9d4a43e1a121ce1","value":"config.yaml: 100%"}},"a24a6d5e7e1a48b7b507700a21366ac4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec2f8b9014b64b3bacd4f2fc6ed3b5c0","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b090818137ff4994875604dcfe1ad48d","value":399}},"fbe3b0d7a83c4d7e98d8c6dd31d4691b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f9aa1442e994072b2e0c9801aeff12c","placeholder":"​","style":"IPY_MODEL_a96c9d5ce7f4493492fe78bb35107735","value":" 399/399 [00:00&lt;00:00, 25.9kB/s]"}},"3e7b58859da54a789fbff8416ffd13a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dbf64f1bf234985831579d4441543e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e2bb978ce314132b9d4a43e1a121ce1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec2f8b9014b64b3bacd4f2fc6ed3b5c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b090818137ff4994875604dcfe1ad48d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f9aa1442e994072b2e0c9801aeff12c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a96c9d5ce7f4493492fe78bb35107735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"737f6f6f1fa847bc8cda373baa2b23d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_530543eaa2ab4b3285b2cfd9f0403c47","IPY_MODEL_20ac451d64f040c889c0fa3b69c6ce19","IPY_MODEL_dda0e4706ec642a89c0188de519cb2f1"],"layout":"IPY_MODEL_807b965a1afb4e9f966f89be22d87f71"}},"530543eaa2ab4b3285b2cfd9f0403c47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0410ad9a684d4a4081fc7dbeb11146ac","placeholder":"​","style":"IPY_MODEL_351cfd0dfe84498aa0bb44bb678c27a7","value":"pytorch_model.bin: 100%"}},"20ac451d64f040c889c0fa3b69c6ce19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c421204ffd13424ca53faaa12abbbacd","max":26645418,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8071c7f44bb34c28af6d0d69d4d69df4","value":26645418}},"dda0e4706ec642a89c0188de519cb2f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff94d1a4dcd446ac92352a19f5711c76","placeholder":"​","style":"IPY_MODEL_88dfa7a899b3486da337348a27cab0f5","value":" 26.6M/26.6M [00:00&lt;00:00, 36.5MB/s]"}},"807b965a1afb4e9f966f89be22d87f71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0410ad9a684d4a4081fc7dbeb11146ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"351cfd0dfe84498aa0bb44bb678c27a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c421204ffd13424ca53faaa12abbbacd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8071c7f44bb34c28af6d0d69d4d69df4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff94d1a4dcd446ac92352a19f5711c76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88dfa7a899b3486da337348a27cab0f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5a539b801cf473db193900af283fff9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_633ad3719546485fa090a91d327a7ae9","IPY_MODEL_83510830a48749e6a61f0cc1f07d57d3","IPY_MODEL_878c1b05535d429e985bfc11321d01a4"],"layout":"IPY_MODEL_cd040184ea7541eca9686a156e4b382f"}},"633ad3719546485fa090a91d327a7ae9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de61b6b4a2c34d55b27be0b345ff045a","placeholder":"​","style":"IPY_MODEL_010a749279674e61862d088fc7805d6a","value":"config.yaml: 100%"}},"83510830a48749e6a61f0cc1f07d57d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_240993104ae3410e89c897a30897e5bf","max":221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23b9024b7fce4137946b65dd03acba9e","value":221}},"878c1b05535d429e985bfc11321d01a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2eca90f279d436490e54f728a315d38","placeholder":"​","style":"IPY_MODEL_5686df568c924a88a89a37cd3a8201a8","value":" 221/221 [00:00&lt;00:00, 20.1kB/s]"}},"cd040184ea7541eca9686a156e4b382f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de61b6b4a2c34d55b27be0b345ff045a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010a749279674e61862d088fc7805d6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"240993104ae3410e89c897a30897e5bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b9024b7fce4137946b65dd03acba9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2eca90f279d436490e54f728a315d38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5686df568c924a88a89a37cd3a8201a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":133932,"status":"ok","timestamp":1753856721572,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"APEFy1kWOh5S","outputId":"b9b5f85f-4e4b-4a5c-ecf1-a82ad13556df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-u0hnl63j\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-u0hnl63j\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.7.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=8dd92862095580782e2996c8ead2f30791702995c5c1323fa12dcb90484e955f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3rkbscmt/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":25664,"status":"ok","timestamp":1753857205551,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ZZ-jXf6mOqp0","outputId":"e22e1e9b-3490-48f8-c19f-de44809da054"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.34.1)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.15.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.12.14)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.0-py3-none-any.whl (29 kB)\n","Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7e6e464b5baed67d9ec5ea7ed9d71469428a4623d26d9b95e36721198beca81f\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=1b7efb355aba63e97bda15775c3c4a7b78a2d03f4f2355ea4d48d1995468c072\n","  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.15.0 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.0\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"18p3Psr_Or2M","executionInfo":{"status":"ok","timestamp":1753857226941,"user_tz":-330,"elapsed":21386,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocess\n","import json"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xk9ilZoQRlCr","executionInfo":{"status":"ok","timestamp":1753866647235,"user_tz":-330,"elapsed":2,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["# Configuration\n","INPUT_AUDIO_PATH = \"call6.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124955,"status":"ok","timestamp":1753857351900,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"bqm2KjivRfeD","outputId":"48d33dba-ec4a-4a7a-c713-86a6eb527a24"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 2.88G/2.88G [01:17<00:00, 39.8MiB/s]\n"]}],"source":["model = whisper.load_model(\"large-v3\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":86,"status":"ok","timestamp":1753857351906,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"N4-pUEoiRfeD","outputId":"ec5d5398-8793-43db-cd71-9121cae4a0b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Whisper(\n","  (encoder): AudioEncoder(\n","    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (decoder): TextDecoder(\n","    (token_embedding): Embedding(51866, 1280)\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (cross_attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":8}],"source":["model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9XecuZI1O5xs","executionInfo":{"status":"ok","timestamp":1753851029491,"user_tz":-330,"elapsed":20,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["def get_audio_duration(audio_path):\n","    \"\"\"Get audio duration using ffprobe\"\"\"\n","    try:\n","        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","        return float(result.stdout.strip())\n","    except Exception as e:\n","        print(f\"Could not get duration: {e}\")\n","        return 0\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"chymACVjO50S","executionInfo":{"status":"ok","timestamp":1753851030353,"user_tz":-330,"elapsed":34,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["def audio_preprocessing_v1(input_path, output_path):\n","    \"\"\"Advanced audio preprocessing with better parameters\"\"\"\n","    print(\"--- Trying Advanced Audio Preprocessing ---\")\n","\n","    # Improved ffmpeg command - less aggressive filtering to preserve speech\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",  # Mono\n","        \"-ar\", \"16000\",  # 16kHz sample rate\n","        \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Advanced preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Advanced preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v2(input_path, output_path):\n","    \"\"\"Simplified but effective preprocessing\"\"\"\n","    print(\"--- Trying Simplified Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Simplified preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Simplified preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v3(input_path, output_path):\n","    \"\"\"Basic but reliable preprocessing\"\"\"\n","    print(\"--- Trying Basic Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Basic preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Basic preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v4(input_path, output_path):\n","    \"\"\"Minimal processing - just format conversion\"\"\"\n","    print(\"--- Trying Minimal Audio Processing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Minimal processing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Minimal processing failed: {e.returncode}\")\n","        return False\n","\n","def smart_audio_preprocessing(input_path, output_path):\n","    \"\"\"Try different preprocessing methods in order of preference\"\"\"\n","    original_duration = get_audio_duration(input_path)\n","    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n","\n","    methods = [\n","        audio_preprocessing_v1,\n","        audio_preprocessing_v2,\n","        audio_preprocessing_v3,\n","        audio_preprocessing_v4\n","    ]\n","\n","    for i, method in enumerate(methods, 1):\n","        if method(input_path, output_path):\n","            if os.path.exists(output_path):\n","                processed_duration = get_audio_duration(output_path)\n","                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n","\n","                if abs(original_duration - processed_duration) < 1.0:\n","                    print(f\"✅ Audio preprocessing successful with method {i}\")\n","                    return True\n","                else:\n","                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n","                    continue\n","\n","    print(\"❌ All preprocessing methods failed!\")\n","    return False"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gq_lrCJMF9Fm","executionInfo":{"status":"ok","timestamp":1753851032575,"user_tz":-330,"elapsed":49,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["def post_process_text(text):\n","    \"\"\"Clean up transcribed text from Whisper output for call center insurance context.\"\"\"\n","    if not text:\n","        return \"\"\n","\n","    # === 1. Remove excessive immediate repetitions ===\n","    words = text.split()\n","    cleaned_words = []\n","    i = 0\n","    while i < len(words):\n","        current_word = words[i].lower()\n","        repetition_count = 1\n","        j = i + 1\n","        while j < len(words) and words[j].lower() == current_word:\n","            repetition_count += 1\n","            j += 1\n","\n","        keep_count = min(repetition_count, 2) if repetition_count <= 3 else 1\n","        for _ in range(keep_count):\n","            cleaned_words.append(words[i])\n","        i += repetition_count\n","\n","    text = ' '.join(cleaned_words)\n","\n","    # === 2. Remove filler sounds (non-verbal, repetitive) ===\n","    filler_sounds = [\"uh\", \"um\", \"mm\", \"hmm\", \"ah\", \"oh\", \"huh\", \"ha ha\"]\n","    soft_fillers = [\"okay okay\", \"yes yes\", \"yes yes yes\", \"i mean\", \"you know\", \"like like\", \"ok ok\"]\n","\n","    for filler in filler_sounds + soft_fillers:\n","        text = re.sub(rf'\\b{re.escape(filler)}\\b', '', text, flags=re.IGNORECASE)\n","\n","    # === 3. Insurance domain term normalization ===\n","    corrections = {\n","        'access max life': 'Axis Max Life',\n","        'axis max life': 'Axis Max Life',\n","        'g pay': 'GPay',\n","        'google pay': 'Google Pay',\n","        'phone pay': 'PhonePe',\n","        'phone pe': 'PhonePe',\n","        'pay tm': 'Paytm',\n","        'net banking': 'netbanking',\n","        'some assured': 'sum assured',\n","        'premium do': 'premium due',\n","        'do date': 'due date',\n","        'okay sir': 'Okay sir',\n","    }\n","\n","    text_lower = text.lower()\n","    for wrong, correct in corrections.items():\n","        text_lower = text_lower.replace(wrong, correct)\n","\n","    # === 3.5 Replace 'Rs.', 'Rs' → '₹' with optional space cleanup ===\n","    text_lower = re.sub(r'\\brs[.]?\\s*', '₹', text_lower)\n","\n","    # === 4. Punctuation cleanup ===\n","    text_lower = re.sub(r'\\s{2,}', ' ', text_lower)          # Extra spaces\n","    text_lower = re.sub(r'[,]{2,}', ',', text_lower)         # Repeated commas\n","    text_lower = re.sub(r'\\s+,', ',', text_lower)            # Space before comma\n","    text_lower = re.sub(r'\\s+\\.', '.', text_lower)           # Space before period\n","    text_lower = re.sub(r'\\s+[!?]', lambda m: m.group(0).strip(), text_lower)\n","\n","    # === 5. Capitalize sentences ===\n","    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n","                        lambda m: m.group(1) + m.group(2).upper(),\n","                        text_lower)\n","\n","    return text_lower.strip()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vy-G7YeEU0TU","executionInfo":{"status":"ok","timestamp":1753851034759,"user_tz":-330,"elapsed":49,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["def enhanced_whisper_transcription(audio_path):\n","    \"\"\"\n","    Enhanced Whisper transcription with optimal anti-repetition parameters\n","    \"\"\"\n","    print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","\n","    # prompt = (\n","    #     \"Axis Maxlife Insurance, Policy number, fund value, Due date,\"\n","    #     \"Sum Assured, Policy Status, Late Fee, Google Pay, GPay, PhonePe, Paytm, netbanking,\"\n","    # )\n","\n","    prompt = (\n","        \"This is a customer support call for Axis Maxlife Insurance. \"\n","        \"We will discuss policy numbers, due date, fund value, sum assured, late fee, \"\n","        \"and payment methods such as Google Pay, PhonePe, Paytm and net banking.\"\n","    )\n","\n","    # Single optimal strategy - no need for multiple attempts\n","    result = model.transcribe(\n","        audio_path,\n","        language=\"ta\",\n","        task=\"translate\",\n","        temperature=0.0,\n","        beam_size=5,\n","        patience=1.2,\n","        condition_on_previous_text=False,\n","        no_speech_threshold=0.8,\n","        compression_ratio_threshold=2.0,\n","        logprob_threshold=-0.35,\n","        word_timestamps=False,\n","        initial_prompt=prompt,\n","        verbose=True,\n","    )\n","\n","    print(\"✅ Whisper transcription completed with optimal parameters\")\n","    return result\n","\n","\n","def calculate_repetition_score(segments):\n","    \"\"\"\n","    Calculate a repetition score for transcription segments\n","    Lower score = less repetition = better\n","    \"\"\"\n","    if not segments:\n","        return 0.0\n","\n","    total_repetition = 0\n","    total_words = 0\n","\n","    for segment in segments:\n","        text = segment.get('text', '').strip().lower()\n","        words = text.split()\n","\n","        if len(words) < 2:\n","            continue\n","\n","        total_words += len(words)\n","\n","        # Count immediate word repetitions\n","        for i in range(len(words) - 1):\n","            if words[i] == words[i + 1]:\n","                total_repetition += 1\n","\n","        # Count phrase repetitions within segment\n","        for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len])\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n","                if phrase1 == phrase2:\n","                    total_repetition += phrase_len * 2  # Heavy penalty\n","\n","    return total_repetition / max(total_words, 1)\n","\n","def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n","    \"\"\"\n","    AGGRESSIVE post-processing function to detect and remove repetitive segments\n","    \"\"\"\n","    print(\"🔍 Starting aggressive repetition detection...\")\n","    cleaned_segments = []\n","\n","    for i, segment in enumerate(segments):\n","        text = segment['text'].strip()\n","        words = text.split()\n","\n","        # Skip very short segments\n","        if len(words) < 2:\n","            continue\n","\n","        # AGGRESSIVE: Check for excessive word repetition\n","        is_repetitive = False\n","\n","        # Count word frequencies\n","        word_counts = {}\n","        for word in words:\n","            word_lower = word.lower().strip('.,!?')\n","            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","        # Check if any single word dominates the segment\n","        max_word_count = max(word_counts.values()) if word_counts else 0\n","        word_dominance = max_word_count / len(words) if words else 0\n","\n","        if word_dominance > 0.4:  # If any word is >40% of the segment\n","            print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n","            continue\n","\n","        # Check for immediate repetitions (same word repeated consecutively)\n","        consecutive_repeats = 0\n","        max_consecutive = 0\n","\n","        for j in range(1, len(words)):\n","            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                consecutive_repeats += 1\n","                max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","            else:\n","                consecutive_repeats = 0\n","\n","        if max_consecutive > 3:  # More than 3 consecutive identical words\n","            print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n","            continue\n","\n","        # Check for pattern repetitions within segment\n","        for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n","\n","                if phrase1 == phrase2:\n","                    repetition_coverage = (phrase_len * 2) / len(words)\n","                    if repetition_coverage > max_repetition_ratio:\n","                        print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n","                        is_repetitive = True\n","                        break\n","            if is_repetitive:\n","                break\n","\n","        if is_repetitive:\n","            continue\n","\n","        # Check for similarity with recent segments (avoid near-duplicates)\n","        is_near_duplicate = False\n","        for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n","            prev_words = prev_segment['text'].lower().split()\n","            current_words = [w.lower() for w in words]\n","\n","            if prev_words and current_words:\n","                # Calculate Jaccard similarity\n","                prev_set = set(prev_words)\n","                current_set = set(current_words)\n","                intersection = len(prev_set.intersection(current_set))\n","                union = len(prev_set.union(current_set))\n","\n","                similarity = intersection / union if union > 0 else 0\n","\n","                if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n","                    print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n","                    is_near_duplicate = True\n","                    break\n","\n","        if is_near_duplicate:\n","            continue\n","\n","        # If we reach here, the segment passed all checks\n","        cleaned_segments.append(segment)\n","\n","    removed_count = len(segments) - len(cleaned_segments)\n","    print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","    print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n","\n","    return cleaned_segments"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4e4cc1819cec415daa23c6aa54fd82e2","f1c66139d40c4975ba9d530e48a471d3","ffd54730d4c742f8b7aa7c0e53e35147","08c29cd7fc5e42bd81e431852ee2bb2d","6c80b31b4d9446ed85931127649eda63","4d8783b0e70840d0ad1250a22f3da008","fdee29c9308d404eb3e8ca2b2c661e50","44ef5ab7e3454880b5a43f39e1ba602c","3fac81451ea64b7db2f6046430a85c3d","18f02a17470f4c31ae75b52a15df4147","ac4bd85bbe9c49fda755a48d65baed9a","694231200353438a9068675e2dc3cc86","c40863d1ee024c7cb89e9c2fcbb5823b","73b51827465c48e5a09434f80d267424","7408028f23404a85bf61beb109532978","1268456cd3eb492c860a589ef63ff2bb","b587e4af60dc4592bf97c541a40a9ff1","3c3a1c2e5fd14b609d95c6d1e81304d3","fbde3b57dea04aba8cfae3e6662b3bf0","565573a5e2ba4858b84250f1296b5444","9cbded7c221c41fbba123f53064d6135","43104ff694c94ecf8b644bf5211f9986","bd8a5f47f8444d2eb0e32f9e4d64dbe2","62d0e61b087c4e5390c78f138cc313ec","a24a6d5e7e1a48b7b507700a21366ac4","fbe3b0d7a83c4d7e98d8c6dd31d4691b","3e7b58859da54a789fbff8416ffd13a7","2dbf64f1bf234985831579d4441543e6","9e2bb978ce314132b9d4a43e1a121ce1","ec2f8b9014b64b3bacd4f2fc6ed3b5c0","b090818137ff4994875604dcfe1ad48d","7f9aa1442e994072b2e0c9801aeff12c","a96c9d5ce7f4493492fe78bb35107735","737f6f6f1fa847bc8cda373baa2b23d8","530543eaa2ab4b3285b2cfd9f0403c47","20ac451d64f040c889c0fa3b69c6ce19","dda0e4706ec642a89c0188de519cb2f1","807b965a1afb4e9f966f89be22d87f71","0410ad9a684d4a4081fc7dbeb11146ac","351cfd0dfe84498aa0bb44bb678c27a7","c421204ffd13424ca53faaa12abbbacd","8071c7f44bb34c28af6d0d69d4d69df4","ff94d1a4dcd446ac92352a19f5711c76","88dfa7a899b3486da337348a27cab0f5","d5a539b801cf473db193900af283fff9","633ad3719546485fa090a91d327a7ae9","83510830a48749e6a61f0cc1f07d57d3","878c1b05535d429e985bfc11321d01a4","cd040184ea7541eca9686a156e4b382f","de61b6b4a2c34d55b27be0b345ff045a","010a749279674e61862d088fc7805d6a","240993104ae3410e89c897a30897e5bf","23b9024b7fce4137946b65dd03acba9e","f2eca90f279d436490e54f728a315d38","5686df568c924a88a89a37cd3a8201a8"]},"executionInfo":{"elapsed":97393,"status":"ok","timestamp":1753851135296,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"wB3OJ0-WU6kA","outputId":"5479e835-7de5-487c-b36e-d27771c2c2c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n","============================================================\n","Original audio duration: 172.48 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 172.45 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n","[00:00.000 --> 00:06.000]  Good afternoon ma'am. I am calling from Axis Maxlife for renewal.\n","[00:06.000 --> 00:09.000]  I am calling for renewal.\n","[00:09.000 --> 00:12.000]  Hello.\n","[00:12.000 --> 00:15.000]  Hello.\n","[00:15.000 --> 00:18.000]  Hello.\n","[00:18.000 --> 00:20.000]  Hello.\n","[00:20.000 --> 00:22.000]  Hello ma'am. Good afternoon.\n","[00:22.000 --> 00:23.000]  Yes, tell me ma'am.\n","[00:23.000 --> 00:28.000]  I am calling from Axis Maxlife. Do you know who is Ms. Maheshwari Vinodkumar?\n","[00:28.000 --> 00:32.000]  It's my madam.\n","[00:32.000 --> 00:36.000]  There is a policy in her name. We have called her to talk about it.\n","[00:36.000 --> 00:43.000]  You spoke to her once the other day. She even told my staff about it.\n","[00:43.000 --> 00:47.000]  What have you updated, madam?\n","[00:47.000 --> 00:53.000]  One minute. One minute, madam. I will talk to my staff.\n","[00:53.000 --> 00:58.000]  Yes, but till last month they have said that they will be paying for it.\n","[01:23.000 --> 01:25.000]  Hello ma'am\n","[01:25.000 --> 01:30.000]  Ma'am, they are saying that they will pay you on Monday\n","[01:30.000 --> 01:37.000]  Monday? Okay ma'am, they have already said that they will pay you, that's why we are calling you\n","[01:37.000 --> 01:41.000]  Okay ma'am, they are saying that they will pay you on Monday\n","[01:41.000 --> 01:43.000]  Will they pay you on Monday for sure ma'am?\n","[01:43.000 --> 01:45.000]  Yes ma'am\n","[01:45.000 --> 01:49.000]  Okay, will they do it in the branch or online?\n","[01:49.000 --> 01:51.000]  Online only ma'am\n","[01:51.000 --> 01:58.000]  Your apology will be active only if you submit the health declaration form once you make the payment.\n","[01:58.000 --> 02:00.000]  Okay ma'am.\n","[02:00.000 --> 02:02.000]  Okay ma'am.\n","[02:02.000 --> 02:06.000]  Do you have any alternative phone number or email id?\n","[02:06.000 --> 02:12.000]  I don't have any number. Do you have this number?\n","[02:12.000 --> 02:21.000]  This is the number ma'am. It is 9-9-4-0-0-9-0-8-0-9.\n","[02:21.000 --> 02:27.000]  Yes, that is the number ma'am. You can take that too.\n","[02:27.000 --> 02:31.000]  Okay ma'am, fine. Thank you ma'am for giving me time.\n","[02:31.000 --> 02:32.000]  Okay ma'am.\n","[02:32.000 --> 02:33.000]  Ok Madam\n","[02:41.000 --> 02:43.000]  Ma'am, please cut the call\n","[02:46.000 --> 02:48.000]  Please cut the call from the customer side\n","[02:49.000 --> 02:52.000]  There is no response from the customer side, so the call disconnects\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","\n","--- Removing Repetitive Segments ---\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting word-dominated segment: Hello ma'am... (dominance: 0.50)\n","🚫 Rejecting near-duplicate: Okay ma'am, they are saying th... (similarity: 0.91)\n","🚫 Rejecting word-dominated segment: Yes ma'am... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Okay ma'am.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Okay ma'am.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Okay ma'am.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: Ok Madam... (dominance: 0.50)\n","📊 Aggressive cleaning: 36 → 25 segments\n","🗑️  Removed 11 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n"]},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4cc1819cec415daa23c6aa54fd82e2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"694231200353438a9068675e2dc3cc86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8a5f47f8444d2eb0e32f9e4d64dbe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737f6f6f1fa847bc8cda373baa2b23d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a539b801cf473db193900af283fff9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Using GPU for diarization\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n","It can be re-enabled by calling\n","   >>> import torch\n","   >>> torch.backends.cuda.matmul.allow_tf32 = True\n","   >>> torch.backends.cudnn.allow_tf32 = True\n","See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n","\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_00 [0.0s - 6.0s]:\n","  📝 Good afternoon ma'am. I am calling from axis maxlife for renewal.\n","\n","SPEAKER_01 [6.0s - 9.0s]:\n","  📝 I am calling for renewal.\n","\n","SPEAKER_00 [20.0s - 22.0s]:\n","  📝 Hello ma'am. Good afternoon.\n","\n","SPEAKER_01 [22.0s - 23.0s]:\n","  📝 Yes, tell me ma'am.\n","\n","SPEAKER_00 [23.0s - 28.0s]:\n","  📝 I am calling from axis maxlife. Do who is ms. Maheshwari vinodkumar?\n","\n","SPEAKER_01 [28.0s - 32.0s]:\n","  📝 It's my madam.\n","\n","SPEAKER_00 [32.0s - 36.0s]:\n","  📝 There is a policy in her name. We have called her to talk about it.\n","\n","SPEAKER_01 [36.0s - 43.0s]:\n","  📝 You spoke to her once the other day. She even told my staff about it.\n","\n","SPEAKER_00 [43.0s - 47.0s]:\n","  📝 What have you updated, madam?\n","\n","SPEAKER_01 [47.0s - 53.0s]:\n","  📝 One minute. One minute, madam. I will talk to my staff.\n","\n","SPEAKER_00 [53.0s - 58.0s]:\n","  📝 Yes, but till last month they have said that they will be paying for it.\n","\n","SPEAKER_01 [85.0s - 90.0s]:\n","  📝 Ma'am, they are saying that they will pay you on monday\n","\n","SPEAKER_00 [90.0s - 97.0s]:\n","  📝 Monday? Okay ma'am, they have already said that they will pay you, that's why we are calling you\n","\n","SPEAKER_00 [101.0s - 109.0s]:\n","  📝 Will they pay you on monday for sure ma'am? Okay, will they do it in the branch or online?\n","\n","SPEAKER_01 [109.0s - 111.0s]:\n","  📝 Online only ma'am\n","\n","SPEAKER_00 [111.0s - 118.0s]:\n","  📝 Your apology will be active only if you submit the health declaration form once you make the payment.\n","\n","SPEAKER_00 [122.0s - 126.0s]:\n","  📝 Do you have any alternative phone number or email id?\n","\n","SPEAKER_01 [126.0s - 132.0s]:\n","  📝 I don't have any number. Do you have this number?\n","\n","SPEAKER_00 [132.0s - 141.0s]:\n","  📝 This is the number ma'am. It is 9-9-4-0-0-9-0-8-0-9.\n","\n","SPEAKER_01 [141.0s - 147.0s]:\n","  📝 Yes, that is the number ma'am. You can take that too.\n","\n","SPEAKER_00 [147.0s - 151.0s]:\n","  📝 Okay ma'am, fine. Thank you ma'am for giving me time.\n","\n","SPEAKER_00 [161.0s - 163.0s]:\n","  📝 Ma'am, please cut the call\n","\n","SPEAKER_00 [166.0s - 172.0s]:\n","  📝 Please cut the call from the customer side There is no response from the customer side, so the call disconnects\n","\n","💾 Results saved to: enhanced_transcription_results-call6.json\n","✅ Processing completed successfully!\n"]}],"source":["def main():\n","    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Smart Audio Preprocessing\n","    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","        return\n","\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 2: Enhanced Whisper Transcription with anti-repetition\n","    try:\n","        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","        print(\"✅ Whisper transcription completed\")\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    # Step 3: Remove repetitive segments BEFORE post-processing\n","    print(\"\\n--- Removing Repetitive Segments ---\")\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    # Step 4: Post-process remaining transcription\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = post_process_text(segment['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    # Step 5: Speaker Diarization\n","    print(\"\\n--- Speaker Diarization ---\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU for diarization\")\n","\n","        diarization = pipeline(CLEAN_AUDIO_PATH)\n","        print(\"✅ Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"⚠️  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 6: Generate Enhanced Dialogue\n","    print(\"\\n--- Generating Dialogue ---\")\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Combine segments by speaker\n","    dialogue = []\n","    current_speaker = None\n","    current_texts = []\n","    current_start = 0\n","    current_end = 0\n","\n","    for segment in processed_segments:\n","        start = segment['start']\n","        end = segment['end']\n","        text = segment['text'].strip()\n","\n","        speaker = get_dominant_speaker(start, end, diarization)\n","\n","        # Merge consecutive segments from same speaker (within 3 seconds)\n","        if (speaker == current_speaker and\n","            current_speaker and\n","            (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            # Save previous speaker's dialogue\n","            if current_speaker and current_texts:\n","                combined_text = ' '.join(current_texts)\n","                # Final check for repetition in combined text\n","                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined_text,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","\n","            # Start new speaker segment\n","            current_speaker = speaker\n","            current_texts = [text]\n","            current_start = start\n","            current_end = end\n","\n","    # Add final segment\n","    if current_speaker and current_texts:\n","        combined_text = ' '.join(current_texts)\n","        if len(combined_text.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined_text,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Step 7: Display Results\n","    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  📝 {entry['text']}\")\n","\n","    # Step 8: Save Results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open('enhanced_transcription_results-call6.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n💾 Results saved to: enhanced_transcription_results-call6.json\")\n","    print(\"✅ Processing completed successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"YdOeHvu9pBw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q4W9zV0jpB50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HJrSJxYgpB8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#test for call4"],"metadata":{"id":"06etCIXRpCjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"69BmF-x7pCnm","executionInfo":{"status":"ok","timestamp":1753851144081,"user_tz":-330,"elapsed":3,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}}},"outputs":[],"source":["# Configuration\n","INPUT_AUDIO_PATH = \"call4.wav\"\n","CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127152,"status":"ok","timestamp":1753851273280,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"outputId":"af6055bb-585d-4a04-98b1-d6cc03eedbcb","id":"WTXL65RSpCnn"},"outputs":[{"output_type":"stream","name":"stdout","text":["🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n","============================================================\n","Original audio duration: 160.96 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 160.93 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n","[00:00.000 --> 00:02.000]  Hello!\n","[00:02.000 --> 00:06.000]  Hello! My name is Axis Maxlife Insurance.\n","[00:06.000 --> 00:08.000]  Hello!\n","[00:08.000 --> 00:10.000]  My name is Chalo Kumar.\n","[00:10.000 --> 00:12.000]  Hello!\n","[00:12.000 --> 00:14.000]  I would like to ask you about the policy for Axis Maxlife Insurance.\n","[00:14.000 --> 00:16.000]  Hello!\n","[00:16.000 --> 00:18.000]  The policy for Axis Maxlife Insurance is,\n","[00:18.000 --> 00:20.000]  The policy for Axis Maxlife Insurance is,\n","[00:20.000 --> 00:22.000]  The policy for Axis Maxlife Insurance is,\n","[00:22.000 --> 00:24.000]  The policy for Axis Maxlife Insurance is,\n","[00:24.000 --> 00:27.000]  It is due on 6th June 2024.\n","[00:27.000 --> 00:29.000]  The amount is Rs.1,200,000.\n","[00:29.000 --> 00:33.000]  17 years have passed and now 2 years are due.\n","[00:33.000 --> 00:34.000]  Yes.\n","[00:34.000 --> 00:37.000]  When are you going to pay for this?\n","[00:37.000 --> 00:40.000]  We will see if we can pay.\n","[00:40.000 --> 00:42.000]  We have to finish it by the end of the year.\n","[00:42.000 --> 00:47.000]  We will see if we can finish it by the end of the year.\n","[00:47.000 --> 00:50.000]  We will see.\n","[00:50.000 --> 00:54.000]  The plan is that you have to pay for the entire 5 years, right?\n","[00:54.000 --> 00:55.000]  Yes.\n","[00:55.000 --> 00:57.000]  So, you have already paid for 2 years.\n","[00:57.000 --> 00:58.000]  Yes.\n","[00:58.000 --> 00:59.000]  Now you have 2 years of life.\n","[00:59.000 --> 01:01.000]  So, you have paid for these 2 years of life, right?\n","[01:01.000 --> 01:02.000]  Yes.\n","[01:02.000 --> 01:05.000]  So, next year you will have only 1 year of life.\n","[01:05.000 --> 01:06.000]  Yes.\n","[01:06.000 --> 01:09.000]  So, after that you will have 5 years of life.\n","[01:09.000 --> 01:15.000]  So, if you look at the amount you have already paid so far, it will be Rs.1,200,000.\n","[01:15.000 --> 01:16.000]  Yes.\n","[01:16.000 --> 01:19.000]  So, if you look at your present fund value,\n","[01:19.000 --> 01:25.000]  2 lakhs, 12000 rupees and 33300 rupees. Seriously, we are in a very high growth.\n","[01:25.000 --> 01:26.000]  Yes, yes, yes.\n","[01:26.000 --> 01:29.000]  So, if you don't pay now, you can keep it. That's what we want, sir.\n","[01:29.000 --> 01:33.000]  But, if you pay now, it will be around 1 lakhs.\n","[01:33.000 --> 01:37.000]  You will get a return of around 1 lakhs, without interest.\n","[01:37.000 --> 01:38.000]  Yes, yes.\n","[01:38.000 --> 01:40.000]  So, if you pay now, you can keep the remaining amount.\n","[01:40.000 --> 01:44.000]  You can add interest to the company and give it to the community.\n","[01:44.000 --> 01:45.000]  Yes, yes, yes.\n","[01:45.000 --> 01:52.000]  If you want to add a discount, you will have to add a discounted fund.\n","[01:52.000 --> 01:59.000]  If you add a discounted fund, you will not get that much returns and you will not have risk average.\n","[01:59.000 --> 02:01.000]  Ok.\n","[02:01.000 --> 02:03.000]  When will you show it?\n","[02:03.000 --> 02:07.000]  I will talk to the VP and let you know.\n","[02:07.000 --> 02:09.000]  Ok.\n","[02:09.000 --> 02:12.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:12.000 --> 02:13.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:13.000 --> 02:14.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:14.000 --> 02:15.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:15.000 --> 02:16.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:16.000 --> 02:17.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:17.000 --> 02:18.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:18.000 --> 02:19.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:19.000 --> 02:20.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:20.000 --> 02:21.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:21.000 --> 02:22.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:22.000 --> 02:23.000]  Sir, I am calling to inform you that the health declaration is not complete.\n","[02:23.000 --> 02:24.000]  Thank you sir.\n","[02:24.000 --> 02:25.000]  Thank you sir.\n","[02:25.000 --> 02:26.000]  Thank you sir.\n","[02:26.000 --> 02:27.000]  Thank you sir.\n","[02:27.000 --> 02:28.000]  Thank you sir.\n","[02:28.000 --> 02:29.000]  Thank you sir.\n","[02:29.000 --> 02:30.000]  Thank you sir.\n","[02:30.000 --> 02:31.000]  Thank you sir.\n","[02:31.000 --> 02:32.000]  Thank you sir.\n","[02:32.000 --> 02:33.000]  Thank you sir.\n","[02:33.000 --> 02:34.000]  Thank you sir.\n","[02:34.000 --> 02:35.000]  Thank you sir.\n","[02:35.000 --> 02:36.000]  Thank you sir.\n","[02:36.000 --> 02:37.000]  Thank you sir.\n","[02:37.000 --> 02:38.000]  Thank you sir.\n","[02:38.000 --> 02:39.000]  Thank you sir.\n","[02:39.000 --> 02:40.000]  Thank you sir.\n","[02:40.000 --> 02:41.000]  Thank you sir.\n","[02:41.000 --> 02:42.000]  Thank you sir.\n","[02:42.000 --> 02:43.000]  Thank you sir.\n","[02:43.000 --> 02:44.000]  Thank you sir.\n","[02:44.000 --> 02:45.000]  Thank you sir.\n","[02:45.000 --> 02:46.000]  Thank you sir.\n","[02:46.000 --> 02:47.000]  Thank you sir.\n","[02:47.000 --> 02:48.000]  Thank you sir.\n","[02:48.000 --> 02:49.000]  Thank you sir.\n","[02:49.000 --> 02:50.000]  Thank you sir.\n","[02:50.000 --> 02:51.000]  Thank you sir.\n","✅ Whisper transcription completed with optimal parameters\n","✅ Whisper transcription completed\n","\n","--- Removing Repetitive Segments ---\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting near-duplicate: The policy for Axis Maxlife In... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: The policy for Axis Maxlife In... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: The policy for Axis Maxlife In... (similarity: 1.00)\n","🚫 Rejecting word-dominated segment: Yes, yes, yes.... (dominance: 1.00)\n","🚫 Rejecting word-dominated segment: Yes, yes.... (dominance: 1.00)\n","🚫 Rejecting word-dominated segment: Yes, yes, yes.... (dominance: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Sir, I am calling to inform yo... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you sir.... (similarity: 1.00)\n","📊 Aggressive cleaning: 88 → 32 segments\n","🗑️  Removed 56 repetitive/problematic segments\n","\n","--- Speaker Diarization ---\n","✅ Using GPU for diarization\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Speaker diarization completed\n","\n","--- Generating Dialogue ---\n","\n","🎭 DIALOGUE OUTPUT========================================\n","\n","SPEAKER_00 [2.0s - 18.0s]:\n","  📝 Hello! My name is axis maxlife insurance. My name is chalo kumar. I would like to ask you about the policy for axis maxlife insurance. The policy for axis maxlife insurance is,\n","\n","SPEAKER_00 [24.0s - 37.0s]:\n","  📝 It is due on 6th june 2024. The amount is ₹1,200,000. 17 years have passed and now 2 years are due. When are you going to pay for this?\n","\n","SPEAKER_01 [37.0s - 50.0s]:\n","  📝 We will see if we can pay. We have to finish it by the end of the year. We will see if we can finish it by the end of the year. We will see.\n","\n","SPEAKER_00 [50.0s - 123.0s]:\n","  📝 The plan is that you have to pay for the entire 5 years, right? So, you have already paid for 2 years. Now you have 2 years of life. So, you have paid for these 2 years of life, right? So, next year you will have only 1 year of life. So, after that you will have 5 years of life. So, if you look at the amount you have already paid so far, it will be ₹1,200,000. So, if you look at your present fund value, 2 lakhs, 12000 rupees and 33300 rupees. Seriously, we are in a very high growth. So, if you don't pay now, you can keep it. That's what we want, sir. But, if you pay now, it will be around 1 lakhs. You will get a return of around 1 lakhs, without interest. So, if you pay now, you can keep the remaining amount. You can add interest to the company and give it to the community. If you want to add a discount, you will have to add a discounted fund. If you add a discounted fund, you will not get that much returns and you will not have risk average. When will you show it?\n","\n","SPEAKER_01 [123.0s - 127.0s]:\n","  📝 I will talk to the vp and let.\n","\n","SPEAKER_00 [129.0s - 132.0s]:\n","  📝 Sir, i am calling to inform you that the health declaration is not complete.\n","\n","SPEAKER_00 [143.0s - 144.0s]:\n","  📝 Thank you sir.\n","\n","💾 Results saved to: enhanced_transcription_results-call4.json\n","✅ Processing completed successfully!\n"]}],"source":["def main():\n","    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n","    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Smart Audio Preprocessing\n","    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n","        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n","        return\n","\n","    # model = whisper.load_model(\"large\")\n","\n","    # Step 2: Enhanced Whisper Transcription with anti-repetition\n","    try:\n","        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n","        print(\"✅ Whisper transcription completed\")\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    # Step 3: Remove repetitive segments BEFORE post-processing\n","    print(\"\\n--- Removing Repetitive Segments ---\")\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    # Step 4: Post-process remaining transcription\n","    processed_segments = []\n","    for segment in cleaned_segments:\n","        processed_text = post_process_text(segment['text'])\n","        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n","            segment_copy = segment.copy()\n","            segment_copy['text'] = processed_text\n","            processed_segments.append(segment_copy)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    # Step 5: Speaker Diarization\n","    print(\"\\n--- Speaker Diarization ---\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU for diarization\")\n","\n","        diarization = pipeline(CLEAN_AUDIO_PATH)\n","        print(\"✅ Speaker diarization completed\")\n","\n","    except Exception as e:\n","        print(f\"⚠️  Speaker diarization failed: {e}\")\n","        diarization = None\n","\n","    # Step 6: Generate Enhanced Dialogue\n","    print(\"\\n--- Generating Dialogue ---\")\n","\n","    def get_dominant_speaker(start_time, end_time, diarization_result):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","\n","        speakers = {}\n","        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n","            overlap_start = max(start_time, segment.start)\n","            overlap_end = min(end_time, segment.end)\n","            overlap_duration = max(0, overlap_end - overlap_start)\n","\n","            if overlap_duration > 0:\n","                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n","\n","        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","    # Combine segments by speaker\n","    dialogue = []\n","    current_speaker = None\n","    current_texts = []\n","    current_start = 0\n","    current_end = 0\n","\n","    for segment in processed_segments:\n","        start = segment['start']\n","        end = segment['end']\n","        text = segment['text'].strip()\n","\n","        speaker = get_dominant_speaker(start, end, diarization)\n","\n","        # Merge consecutive segments from same speaker (within 3 seconds)\n","        if (speaker == current_speaker and\n","            current_speaker and\n","            (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            # Save previous speaker's dialogue\n","            if current_speaker and current_texts:\n","                combined_text = ' '.join(current_texts)\n","                # Final check for repetition in combined text\n","                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined_text,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","\n","            # Start new speaker segment\n","            current_speaker = speaker\n","            current_texts = [text]\n","            current_start = start\n","            current_end = end\n","\n","    # Add final segment\n","    if current_speaker and current_texts:\n","        combined_text = ' '.join(current_texts)\n","        if len(combined_text.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined_text,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Step 7: Display Results\n","    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n","\n","    for entry in dialogue:\n","        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n","        print(f\"\\n{entry['speaker']} {timestamp}:\")\n","        print(f\"  📝 {entry['text']}\")\n","\n","    # Step 8: Save Results\n","    output_data = {\n","        'metadata': {\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open('enhanced_transcription_results-call4.json', 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"\\n💾 Results saved to: enhanced_transcription_results-call4.json\")\n","    print(\"✅ Processing completed successfully!\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"NC0URKNw9CnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lfB_7LPn9CqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1WYNtPzi9Cth"},"execution_count":null,"outputs":[]}]}