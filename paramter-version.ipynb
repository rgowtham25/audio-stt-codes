{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 103845,
     "status": "ok",
     "timestamp": 1754629724607,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "APEFy1kWOh5S",
    "outputId": "2590dbdf-f84c-4b33-f45f-afc1043d4f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-67zmndq6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-67zmndq6\n",
      "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=7ce26accb1eca905437cf6d64d034578975c212183c303df00e80baf3d61e52a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dtjne71x/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-packages git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13943,
     "status": "ok",
     "timestamp": 1754629738544,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "ZZ-jXf6mOqp0",
    "outputId": "d1c0918a-bb98-4d94-8c6b-d1def25c601c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.audio\n",
      "  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.34.3)\n",
      "Collecting lightning>=2.0.1 (from pyannote.audio)\n",
      "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n",
      "Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n",
      "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
      "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n",
      "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n",
      "Collecting semver>=3.0.0 (from pyannote.audio)\n",
      "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n",
      "Collecting speechbrain>=1.0.0 (from pyannote.audio)\n",
      "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tensorboardX>=2.6 (from pyannote.audio)\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
      "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n",
      "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n",
      "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n",
      "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.1)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
      "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n",
      "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n",
      "Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
      "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n",
      "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.12.15)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
      "Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.42)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
      "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n",
      "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n",
      "Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
      "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt, julius\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=69a51eef1f35c240021f8346fa17e19f11a606821267b7084fa6ab481b8e90da\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=1fbb46887a89929b7e1cab11e850dde5b5c937874a3f4ebe756f3a914c82c781\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n",
      "Successfully built docopt julius\n",
      "Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n",
      "Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.15.2 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-packages pyannote.audio torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18p3Psr_Or2M"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk9ilZoQRlCr"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_AUDIO_PATH = \"call6.wav\"\n",
    "CLEAN_AUDIO_PATH = \"cleaned_audio_for_asr_and_diarization.wav\"\n",
    "HUGGING_FACE_ACCESS_TOKEN = \"hf_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237009,
     "status": "ok",
     "timestamp": 1754629997647,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "bqm2KjivRfeD",
    "outputId": "43523b87-10c4-47ab-85e8-aa93a4d069c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [03:25<00:00, 15.0MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1754629997709,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "N4-pUEoiRfeD",
    "outputId": "1cf66fb0-4e9d-46d4-c27d-6a600c81378c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XecuZI1O5xs"
   },
   "outputs": [],
   "source": [
    "def get_audio_duration(audio_path):\n",
    "    \"\"\"Get audio duration using ffprobe\"\"\"\n",
    "    try:\n",
    "        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
    "               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        return float(result.stdout.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get duration: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chymACVjO50S"
   },
   "outputs": [],
   "source": [
    "def audio_preprocessing_v1(input_path, output_path):\n",
    "    \"\"\"Advanced audio preprocessing with better parameters\"\"\"\n",
    "    print(\"--- Trying Advanced Audio Preprocessing ---\")\n",
    "\n",
    "    # Improved ffmpeg command - less aggressive filtering to preserve speech\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",  # Mono\n",
    "        \"-ar\", \"16000\",  # 16kHz sample rate\n",
    "        \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Advanced preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Advanced preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v2(input_path, output_path):\n",
    "    \"\"\"Simplified but effective preprocessing\"\"\"\n",
    "    print(\"--- Trying Simplified Audio Preprocessing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Simplified preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Simplified preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v3(input_path, output_path):\n",
    "    \"\"\"Basic but reliable preprocessing\"\"\"\n",
    "    print(\"--- Trying Basic Audio Preprocessing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-af\", \"loudnorm\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Basic preprocessing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Basic preprocessing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def audio_preprocessing_v4(input_path, output_path):\n",
    "    \"\"\"Minimal processing - just format conversion\"\"\"\n",
    "    print(\"--- Trying Minimal Audio Processing ---\")\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", input_path,\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "        print(\"Minimal processing successful\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Minimal processing failed: {e.returncode}\")\n",
    "        return False\n",
    "\n",
    "def smart_audio_preprocessing(input_path, output_path):\n",
    "    \"\"\"Try different preprocessing methods in order of preference\"\"\"\n",
    "    original_duration = get_audio_duration(input_path)\n",
    "    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n",
    "\n",
    "    methods = [\n",
    "        audio_preprocessing_v1,\n",
    "        audio_preprocessing_v2,\n",
    "        audio_preprocessing_v3,\n",
    "        audio_preprocessing_v4\n",
    "    ]\n",
    "\n",
    "    for i, method in enumerate(methods, 1):\n",
    "        if method(input_path, output_path):\n",
    "            if os.path.exists(output_path):\n",
    "                processed_duration = get_audio_duration(output_path)\n",
    "                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n",
    "\n",
    "                if abs(original_duration - processed_duration) < 1.0:\n",
    "                    print(f\"✅ Audio preprocessing successful with method {i}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n",
    "                    continue\n",
    "\n",
    "    print(\"❌ All preprocessing methods failed!\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq_lrCJMF9Fm"
   },
   "outputs": [],
   "source": [
    "def post_process_text(text):\n",
    "    \"\"\"Clean up transcribed text from Whisper output for call center insurance context.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # === 1. Remove excessive immediate repetitions ===\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        current_word = words[i].lower()\n",
    "        repetition_count = 1\n",
    "        j = i + 1\n",
    "        while j < len(words) and words[j].lower() == current_word:\n",
    "            repetition_count += 1\n",
    "            j += 1\n",
    "\n",
    "        keep_count = min(repetition_count, 2) if repetition_count <= 3 else 1\n",
    "        for _ in range(keep_count):\n",
    "            cleaned_words.append(words[i])\n",
    "        i += repetition_count\n",
    "\n",
    "    text = ' '.join(cleaned_words)\n",
    "\n",
    "    # === 2. Remove filler sounds (non-verbal, repetitive) ===\n",
    "    filler_sounds = [\"uh\", \"um\", \"mm\", \"hmm\", \"ah\", \"oh\", \"huh\", \"ha ha\"]\n",
    "    soft_fillers = [\"okay okay\", \"yes yes\", \"yes yes yes\", \"i mean\", \"you know\", \"like like\", \"ok ok\"]\n",
    "\n",
    "    for filler in filler_sounds + soft_fillers:\n",
    "        text = re.sub(rf'\\b{re.escape(filler)}\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # === 3. Insurance domain term normalization ===\n",
    "    corrections = {\n",
    "        'access max life': 'Axis Max Life',\n",
    "        'axis max life': 'Axis Max Life',\n",
    "        'g pay': 'GPay',\n",
    "        'google pay': 'Google Pay',\n",
    "        'phone pay': 'PhonePe',\n",
    "        'phone pe': 'PhonePe',\n",
    "        'pay tm': 'Paytm',\n",
    "        'net banking': 'netbanking',\n",
    "        'some assured': 'sum assured',\n",
    "        'premium do': 'premium due',\n",
    "        'do date': 'due date',\n",
    "        'okay sir': 'Okay sir',\n",
    "    }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    for wrong, correct in corrections.items():\n",
    "        text_lower = text_lower.replace(wrong, correct)\n",
    "\n",
    "    # === 3.5 Replace 'Rs.', 'Rs' → '₹' with optional space cleanup ===\n",
    "    text_lower = re.sub(r'\\brs[.]?\\s*', '₹', text_lower)\n",
    "\n",
    "    # === 4. Punctuation cleanup ===\n",
    "    text_lower = re.sub(r'\\s{2,}', ' ', text_lower)          # Extra spaces\n",
    "    text_lower = re.sub(r'[,]{2,}', ',', text_lower)         # Repeated commas\n",
    "    text_lower = re.sub(r'\\s+,', ',', text_lower)            # Space before comma\n",
    "    text_lower = re.sub(r'\\s+\\.', '.', text_lower)           # Space before period\n",
    "    text_lower = re.sub(r'\\s+[!?]', lambda m: m.group(0).strip(), text_lower)\n",
    "\n",
    "    # === 5. Capitalize sentences ===\n",
    "    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n",
    "                        lambda m: m.group(1) + m.group(2).upper(),\n",
    "                        text_lower)\n",
    "\n",
    "    return text_lower.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-G7YeEU0TU"
   },
   "outputs": [],
   "source": [
    "def enhanced_whisper_transcription(audio_path):\n",
    "    \"\"\"\n",
    "    Enhanced Whisper transcription with optimal anti-repetition parameters\n",
    "    \"\"\"\n",
    "    print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n",
    "\n",
    "    prompt = (\n",
    "        \"This is a customer support call for Axis Maxlife Insurance. \"\n",
    "        \"We will discuss policy numbers, due date, fund value, sum assured, late fee, \"\n",
    "        \"and payment methods such as Google Pay, PhonePe, Paytm and net banking.\"\n",
    "    )\n",
    "\n",
    "    # Single optimal strategy - no need for multiple attempts\n",
    "    result = model.transcribe(\n",
    "        audio_path,\n",
    "        language=\"ta\",\n",
    "        task=\"translate\",\n",
    "        temperature=0.0,\n",
    "        beam_size=5,\n",
    "        patience=1.2,\n",
    "        condition_on_previous_text=False,\n",
    "        no_speech_threshold=0.8,\n",
    "        compression_ratio_threshold=2.0,\n",
    "        logprob_threshold=-0.35,\n",
    "        word_timestamps=False,\n",
    "        initial_prompt=prompt,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(\"✅ Whisper transcription completed with optimal parameters\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_repetition_score(segments):\n",
    "    \"\"\"\n",
    "    Calculate a repetition score for transcription segments\n",
    "    Lower score = less repetition = better\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return 0.0\n",
    "\n",
    "    total_repetition = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for segment in segments:\n",
    "        text = segment.get('text', '').strip().lower()\n",
    "        words = text.split()\n",
    "\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "\n",
    "        total_words += len(words)\n",
    "\n",
    "        # Count immediate word repetitions\n",
    "        for i in range(len(words) - 1):\n",
    "            if words[i] == words[i + 1]:\n",
    "                total_repetition += 1\n",
    "\n",
    "        # Count phrase repetitions within segment\n",
    "        for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n",
    "            for start in range(len(words) - phrase_len * 2 + 1):\n",
    "                phrase1 = ' '.join(words[start:start + phrase_len])\n",
    "                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n",
    "                if phrase1 == phrase2:\n",
    "                    total_repetition += phrase_len * 2  # Heavy penalty\n",
    "\n",
    "    return total_repetition / max(total_words, 1)\n",
    "\n",
    "def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n",
    "    \"\"\"\n",
    "    AGGRESSIVE post-processing function to detect and remove repetitive segments\n",
    "    \"\"\"\n",
    "    print(\"🔍 Starting aggressive repetition detection...\")\n",
    "    cleaned_segments = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        text = segment['text'].strip()\n",
    "        words = text.split()\n",
    "\n",
    "        # Skip very short segments\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "\n",
    "        # AGGRESSIVE: Check for excessive word repetition\n",
    "        is_repetitive = False\n",
    "\n",
    "        # Count word frequencies\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_lower = word.lower().strip('.,!?')\n",
    "            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n",
    "\n",
    "        # Check if any single word dominates the segment\n",
    "        max_word_count = max(word_counts.values()) if word_counts else 0\n",
    "        word_dominance = max_word_count / len(words) if words else 0\n",
    "\n",
    "        if word_dominance > 0.4:  # If any word is >40% of the segment\n",
    "            print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n",
    "            continue\n",
    "\n",
    "        # Check for immediate repetitions (same word repeated consecutively)\n",
    "        consecutive_repeats = 0\n",
    "        max_consecutive = 0\n",
    "\n",
    "        for j in range(1, len(words)):\n",
    "            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n",
    "                consecutive_repeats += 1\n",
    "                max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n",
    "            else:\n",
    "                consecutive_repeats = 0\n",
    "\n",
    "        if max_consecutive > 3:  # More than 3 consecutive identical words\n",
    "            print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n",
    "            continue\n",
    "\n",
    "        # Check for pattern repetitions within segment\n",
    "        for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n",
    "            for start in range(len(words) - phrase_len * 2 + 1):\n",
    "                phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n",
    "                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n",
    "\n",
    "                if phrase1 == phrase2:\n",
    "                    repetition_coverage = (phrase_len * 2) / len(words)\n",
    "                    if repetition_coverage > max_repetition_ratio:\n",
    "                        print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n",
    "                        is_repetitive = True\n",
    "                        break\n",
    "            if is_repetitive:\n",
    "                break\n",
    "\n",
    "        if is_repetitive:\n",
    "            continue\n",
    "\n",
    "        # Check for similarity with recent segments (avoid near-duplicates)\n",
    "        is_near_duplicate = False\n",
    "        for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n",
    "            prev_words = prev_segment['text'].lower().split()\n",
    "            current_words = [w.lower() for w in words]\n",
    "\n",
    "            if prev_words and current_words:\n",
    "                # Calculate Jaccard similarity\n",
    "                prev_set = set(prev_words)\n",
    "                current_set = set(current_words)\n",
    "                intersection = len(prev_set.intersection(current_set))\n",
    "                union = len(prev_set.union(current_set))\n",
    "\n",
    "                similarity = intersection / union if union > 0 else 0\n",
    "\n",
    "                if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n",
    "                    print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n",
    "                    is_near_duplicate = True\n",
    "                    break\n",
    "\n",
    "        if is_near_duplicate:\n",
    "            continue\n",
    "\n",
    "        # If we reach here, the segment passed all checks\n",
    "        cleaned_segments.append(segment)\n",
    "\n",
    "    removed_count = len(segments) - len(cleaned_segments)\n",
    "    print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n",
    "    print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n",
    "\n",
    "    return cleaned_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "error",
     "timestamp": 1754630011345,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "wB3OJ0-WU6kA",
    "outputId": "25dd1ba0-7a6b-4bd6-bdfb-de428e3190bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_AUDIO_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2347374777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-2347374777.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Step 1: Smart Audio Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msmart_audio_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_AUDIO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLEAN_AUDIO_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"❌ Audio preprocessing failed completely. Exiting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'INPUT_AUDIO_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main processing pipeline with repetition prevention\"\"\"\n",
    "    print(\"🎯 Starting Enhanced Audio Processing Pipeline (Anti-Repetition)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Smart Audio Preprocessing\n",
    "    if not smart_audio_preprocessing(INPUT_AUDIO_PATH, CLEAN_AUDIO_PATH):\n",
    "        print(\"❌ Audio preprocessing failed completely. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # model = whisper.load_model(\"large\")\n",
    "\n",
    "    # Step 2: Enhanced Whisper Transcription with anti-repetition\n",
    "    try:\n",
    "        whisper_result = enhanced_whisper_transcription(CLEAN_AUDIO_PATH)\n",
    "        print(\"✅ Whisper transcription completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Whisper transcription failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Remove repetitive segments BEFORE post-processing\n",
    "    print(\"\\n--- Removing Repetitive Segments ---\")\n",
    "    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n",
    "\n",
    "    # Step 4: Post-process remaining transcription\n",
    "    processed_segments = []\n",
    "    for segment in cleaned_segments:\n",
    "        processed_text = post_process_text(segment['text'])\n",
    "        if processed_text.strip() and len(processed_text.strip()) > 5:  # Only keep meaningful segments\n",
    "            segment_copy = segment.copy()\n",
    "            segment_copy['text'] = processed_text\n",
    "            processed_segments.append(segment_copy)\n",
    "\n",
    "    whisper_result[\"segments\"] = processed_segments\n",
    "\n",
    "    # Step 5: Speaker Diarization\n",
    "    print(\"\\n--- Speaker Diarization ---\")\n",
    "    try:\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"✅ Using GPU for diarization\")\n",
    "\n",
    "        diarization = pipeline(CLEAN_AUDIO_PATH)\n",
    "        print(\"✅ Speaker diarization completed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Speaker diarization failed: {e}\")\n",
    "        diarization = None\n",
    "\n",
    "    # Step 6: Generate Enhanced Dialogue\n",
    "    print(\"\\n--- Generating Dialogue ---\")\n",
    "\n",
    "    def get_dominant_speaker(start_time, end_time, diarization_result):\n",
    "        if not diarization_result:\n",
    "            return \"Speaker_Unknown\"\n",
    "\n",
    "        speakers = {}\n",
    "        for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "            overlap_start = max(start_time, segment.start)\n",
    "            overlap_end = min(end_time, segment.end)\n",
    "            overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap_duration > 0:\n",
    "                speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n",
    "\n",
    "        return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n",
    "\n",
    "    # Combine segments by speaker\n",
    "    dialogue = []\n",
    "    current_speaker = None\n",
    "    current_texts = []\n",
    "    current_start = 0\n",
    "    current_end = 0\n",
    "\n",
    "    for segment in processed_segments:\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        text = segment['text'].strip()\n",
    "\n",
    "        speaker = get_dominant_speaker(start, end, diarization)\n",
    "\n",
    "        # Merge consecutive segments from same speaker (within 3 seconds)\n",
    "        if (speaker == current_speaker and\n",
    "            current_speaker and\n",
    "            (start - current_end) < 3.0):\n",
    "            current_texts.append(text)\n",
    "            current_end = end\n",
    "        else:\n",
    "            # Save previous speaker's dialogue\n",
    "            if current_speaker and current_texts:\n",
    "                combined_text = ' '.join(current_texts)\n",
    "                # Final check for repetition in combined text\n",
    "                if len(combined_text.strip()) > 10:  # Only keep substantial dialogue\n",
    "                    dialogue.append({\n",
    "                        'speaker': current_speaker,\n",
    "                        'text': combined_text,\n",
    "                        'start_time': current_start,\n",
    "                        'end_time': current_end\n",
    "                    })\n",
    "\n",
    "            # Start new speaker segment\n",
    "            current_speaker = speaker\n",
    "            current_texts = [text]\n",
    "            current_start = start\n",
    "            current_end = end\n",
    "\n",
    "    # Add final segment\n",
    "    if current_speaker and current_texts:\n",
    "        combined_text = ' '.join(current_texts)\n",
    "        if len(combined_text.strip()) > 10:\n",
    "            dialogue.append({\n",
    "                'speaker': current_speaker,\n",
    "                'text': combined_text,\n",
    "                'start_time': current_start,\n",
    "                'end_time': current_end\n",
    "            })\n",
    "\n",
    "    # Step 7: Display Results\n",
    "    print(\"\\n\" + \"🎭 DIALOGUE OUTPUT\" + \"=\" * 40)\n",
    "\n",
    "    for entry in dialogue:\n",
    "        timestamp = f\"[{entry['start_time']:.1f}s - {entry['end_time']:.1f}s]\"\n",
    "        print(f\"\\n{entry['speaker']} {timestamp}:\")\n",
    "        print(f\"  📝 {entry['text']}\")\n",
    "\n",
    "    # Step 8: Save Results\n",
    "    output_data = {\n",
    "        'metadata': {\n",
    "            'total_duration': whisper_result.get('duration', 0),\n",
    "            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n",
    "            'total_segments': len(dialogue),\n",
    "            'model_used': 'whisper-large',\n",
    "            'processing_successful': True,\n",
    "            'anti_repetition_applied': True\n",
    "        },\n",
    "        'dialogue': dialogue,\n",
    "        'raw_transcription': whisper_result\n",
    "    }\n",
    "\n",
    "    with open('enhanced_transcription_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n💾 Results saved to: enhanced_transcription_results.json\")\n",
    "    print(\"✅ Processing completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2hw-ivHGS5B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOSkqbhlXhnk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD7ObD_6Xhp3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6zyR7XxXhsa"
   },
   "outputs": [],
   "source": [
    "#batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d10237ffeee54143a8867720bbba2029",
      "20dd7c3c5c6f4ad8904bd4548edbcb4e",
      "977e243d84ef4638935e3f85c4a11c67",
      "856c8a2724ad4595a85daae1cef34d82",
      "17ec391a97ef443fba437924edde0d03",
      "d914e6f43d924394a3122ebf5b949b44",
      "6a4b5229c6b94f0cb2bc1727142ef155",
      "5a0c6d2f66a24ea981789126c7f91146",
      "70dd828fe8e04b15be916b39b913dae1",
      "4ba2eb80e3734d4eb1f494163f4411b6",
      "be624d5768964419a1ccf01f9d105f35",
      "b5ac944d7b5b419e9898100903a8d0c7",
      "cc9aa7c3c2874bb2abe16ca2e9643ca7",
      "3202232ed489444982be169ece3b7dc6",
      "7d20e5f47be74c728eb2a99eab7396f8",
      "441be9abb6f04d4d8b6da4b75fd5ef79",
      "06a4f8f74ae840bd9b33d277b29d19fa",
      "c2477333d094485692244213506d7e87",
      "9c72c1f89f5f4fb5b92d9c3d5e1168bd",
      "bf2be1e19c2a4b55bf21a9ecd339f346",
      "2ab8dcf697354b15ad8ff0d7b3841598",
      "3dbb4e2d3f624f2d863f64203a03f6ee",
      "c38f9c88b1be4f5c8c9d2af115ac5f24",
      "c575feeb29314c919c79c05b9922e88e",
      "89c48922ce334cdb83022427b2c184a3",
      "495893605e63493a94339f41aabd36a3",
      "88886b92de45421fb2d22513e3e67b1d",
      "baad9f91a5a442f58bd981e373af3222",
      "2a227cf72a4e48aaafa797153dc5e624",
      "c5addaccc45547399b275a877e6f5211",
      "3d082d749881455881ef9ca21b6562b8",
      "d444071da19f44c59e3b8b092fa56930",
      "e61b2eb45ec94c449fdfcb537c13f8ff",
      "456e2c599a3546b886b3d884607c5ebf",
      "20cfcff9ab1c460c90c2e846b5c94c7f",
      "ccef9633e3e14d408ae5c82967ae4bf1",
      "e171bc23a88e463eabf79bd82c731b46",
      "fbe0f8112b6d4f479e14c3d8b5c1c3fa",
      "24449a6253414ea69258985f6665f0fc",
      "cf09d73f205a49419ad57e604637f795",
      "210bef0525cf4c32a4639ff9903470f0",
      "aa1f2b4ef26a4466904109301d222f11",
      "6ca23776586443caad86e5e09434019a",
      "90c6b45569184020984478ff76ec0524",
      "dab5836c9c3749e7a67c43742a589591",
      "ae2ac41020974c64bfd33d3f5115d96f",
      "521642c9979640888c65f61a7fe26058",
      "12d05bbab54c40b9b30366bb4f091c43",
      "47d2edce309b42bba0d2d53e29336049",
      "d23e9dd7c83743699b8a98ab76291adb",
      "1b83e8734ae449818923a6972f6ccf8f",
      "0f4533b64f9c4eab8cffeb81285f9e68",
      "7e4d24bac1714cd394a7d6301f5b4b68",
      "15f9ac163bdf4a84beb8628292c47b92",
      "1aa528fa88524a1cba978bbf0b168679"
     ]
    },
    "executionInfo": {
     "elapsed": 198917,
     "status": "ok",
     "timestamp": 1754630233642,
     "user": {
      "displayName": "VBS Analytics",
      "userId": "12942825704271409009"
     },
     "user_tz": -330
    },
    "id": "Tvhl3CAOXjR4",
    "outputId": "aac5adf5-21a1-4ddb-bf6a-2bb43d893b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 2 audio files in: training_data\n",
      "\n",
      "======================================================================\n",
      "🚀 Processing: call2.wav\n",
      "Original audio duration: 190.76 seconds\n",
      "--- Trying Advanced Audio Preprocessing ---\n",
      "Advanced preprocessing successful\n",
      "Processed audio duration: 190.74 seconds\n",
      "✅ Audio preprocessing successful with method 1\n",
      "--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n",
      "[00:00.000 --> 00:10.000]  Hello! Greetings! My name is Aathi. We are calling from Licensure. This is an email call. You have taken a policy from Axis Maxlife Insurance. Can you speak for 2 minutes?\n",
      "[00:10.000 --> 00:18.000]  Yes, Madam. Quick call. I can't get it. If we cancel the policy, will it be refunded?\n",
      "[00:18.000 --> 00:23.000]  Okay. I will inform you about the details of your policy in a short time.\n",
      "[00:23.000 --> 00:27.000]  Okay.\n",
      "[00:27.000 --> 00:29.000]  Okay. Can you speak in Himalayan?\n",
      "[00:29.000 --> 00:31.000]  Yes.\n",
      "[00:31.000 --> 00:33.000]  Okay. Can you tell me the reason why you did not commit the crime?\n",
      "[00:33.000 --> 00:35.000]  I did not have a job.\n",
      "[00:35.000 --> 00:37.000]  I did not have a job.\n",
      "[00:37.000 --> 00:39.000]  Okay.\n",
      "[00:39.000 --> 00:41.000]  Okay. I understand the situation.\n",
      "[00:41.000 --> 00:43.000]  You have already paid for a year.\n",
      "[00:43.000 --> 00:45.000]  When you surrender after paying for the first time,\n",
      "[00:45.000 --> 00:47.000]  do you have any attempts?\n",
      "[00:47.000 --> 00:49.000]  Because for the surrender value to be generated,\n",
      "[00:49.000 --> 00:51.000]  you should have paid for at least 3 years.\n",
      "[00:51.000 --> 00:53.000]  So, usually, if you pay for at least 3 years\n",
      "[00:53.000 --> 00:55.000]  and surrender,\n",
      "[00:55.000 --> 00:57.000]  They will give you a surrender value of some amount.\n",
      "[00:57.000 --> 01:01.000]  But when you surrender after first year payment, there won't be any attempts.\n",
      "[01:01.000 --> 01:04.000]  You can at least take some time to continue.\n",
      "[01:04.000 --> 01:08.000]  How much will it cost if I do this now?\n",
      "[01:08.000 --> 01:14.000]  That's it. When you surrender after first year payment, there won't be any attempts.\n",
      "[01:14.000 --> 01:17.000]  You are saying that the amount will go like that.\n",
      "[01:17.000 --> 01:24.000]  Yes. Because if you pay for at least 3 years and surrender, then only the cash surrender value will be generated.\n",
      "[01:24.000 --> 01:28.000]  So, when you surrender, they will give you the surrender amount accordingly.\n",
      "[01:28.000 --> 01:32.000]  When you surrender with the first payment, will there be any other benefits?\n",
      "[01:32.000 --> 01:37.000]  Can you please continue with the timer for a while?\n",
      "[01:37.000 --> 01:39.000]  Yes, I will continue.\n",
      "[01:39.000 --> 01:42.000]  Okay, please tell me what you are thinking.\n",
      "[01:42.000 --> 01:45.000]  There are lapses in the policy status and there are holes in the summaries.\n",
      "[01:45.000 --> 01:47.000]  There will be no benefit in the lapses.\n",
      "[01:47.000 --> 01:51.000]  In case, there is a sudden policy change, there will be no benefit for us.\n",
      "[01:51.000 --> 01:59.000]  If the delay is too high according to the lay payment charges, you will have to pay the payment soon.\n",
      "[01:59.000 --> 02:04.000]  Your policy number is 149607129.\n",
      "[02:04.000 --> 02:08.000]  Next is Smart Wealth Advantage Growth Per Price Insta Income Fixed Return Policy.\n",
      "[02:08.000 --> 02:13.000]  You have to pay Rs.66,537 to cross Avanadu 102024.\n",
      "[02:43.000 --> 02:45.000]  Okay, I will call you back.\n",
      "[02:45.000 --> 02:50.000]  Okay ma'am. In case you need any special services, you can contact the branch.\n",
      "[02:50.000 --> 02:55.000]  I will submit the health declaration form. I will arrange for a call back next week.\n",
      "[02:55.000 --> 02:58.000]  Just inform me what you need to know.\n",
      "[02:58.000 --> 02:59.000]  Okay ma'am.\n",
      "[02:59.000 --> 03:01.000]  Any other questions?\n",
      "[03:01.000 --> 03:02.000]  No, ma'am.\n",
      "[03:02.000 --> 03:05.000]  Do you want to update anything via alternate mobile?\n",
      "[03:05.000 --> 03:06.000]  No, ma'am.\n",
      "[03:06.000 --> 03:10.000]  Okay ma'am. I will try my best to access you. Thank you.\n",
      "✅ Whisper transcription completed with optimal parameters\n",
      "🔍 Starting aggressive repetition detection...\n",
      "🚫 Rejecting near-duplicate: I did not have a job.... (similarity: 1.00)\n",
      "🚫 Rejecting word-dominated segment: and surrender,... (dominance: 0.50)\n",
      "🚫 Rejecting near-duplicate: That's it. When you surrender ... (similarity: 0.80)\n",
      "🚫 Rejecting word-dominated segment: Okay ma'am.... (dominance: 0.50)\n",
      "🚫 Rejecting word-dominated segment: No, ma'am.... (dominance: 0.50)\n",
      "🚫 Rejecting word-dominated segment: No, ma'am.... (dominance: 0.50)\n",
      "📊 Aggressive cleaning: 47 → 38 segments\n",
      "🗑️  Removed 9 repetitive/problematic segments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10237ffeee54143a8867720bbba2029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ac944d7b5b419e9898100903a8d0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38f9c88b1be4f5c8c9d2af115ac5f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456e2c599a3546b886b3d884607c5ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab5836c9c3749e7a67c43742a589591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved transcription: processed_outputs/call2_transcription.json\n",
      "\n",
      "======================================================================\n",
      "🚀 Processing: call1.wav\n",
      "Original audio duration: 106.92 seconds\n",
      "--- Trying Advanced Audio Preprocessing ---\n",
      "Advanced preprocessing successful\n",
      "Processed audio duration: 106.89 seconds\n",
      "✅ Audio preprocessing successful with method 1\n",
      "--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n",
      "[00:00.000 --> 00:09.000]  Hello sir, I am Jai Prakash from Axis Maxlife. I am calling from Govindhara.\n",
      "[00:09.000 --> 00:14.000]  We have a renewal call for policy DVR. Shall we talk about the policy details?\n",
      "[00:44.000 --> 00:46.000]  It has been paid up.\n",
      "[00:46.000 --> 00:52.000]  Rs.1,14,713.47\n",
      "[00:54.000 --> 00:56.000]  The policy has been paid up.\n",
      "[00:56.000 --> 00:57.000]  It is not active.\n",
      "[00:57.000 --> 00:58.000]  It is not safe.\n",
      "[00:58.000 --> 00:59.000]  It is not safe.\n",
      "[00:59.000 --> 01:00.000]  It is not safe.\n",
      "[01:00.000 --> 01:01.000]  It is not safe.\n",
      "[01:01.000 --> 01:02.000]  It is not safe.\n",
      "[01:02.000 --> 01:03.000]  It is not safe.\n",
      "[01:03.000 --> 01:04.000]  It is not safe.\n",
      "[01:04.000 --> 01:05.000]  It is not safe.\n",
      "[01:05.000 --> 01:06.000]  It is not safe.\n",
      "[01:06.000 --> 01:07.000]  It is not safe.\n",
      "[01:07.000 --> 01:08.000]  It is not safe.\n",
      "[01:08.000 --> 01:09.000]  It is not safe.\n",
      "[01:09.000 --> 01:10.000]  It is not safe.\n",
      "[01:10.000 --> 01:11.000]  It is not safe.\n",
      "[01:11.000 --> 01:13.000]  ok\n",
      "[01:13.000 --> 01:15.000]  hmm\n",
      "[01:15.000 --> 01:17.000]  ok\n",
      "[01:17.000 --> 01:19.000]  ok\n",
      "[01:19.000 --> 01:21.000]  ok\n",
      "[01:21.000 --> 01:23.000]  ok\n",
      "[01:23.000 --> 01:25.000]  ok\n",
      "[01:25.000 --> 01:27.000]  ok\n",
      "[01:27.000 --> 01:29.000]  ok\n",
      "[01:29.000 --> 01:31.000]  ok\n",
      "[01:31.000 --> 01:33.000]  ok\n",
      "[01:33.000 --> 01:35.000]  ok\n",
      "[01:35.000 --> 01:37.000]  ok\n",
      "[01:37.000 --> 01:38.000]  Okay, sir.\n",
      "[01:38.000 --> 01:39.000]  Okay.\n",
      "[01:39.000 --> 01:40.000]  Thank you.\n",
      "[01:40.000 --> 01:41.000]  Thank you.\n",
      "[01:41.000 --> 01:42.000]  Bye.\n",
      "[01:42.000 --> 01:43.000]  Bye.\n",
      "[01:43.000 --> 01:44.000]  Bye.\n",
      "[01:44.000 --> 01:45.000]  Bye.\n",
      "[01:45.000 --> 01:46.000]  Bye.\n",
      "[01:46.000 --> 01:47.000]  Bye.\n",
      "[01:47.000 --> 01:48.000]  Bye.\n",
      "[01:48.000 --> 01:49.000]  Bye.\n",
      "[01:49.000 --> 01:50.000]  Bye.\n",
      "[01:50.000 --> 01:51.000]  Bye.\n",
      "[01:51.000 --> 01:52.000]  Bye.\n",
      "[01:52.000 --> 01:53.000]  Bye.\n",
      "[01:53.000 --> 01:54.000]  Bye.\n",
      "[01:54.000 --> 01:55.000]  Bye.\n",
      "[01:55.000 --> 01:56.000]  Bye.\n",
      "[01:56.000 --> 01:57.000]  Bye.\n",
      "[01:57.000 --> 01:58.000]  Bye.\n",
      "[01:58.000 --> 01:59.000]  Bye.\n",
      "[01:59.000 --> 02:00.000]  Bye.\n",
      "[02:00.000 --> 02:01.000]  Bye.\n",
      "[02:01.000 --> 02:02.000]  Bye.\n",
      "[02:02.000 --> 02:03.000]  Bye.\n",
      "[02:03.000 --> 02:04.000]  Bye.\n",
      "[02:04.000 --> 02:05.000]  Bye.\n",
      "[02:05.000 --> 02:06.000]  Bye.\n",
      "✅ Whisper transcription completed with optimal parameters\n",
      "🔍 Starting aggressive repetition detection...\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting near-duplicate: It is not safe.... (similarity: 1.00)\n",
      "🚫 Rejecting word-dominated segment: Okay, sir.... (dominance: 0.50)\n",
      "🚫 Rejecting word-dominated segment: Thank you.... (dominance: 0.50)\n",
      "🚫 Rejecting word-dominated segment: Thank you.... (dominance: 0.50)\n",
      "📊 Aggressive cleaning: 62 → 6 segments\n",
      "🗑️  Removed 56 repetitive/problematic segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved transcription: processed_outputs/call1_transcription.json\n"
     ]
    }
   ],
   "source": [
    "def process_folder(input_folder, output_folder):\n",
    "    \"\"\"Process all audio files in a folder and generate per-file JSON outputs\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    audio_files = [f for f in os.listdir(input_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "    print(f\"📁 Found {len(audio_files)} audio files in: {input_folder}\")\n",
    "\n",
    "    for filename in audio_files:\n",
    "        try:\n",
    "            input_audio_path = os.path.join(input_folder, filename)\n",
    "            cleaned_audio_path = os.path.join(output_folder, f\"cleaned_{filename}\")\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            output_json_path = os.path.join(output_folder, f\"{base_name}_transcription.json\")\n",
    "\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"🚀 Processing: {filename}\")\n",
    "\n",
    "            # Preprocessing\n",
    "            if not smart_audio_preprocessing(input_audio_path, cleaned_audio_path):\n",
    "                print(f\"❌ Skipping {filename} due to preprocessing failure.\")\n",
    "                continue\n",
    "\n",
    "            # Transcription\n",
    "            whisper_result = enhanced_whisper_transcription(cleaned_audio_path)\n",
    "            cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n",
    "\n",
    "            processed_segments = []\n",
    "            for segment in cleaned_segments:\n",
    "                processed_text = post_process_text(segment['text'])\n",
    "                if processed_text.strip() and len(processed_text.strip()) > 5:\n",
    "                    segment_copy = segment.copy()\n",
    "                    segment_copy['text'] = processed_text\n",
    "                    processed_segments.append(segment_copy)\n",
    "\n",
    "            whisper_result[\"segments\"] = processed_segments\n",
    "\n",
    "            # Diarization\n",
    "            try:\n",
    "                pipeline = Pipeline.from_pretrained(\n",
    "                    \"pyannote/speaker-diarization-3.1\",\n",
    "                    use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n",
    "                )\n",
    "                if torch.cuda.is_available():\n",
    "                    pipeline.to(torch.device(\"cuda\"))\n",
    "                diarization = pipeline(cleaned_audio_path)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Diarization failed for {filename}: {e}\")\n",
    "                diarization = None\n",
    "\n",
    "            # Dialogue generation\n",
    "            def get_dominant_speaker(start_time, end_time, diarization_result):\n",
    "                if not diarization_result:\n",
    "                    return \"Speaker_Unknown\"\n",
    "                speakers = {}\n",
    "                for segment, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "                    overlap_start = max(start_time, segment.start)\n",
    "                    overlap_end = min(end_time, segment.end)\n",
    "                    overlap_duration = max(0, overlap_end - overlap_start)\n",
    "                    if overlap_duration > 0:\n",
    "                        speakers[speaker] = speakers.get(speaker, 0) + overlap_duration\n",
    "                return max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n",
    "\n",
    "            dialogue = []\n",
    "            current_speaker = None\n",
    "            current_texts = []\n",
    "            current_start = 0\n",
    "            current_end = 0\n",
    "\n",
    "            for segment in processed_segments:\n",
    "                start = segment['start']\n",
    "                end = segment['end']\n",
    "                text = segment['text'].strip()\n",
    "                speaker = get_dominant_speaker(start, end, diarization)\n",
    "\n",
    "                if (speaker == current_speaker and current_speaker and (start - current_end) < 3.0):\n",
    "                    current_texts.append(text)\n",
    "                    current_end = end\n",
    "                else:\n",
    "                    if current_speaker and current_texts:\n",
    "                        combined_text = ' '.join(current_texts)\n",
    "                        if len(combined_text.strip()) > 10:\n",
    "                            dialogue.append({\n",
    "                                'speaker': current_speaker,\n",
    "                                'text': combined_text,\n",
    "                                'start_time': current_start,\n",
    "                                'end_time': current_end\n",
    "                            })\n",
    "                    current_speaker = speaker\n",
    "                    current_texts = [text]\n",
    "                    current_start = start\n",
    "                    current_end = end\n",
    "\n",
    "            if current_speaker and current_texts:\n",
    "                combined_text = ' '.join(current_texts)\n",
    "                if len(combined_text.strip()) > 10:\n",
    "                    dialogue.append({\n",
    "                        'speaker': current_speaker,\n",
    "                        'text': combined_text,\n",
    "                        'start_time': current_start,\n",
    "                        'end_time': current_end\n",
    "                    })\n",
    "\n",
    "            output_data = {\n",
    "                'metadata': {\n",
    "                    'file': filename,\n",
    "                    'total_duration': whisper_result.get('duration', 0),\n",
    "                    'total_speakers': len(set(d['speaker'] for d in dialogue)),\n",
    "                    'total_segments': len(dialogue),\n",
    "                    'model_used': 'whisper-large',\n",
    "                    'processing_successful': True,\n",
    "                    'anti_repetition_applied': True\n",
    "                },\n",
    "                'dialogue': dialogue,\n",
    "                'raw_transcription': whisper_result\n",
    "            }\n",
    "\n",
    "            with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\"✅ Saved transcription: {output_json_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "# ✅ Call the batch processor\n",
    "if __name__ == \"__main__\":\n",
    "    process_folder(\"training_data\", \"processed_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_iv_MKWLXovJ"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Zip the folder into processed_outputs.zip\n",
    "# shutil.make_archive('processed_outputs', 'zip', 'processed_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN2aRHziXut7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# # Download the zipped folder\n",
    "# files.download('processed_outputs.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxJsHn0C895/ZFzTi4hG2X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06a4f8f74ae840bd9b33d277b29d19fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f4533b64f9c4eab8cffeb81285f9e68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12d05bbab54c40b9b30366bb4f091c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15f9ac163bdf4a84beb8628292c47b92",
      "placeholder": "​",
      "style": "IPY_MODEL_1aa528fa88524a1cba978bbf0b168679",
      "value": " 221/221 [00:00&lt;00:00, 24.7kB/s]"
     }
    },
    "15f9ac163bdf4a84beb8628292c47b92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17ec391a97ef443fba437924edde0d03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aa528fa88524a1cba978bbf0b168679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b83e8734ae449818923a6972f6ccf8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20cfcff9ab1c460c90c2e846b5c94c7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24449a6253414ea69258985f6665f0fc",
      "placeholder": "​",
      "style": "IPY_MODEL_cf09d73f205a49419ad57e604637f795",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "20dd7c3c5c6f4ad8904bd4548edbcb4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d914e6f43d924394a3122ebf5b949b44",
      "placeholder": "​",
      "style": "IPY_MODEL_6a4b5229c6b94f0cb2bc1727142ef155",
      "value": "config.yaml: 100%"
     }
    },
    "210bef0525cf4c32a4639ff9903470f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24449a6253414ea69258985f6665f0fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a227cf72a4e48aaafa797153dc5e624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ab8dcf697354b15ad8ff0d7b3841598": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3202232ed489444982be169ece3b7dc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c72c1f89f5f4fb5b92d9c3d5e1168bd",
      "max": 5905440,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf2be1e19c2a4b55bf21a9ecd339f346",
      "value": 5905440
     }
    },
    "3d082d749881455881ef9ca21b6562b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3dbb4e2d3f624f2d863f64203a03f6ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "441be9abb6f04d4d8b6da4b75fd5ef79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "456e2c599a3546b886b3d884607c5ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20cfcff9ab1c460c90c2e846b5c94c7f",
       "IPY_MODEL_ccef9633e3e14d408ae5c82967ae4bf1",
       "IPY_MODEL_e171bc23a88e463eabf79bd82c731b46"
      ],
      "layout": "IPY_MODEL_fbe0f8112b6d4f479e14c3d8b5c1c3fa"
     }
    },
    "47d2edce309b42bba0d2d53e29336049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "495893605e63493a94339f41aabd36a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d444071da19f44c59e3b8b092fa56930",
      "placeholder": "​",
      "style": "IPY_MODEL_e61b2eb45ec94c449fdfcb537c13f8ff",
      "value": " 399/399 [00:00&lt;00:00, 35.3kB/s]"
     }
    },
    "4ba2eb80e3734d4eb1f494163f4411b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "521642c9979640888c65f61a7fe26058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4533b64f9c4eab8cffeb81285f9e68",
      "max": 221,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e4d24bac1714cd394a7d6301f5b4b68",
      "value": 221
     }
    },
    "5a0c6d2f66a24ea981789126c7f91146": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a4b5229c6b94f0cb2bc1727142ef155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ca23776586443caad86e5e09434019a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70dd828fe8e04b15be916b39b913dae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d20e5f47be74c728eb2a99eab7396f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ab8dcf697354b15ad8ff0d7b3841598",
      "placeholder": "​",
      "style": "IPY_MODEL_3dbb4e2d3f624f2d863f64203a03f6ee",
      "value": " 5.91M/5.91M [00:01&lt;00:00, 4.43MB/s]"
     }
    },
    "7e4d24bac1714cd394a7d6301f5b4b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "856c8a2724ad4595a85daae1cef34d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ba2eb80e3734d4eb1f494163f4411b6",
      "placeholder": "​",
      "style": "IPY_MODEL_be624d5768964419a1ccf01f9d105f35",
      "value": " 469/469 [00:00&lt;00:00, 48.6kB/s]"
     }
    },
    "88886b92de45421fb2d22513e3e67b1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89c48922ce334cdb83022427b2c184a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5addaccc45547399b275a877e6f5211",
      "max": 399,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d082d749881455881ef9ca21b6562b8",
      "value": 399
     }
    },
    "90c6b45569184020984478ff76ec0524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "977e243d84ef4638935e3f85c4a11c67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a0c6d2f66a24ea981789126c7f91146",
      "max": 469,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70dd828fe8e04b15be916b39b913dae1",
      "value": 469
     }
    },
    "9c72c1f89f5f4fb5b92d9c3d5e1168bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa1f2b4ef26a4466904109301d222f11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae2ac41020974c64bfd33d3f5115d96f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d23e9dd7c83743699b8a98ab76291adb",
      "placeholder": "​",
      "style": "IPY_MODEL_1b83e8734ae449818923a6972f6ccf8f",
      "value": "config.yaml: 100%"
     }
    },
    "b5ac944d7b5b419e9898100903a8d0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc9aa7c3c2874bb2abe16ca2e9643ca7",
       "IPY_MODEL_3202232ed489444982be169ece3b7dc6",
       "IPY_MODEL_7d20e5f47be74c728eb2a99eab7396f8"
      ],
      "layout": "IPY_MODEL_441be9abb6f04d4d8b6da4b75fd5ef79"
     }
    },
    "baad9f91a5a442f58bd981e373af3222": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be624d5768964419a1ccf01f9d105f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf2be1e19c2a4b55bf21a9ecd339f346": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c2477333d094485692244213506d7e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c38f9c88b1be4f5c8c9d2af115ac5f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c575feeb29314c919c79c05b9922e88e",
       "IPY_MODEL_89c48922ce334cdb83022427b2c184a3",
       "IPY_MODEL_495893605e63493a94339f41aabd36a3"
      ],
      "layout": "IPY_MODEL_88886b92de45421fb2d22513e3e67b1d"
     }
    },
    "c575feeb29314c919c79c05b9922e88e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baad9f91a5a442f58bd981e373af3222",
      "placeholder": "​",
      "style": "IPY_MODEL_2a227cf72a4e48aaafa797153dc5e624",
      "value": "config.yaml: 100%"
     }
    },
    "c5addaccc45547399b275a877e6f5211": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc9aa7c3c2874bb2abe16ca2e9643ca7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06a4f8f74ae840bd9b33d277b29d19fa",
      "placeholder": "​",
      "style": "IPY_MODEL_c2477333d094485692244213506d7e87",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "ccef9633e3e14d408ae5c82967ae4bf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_210bef0525cf4c32a4639ff9903470f0",
      "max": 26645418,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa1f2b4ef26a4466904109301d222f11",
      "value": 26645418
     }
    },
    "cf09d73f205a49419ad57e604637f795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d10237ffeee54143a8867720bbba2029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20dd7c3c5c6f4ad8904bd4548edbcb4e",
       "IPY_MODEL_977e243d84ef4638935e3f85c4a11c67",
       "IPY_MODEL_856c8a2724ad4595a85daae1cef34d82"
      ],
      "layout": "IPY_MODEL_17ec391a97ef443fba437924edde0d03"
     }
    },
    "d23e9dd7c83743699b8a98ab76291adb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d444071da19f44c59e3b8b092fa56930": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d914e6f43d924394a3122ebf5b949b44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dab5836c9c3749e7a67c43742a589591": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae2ac41020974c64bfd33d3f5115d96f",
       "IPY_MODEL_521642c9979640888c65f61a7fe26058",
       "IPY_MODEL_12d05bbab54c40b9b30366bb4f091c43"
      ],
      "layout": "IPY_MODEL_47d2edce309b42bba0d2d53e29336049"
     }
    },
    "e171bc23a88e463eabf79bd82c731b46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ca23776586443caad86e5e09434019a",
      "placeholder": "​",
      "style": "IPY_MODEL_90c6b45569184020984478ff76ec0524",
      "value": " 26.6M/26.6M [00:01&lt;00:00, 22.4MB/s]"
     }
    },
    "e61b2eb45ec94c449fdfcb537c13f8ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbe0f8112b6d4f479e14c3d8b5c1c3fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
