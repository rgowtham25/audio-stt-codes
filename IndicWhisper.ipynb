{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNEVg8ukPdULNZoAOlMX0rV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Upgrade pip first (optional but helpful)\n","!pip -q install --upgrade pip\n","\n","# (A) Keep your existing Torch (2.8.0+cu126) — just install the rest from PyPI\n","!pip -q install \"transformers>=4.45\" accelerate librosa soundfile jiwer\n","\n","# (Optional B) If you DO want to reinstall Torch explicitly, use the PyTorch index ONLY for Torch pkgs:\n","# !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n","# then install the rest from PyPI (as above).\n","\n","import torch, platform, importlib\n","print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Python:\", platform.python_version())\n","\n","# quick import checks\n","for pkg in [\"transformers\", \"librosa\", \"soundfile\", \"jiwer\"]:\n","    try:\n","        importlib.import_module(pkg)\n","        print(f\"import {pkg}: OK\")\n","    except Exception as e:\n","        print(f\"import {pkg}: FAILED -> {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxefloqb6ltD","executionInfo":{"status":"ok","timestamp":1756891881176,"user_tz":-330,"elapsed":10633,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"1066b03a-a138-4acd-cbc8-c99245b061e8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hTorch: 2.8.0+cu126 | CUDA available: True | Python: 3.12.11\n","import transformers: OK\n","import librosa: OK\n","import soundfile: OK\n","import jiwer: OK\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","import os, zipfile, glob\n","\n","# 1) Upload your ZIP from your Ubuntu machine\n","# uploaded = files.upload()  # choose your local ZIP (e.g., indicwhisper_ta_medium.zip)\n","# zip_name = next(iter(uploaded.keys()))\n","zip_name=\"/content/tamil_models.zip\"\n","print(\"Uploaded ZIP:\", zip_name)\n","\n","# 2) Extract it\n","EXTRACT_ROOT = \"/content/models_extracted\"\n","os.makedirs(EXTRACT_ROOT, exist_ok=True)\n","with zipfile.ZipFile(zip_name, 'r') as z:\n","    z.extractall(EXTRACT_ROOT)\n","\n","# 3) Find the directory that actually holds config + weights\n","def find_model_dir(root):\n","    for r, d, f in os.walk(root):\n","        has_cfg = \"config.json\" in f\n","        has_weights = (\"pytorch_model.bin\" in f) or any(x.endswith(\".safetensors\") for x in f)\n","        if has_cfg and has_weights:\n","            return r\n","    return None\n","\n","MODEL_DIR = find_model_dir(EXTRACT_ROOT)\n","assert MODEL_DIR is not None, \"Could not find a folder with config.json + weights. Check your ZIP.\"\n","print(\"MODEL_DIR:\", MODEL_DIR)\n","\n","# (Optional) peek at files inside\n","print(\"Files in MODEL_DIR:\", os.listdir(MODEL_DIR)[:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClUBJF_J6xxq","executionInfo":{"status":"ok","timestamp":1756892289258,"user_tz":-330,"elapsed":19022,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"3e15710f-5f86-479b-e14e-06aaac28d418"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Uploaded ZIP: /content/tamil_models.zip\n","MODEL_DIR: /content/models_extracted/tamil_models/whisper-medium-ta_alldata_multigpu\n","Files in MODEL_DIR: ['added_tokens.json', 'normalizer.json', 'training_args.bin', 'rng_state_2.pth', 'vocab.json', 'rng_state_1.pth', 'trainer_state.json', 'config.json', 'pytorch_model.bin', 'special_tokens_map.json']\n"]}]},{"cell_type":"code","source":["import os, time, math, gc, sys, torch\n","from pathlib import Path\n","from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","\n","assert 'MODEL_DIR' in globals(), \"Run Cell 2 first so MODEL_DIR is defined.\"\n","MODEL_DIR = Path(MODEL_DIR)\n","\n","device = 0 if torch.cuda.is_available() else -1\n","dtype  = torch.float16 if device == 0 else torch.float32\n","\n","# --- Inspect the folder so we know what we're loading ---\n","files = os.listdir(MODEL_DIR)\n","print(\"MODEL_DIR:\", MODEL_DIR)\n","print(\"Files:\", files)\n","\n","# Show sizes of relevant files\n","def human(n):\n","    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n","        if n < 1024: return f\"{n:.2f}{u}\"\n","        n/=1024\n","    return f\"{n:.2f}PB\"\n","\n","for name in [\"pytorch_model.bin\", \"pytorch_model.bin.index.json\"]:\n","    p = MODEL_DIR / name\n","    if p.exists():\n","        print(f\"{name}: {human(p.stat().st_size)}\")\n","safetensors = sorted(MODEL_DIR.glob(\"*.safetensors\"))\n","if safetensors:\n","    for p in safetensors:\n","        print(f\"{p.name}: {human(p.stat().st_size)}\")\n","else:\n","    print(\"No .safetensors found; will load .bin via torch (still ok).\")\n","\n","# --- Load processor: prefer local; fallback to base if missing ---\n","BASE_PROCESSOR_ID = \"openai/whisper-medium\"   # change if your finetune base is small/large\n","try:\n","    processor = WhisperProcessor.from_pretrained(MODEL_DIR, local_files_only=True)\n","    print(\"Processor loaded from ZIP ✅\")\n","except Exception as e:\n","    print(\"Processor not in ZIP; falling back to:\", BASE_PROCESSOR_ID, \"\\nReason:\", repr(e))\n","    processor = WhisperProcessor.from_pretrained(BASE_PROCESSOR_ID)\n","\n","# --- Memory hygiene before big load ---\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","# --- Choose best load parameters ---\n","use_safetensors = len(safetensors) > 0\n","\n","load_kwargs = dict(\n","    local_files_only=True,\n","    low_cpu_mem_usage=True,      # important\n","    torch_dtype=dtype,\n",")\n","# For .bin, ask torch to only read weights (faster/safer on Torch ≥ 2.0)\n","if not use_safetensors:\n","    load_kwargs[\"weights_only\"] = True\n","\n","t0 = time.time()\n","print(f\"\\nLoading model ({'safetensors' if use_safetensors else 'bin'})... this can take a minute for large checkpoints.\")\n","model = WhisperForConditionalGeneration.from_pretrained(\n","    MODEL_DIR,\n","    **load_kwargs\n",")\n","load_secs = time.time() - t0\n","print(f\"Model loaded in {load_secs:.1f}s ✅\")\n","\n","# Move to GPU if available\n","if device == 0:\n","    model = model.to(\"cuda\")\n","print(\"Device:\", \"cuda\" if device == 0 else \"cpu\", \"| dtype:\", dtype)\n","\n","# --- Force English transcript from Tamil audio ---\n","model.generation_config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n","    language=\"ta\", task=\"translate\"   # ta -> en\n",")\n","print(\"Forced source_language='ta', task='translate' (English output) ✅\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgeL9eu393IW","executionInfo":{"status":"ok","timestamp":1756894044188,"user_tz":-330,"elapsed":904133,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"e2da84d0-85c4-4d3e-d6b5-de096f4ba19f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["MODEL_DIR: /content/models_extracted/tamil_models/whisper-medium-ta_alldata_multigpu\n","Files: ['added_tokens.json', 'normalizer.json', 'training_args.bin', 'rng_state_2.pth', 'vocab.json', 'rng_state_1.pth', 'trainer_state.json', 'config.json', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'latest', 'rng_state_3.pth', 'rng_state_0.pth', 'merges.txt', 'preprocessor_config.json', 'generation_config.json']\n","pytorch_model.bin: 1.42GB\n","No .safetensors found; will load .bin via torch (still ok).\n","Processor loaded from ZIP ✅\n","\n","Loading model (bin)... this can take a minute for large checkpoints.\n","Model loaded in 902.6s ✅\n","Device: cuda | dtype: torch.float16\n","Forced source_language='ta', task='translate' (English output) ✅\n"]}]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","pipe = pipeline(\n","    task=\"automatic-speech-recognition\",\n","    model=model,\n","    tokenizer=processor.tokenizer,\n","    feature_extractor=processor.feature_extractor,\n","    chunk_length_s=30,        # if OOM, try 20 or 15\n","    stride_length_s=(4, 2),\n","    return_timestamps=True,\n","    device=device,\n","    batch_size=8\n",")\n","print(\"Pipeline ready ✅ (English transcripts will be produced)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KXXDN0E93FJ","executionInfo":{"status":"ok","timestamp":1756894183277,"user_tz":-330,"elapsed":21,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"70bab003-5182-4d59-c242-141edf321062"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"]},{"output_type":"stream","name":"stdout","text":["Pipeline ready ✅ (English transcripts will be produced)\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","import os, json\n","\n","#uploaded = files.upload()  # choose a local file like tamil_test.wav / .mp3 / .m4a\n","#audio_path = next(iter(uploaded.keys()))\n","audio_path=\"/content/call2.wav\"\n","print(\"Audio:\", audio_path)\n","\n","SAVE_DIR = \"/content/outputs\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Run inference\n","result = pipe(audio_path)\n","text   = result[\"text\"]\n","\n","# Save TXT + JSON\n","base = os.path.splitext(os.path.basename(audio_path))[0]\n","txt_path  = os.path.join(SAVE_DIR, f\"{base}.txt\")\n","json_path = os.path.join(SAVE_DIR, f\"{base}.json\")\n","with open(txt_path, \"w\", encoding=\"utf-8\") as f: f.write(text)\n","with open(json_path, \"w\", encoding=\"utf-8\") as f: json.dump(result, f, ensure_ascii=False, indent=2)\n","\n","print(\"\\nTranscript preview (~300 chars):\")\n","print(text[:300])\n","print(\"\\nSaved:\", txt_path)\n","print(\"Saved:\", json_path)\n","\n","# Show a quick peek at the first 2 timestamped chunks\n","chunks = result.get(\"chunks\") or []\n","print(f\"\\nChunks detected: {len(chunks)}\")\n","print(\"First 2 chunks:\", chunks[:2])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kp8Mdves924p","executionInfo":{"status":"ok","timestamp":1756894337909,"user_tz":-330,"elapsed":77943,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"b2bda4e1-8b90-4c8c-b77a-9e33b0983313"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Audio: /content/call2.wav\n"]},{"output_type":"stream","name":"stderr","text":["Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n","Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Transcript preview (~300 chars):\n"," வணக்கம் என் தேருஸ் வாழ்த்தி நாங்கள் லைசென்ஸ் செல்ந்து கால் பண்ணியிருக்கோம் இது என்ன நியமனிக்கு வருங்கள் ஆக்சிஸ் மக்லெஃப் இன்சுரன்சில பாலிசை ட்ரிக்கிங் என்ன விஷயம் பேசுவாங்களா பேசும் முன்னை நோக்கிக்கொள்வோம் டீவின் பட்டமுடையாது பாலிசிக் ஆன்சல் பண்ணா ரெனரேட் அவருக்கு கட்லிஸ்ட் ஒரு மூன்று வருஷமாக பரிவி\n","\n","Saved: /content/outputs/call2.txt\n","Saved: /content/outputs/call2.json\n","\n","Chunks detected: 1\n","First 2 chunks: [{'timestamp': (None, None), 'text': ' வணக்கம் என் தேருஸ் வாழ்த்தி நாங்கள் லைசென்ஸ் செல்ந்து கால் பண்ணியிருக்கோம் இது என்ன நியமனிக்கு வருங்கள் ஆக்சிஸ் மக்லெஃப் இன்சுரன்சில பாலிசை ட்ரிக்கிங் என்ன விஷயம் பேசுவாங்களா பேசும் முன்னை நோக்கிக்கொள்வோம் டீவின் பட்டமுடையாது பாலிசிக் ஆன்சல் பண்ணா ரெனரேட் அவருக்கு கட்லிஸ்ட் ஒரு மூன்று வருஷமாக பரிவிண்ட் பண்ணியிருக்கணும் சில இவ்வளவு பாத்திங்கப்படும் அதிர்ச்சி மூன்று வருஷ பரிவின் பண்ணி சரண்டர் பண்ணுங்கன்னா சரண்டர் வல்லின்றி அவர் மும்முன் புடிச்சிட்டு உன் கொடுப்பாங்க பஸ்ட்ரஸ்ட்டில் பேமின் பண்ணி சரண்டர் பண்ணும்போது இந்தியார்மன் டென்சி இருக்காது இல்லையா கொஞ்சம் டைம் எடுத்தால் கண்டிகை பம் போது சரண்டர் மூட்டும் சமமும் புடிச்சிட்டு கொடுப்பாங்க புத்தே டேமிட் பண்ணும் சரண்டர் பண்ணும் போது டேமிட்டர் பண்ணும் சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொல்லுங்க சொகப்பட்டு சொல்லுங்கள் ஆனால் பாலிசிஸ் ஸ்டேட்டஸ் லாப்ஸ்டருக்கு சுமார் சூடு ஹோல்லருக்கு லாப்ஸ்டோ என்னவென்றுமான பணி கேட்டு இருக்காது வெங்கேசன் சார்ந்தி பாலிசென்டருக்கு நடந்த கூட நாம் முனைக்கு இந்த வேலை பணிக்கு செல்ல இருக்காது லித்தன்மேஸ் அழுத்தருக்கு போற்றுக்காங்க டிலே பணியின் ஹையாகான ரெய் சான்பிஸ்டருக்கு கொஞ்சம் புனைந்தவருக்கு சிக்கிரப் போய் வேண்டு பாருங்க அதாவது அங்கு ஊக்குகள் பணி கேட்டிருக்கும் சரியா உங்க பாலிசின் நபர் 149 அறுப நூறு ஏழு ஒன்று நூறு இருபத்து ஒன்பது மக்ஸ்ட்ரேட் ஸ்மார்ட்வெல்ட் அட்வாண்டஜி புரோஸ் பெர்பலச்சின் ஸ்டேங்கம் கிட்டடம் பாலிசிக் பதினாறு ஆயிரம் இருபத்தி நான்கு நூறு தொன்னூறு கிராஸ்ட் ஆணையும் அறுபத்து ஆறு ஆயிரம் தேனாயிரம் முப்பத்து ஏழு ரூபாய் கட்ட வண்டிகள் வாம்பில்லப் வேலைவை பசுமைக்காக ஸ்பெஷன் ரிவர்வல் ஸ்கீம்முன் சொல்ட்டோன்னு கொடுப்பாய் சரஸ் ஆப்சியனர் உங்களுக்கு ஒரு வருஷத்துக்குண்டானடி பன்னிங்லேருக்க்கு அதில் ஆறு மாத்துக்குண்டானடி வேவுநா நெடியு கட்டணி என்ன போதும் இதனால் உங்களுக்கு மெச்யூரிட்டி வேட்டு ஆர்மாஸ் ஷிப்ட் ஆகும் பேயிண்டர் ஆர்மாஸ் ஷிப்ட் ஆகும் பாலிசி பெனிஃபிட் ஆர்மாஸ் ஷிப்ட் ஆகும் அதுமாதிரி இருப்பதால் ஆர்மாஸ் வேடிக்காட்டிக்கு மடங்கு பாலிசி மேட்டாக்டிஸ் சேர்ந்துுங்களா இல்லை மேலும் இல்லை ஆர்ச்செஸ் மற்றும் ரெபென்ஜன்ஸ் கிராம் கொடுத்ததுக்கு நன்றி'}]\n"]}]},{"cell_type":"code","source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n","import torch, gc\n","\n","assert 'MODEL_DIR' in globals(), \"Run Cell 2 first so MODEL_DIR is defined.\"\n","\n","device = 0 if torch.cuda.is_available() else -1\n","dtype  = torch.float16 if device == 0 else torch.float32\n","\n","# Load processor/model (you already did this earlier; safe to re-run)\n","processor = WhisperProcessor.from_pretrained(MODEL_DIR, local_files_only=True)\n","model = WhisperForConditionalGeneration.from_pretrained(\n","    MODEL_DIR,\n","    local_files_only=True,\n","    low_cpu_mem_usage=True,\n","    torch_dtype=dtype,\n",")\n","\n","# Move to GPU if available\n","if device == 0:\n","    model = model.to(\"cuda\")\n","\n","# ❌ IMPORTANT: stop using model-config forced ids (deprecated path)\n","model.generation_config.forced_decoder_ids = None\n","\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","# ✅ Build pipeline that ALWAYS translates to EN at call-time\n","pipe = pipeline(\n","    task=\"automatic-speech-recognition\",\n","    model=model,\n","    tokenizer=processor.tokenizer,\n","    feature_extractor=processor.feature_extractor,\n","    chunk_length_s=30,          # lower to 20/15 if OOM\n","    stride_length_s=(4, 2),\n","    return_timestamps=True,     # phrase-level timestamps\n","    device=device,\n","    batch_size=8,\n",")\n","\n","print(\"Pipeline ready (translation to EN will be set in generate_kwargs) ✅\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lzxJ_TNVEcsp","executionInfo":{"status":"ok","timestamp":1756895592510,"user_tz":-330,"elapsed":864438,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"e8f9f28b-6a29-4297-a6ff-5ad1c0a7fb90"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"]},{"output_type":"stream","name":"stdout","text":["Pipeline ready (translation to EN will be set in generate_kwargs) ✅\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zt8JqD5-MTEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yEjaVbbIMTBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0fkV9foZMS-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wYHleoUQMS7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HtzghGHDMS2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VAjHO5OYMSzR"},"execution_count":null,"outputs":[]}]}