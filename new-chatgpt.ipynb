{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNBCApdGzwVXF2LnqrEfuR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a48d9b0339044980ab44a84e7d00e5e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df0a4c47f03c40dc8f648e9fb6584933","IPY_MODEL_5a5a20c7b54743798d6c4617dc737f0c","IPY_MODEL_7714afba286d4838990cd41c8294b420"],"layout":"IPY_MODEL_e8a687f83b5344bfa10bfc0f4eb3ad3c"}},"df0a4c47f03c40dc8f648e9fb6584933":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_410f088932e245bc9479608be0771f5d","placeholder":"​","style":"IPY_MODEL_7c6909c0a6484c37931071a13a5a8138","value":"config.yaml: 100%"}},"5a5a20c7b54743798d6c4617dc737f0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9557613c0df141be824bccc5d9a3a1fb","max":469,"min":0,"orientation":"horizontal","style":"IPY_MODEL_debe118acd2c49c598133b407711ce11","value":469}},"7714afba286d4838990cd41c8294b420":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73272477371e4e938c011affdcafb17c","placeholder":"​","style":"IPY_MODEL_98bc375ff00c4c0fb1948b7294a938d0","value":" 469/469 [00:00&lt;00:00, 41.6kB/s]"}},"e8a687f83b5344bfa10bfc0f4eb3ad3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"410f088932e245bc9479608be0771f5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c6909c0a6484c37931071a13a5a8138":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9557613c0df141be824bccc5d9a3a1fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"debe118acd2c49c598133b407711ce11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73272477371e4e938c011affdcafb17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98bc375ff00c4c0fb1948b7294a938d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7af769568c844dea6b5c487c24fd606":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b7c65261c874d709c5d134bb48cdf31","IPY_MODEL_c9d16fa45f5645b2805b9c75869a4217","IPY_MODEL_80504c343fe8409586b95271ff19f42e"],"layout":"IPY_MODEL_065f2d6831b94600b7059e5dbd154358"}},"8b7c65261c874d709c5d134bb48cdf31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b552f3418b2487cb47b7a1b995d5557","placeholder":"​","style":"IPY_MODEL_0eceb1e1ac524585a3aa147a6ec1cdd1","value":"pytorch_model.bin: 100%"}},"c9d16fa45f5645b2805b9c75869a4217":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afd8605231ad4c78bb3e7fb0948deff8","max":5905440,"min":0,"orientation":"horizontal","style":"IPY_MODEL_483884bdbe354eafadb85c1c28b9c07f","value":5905440}},"80504c343fe8409586b95271ff19f42e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecc34a7c811f4f74b4652ec7b77700f9","placeholder":"​","style":"IPY_MODEL_e2ab3119cee0412fa0a8769820358fa8","value":" 5.91M/5.91M [00:00&lt;00:00, 9.97MB/s]"}},"065f2d6831b94600b7059e5dbd154358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b552f3418b2487cb47b7a1b995d5557":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eceb1e1ac524585a3aa147a6ec1cdd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afd8605231ad4c78bb3e7fb0948deff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"483884bdbe354eafadb85c1c28b9c07f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecc34a7c811f4f74b4652ec7b77700f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2ab3119cee0412fa0a8769820358fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"831c1548686d4da2bfbcb0076e036f36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72d59f9ecf174e4eb1a77cc9c9663655","IPY_MODEL_9f9acaa3e891474ba600dd639da848c4","IPY_MODEL_c1f2f23a44cf4a07ad84bca6f83ddc20"],"layout":"IPY_MODEL_86d79d16c635453bb5c92483b12e77e2"}},"72d59f9ecf174e4eb1a77cc9c9663655":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c7aa1ce26e644f28e48c53f22fdd7c3","placeholder":"​","style":"IPY_MODEL_3936b75980ce4c779ad708de435ad0f8","value":"config.yaml: 100%"}},"9f9acaa3e891474ba600dd639da848c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f2d0268392d469d9263a168ae615c0d","max":399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35174264800f4149be64812592bbdc09","value":399}},"c1f2f23a44cf4a07ad84bca6f83ddc20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e98cedfdda2546c29f9a66aeda58cc8b","placeholder":"​","style":"IPY_MODEL_752eab6faa1740fc96699995b925218e","value":" 399/399 [00:00&lt;00:00, 36.6kB/s]"}},"86d79d16c635453bb5c92483b12e77e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c7aa1ce26e644f28e48c53f22fdd7c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3936b75980ce4c779ad708de435ad0f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f2d0268392d469d9263a168ae615c0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35174264800f4149be64812592bbdc09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e98cedfdda2546c29f9a66aeda58cc8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"752eab6faa1740fc96699995b925218e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c9ad88d905946ac94d7cc71063b5111":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6b0d866a53a4d2eabe2a73cda6404c2","IPY_MODEL_544168548d40482ab01588ff530ca605","IPY_MODEL_578563bfa3094440a5128f774258a52e"],"layout":"IPY_MODEL_f93dd9879e2141a4a91fb09268bb052a"}},"e6b0d866a53a4d2eabe2a73cda6404c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41a57034b3cb4d1ba9e3a95666e601a2","placeholder":"​","style":"IPY_MODEL_0176d2710eed4b72be67ec3e5dbfb9e1","value":"pytorch_model.bin: 100%"}},"544168548d40482ab01588ff530ca605":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3877d3585fe443e1aa5b0f89453db612","max":26645418,"min":0,"orientation":"horizontal","style":"IPY_MODEL_190d8804c7eb4cb7a0d4cbd89586b337","value":26645418}},"578563bfa3094440a5128f774258a52e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_040e90a6e0d84c95a5413f61e218c974","placeholder":"​","style":"IPY_MODEL_c9548f078450485fb46e43821a7db691","value":" 26.6M/26.6M [00:00&lt;00:00, 44.4MB/s]"}},"f93dd9879e2141a4a91fb09268bb052a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41a57034b3cb4d1ba9e3a95666e601a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0176d2710eed4b72be67ec3e5dbfb9e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3877d3585fe443e1aa5b0f89453db612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190d8804c7eb4cb7a0d4cbd89586b337":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"040e90a6e0d84c95a5413f61e218c974":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9548f078450485fb46e43821a7db691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f046467850024ec88981726cc6be438c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2aabd79e8d4f45d187aed4172e23c2ac","IPY_MODEL_885800a1b1b846648006f1de6b268980","IPY_MODEL_94fddfda49fe4380bb87817f5b4fe543"],"layout":"IPY_MODEL_6f0abae8420c47509710cd149f946eb2"}},"2aabd79e8d4f45d187aed4172e23c2ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a373c54cb94faeb37a20f0d5ee0892","placeholder":"​","style":"IPY_MODEL_7149bdc91e7847b29dd745f8f46be114","value":"config.yaml: 100%"}},"885800a1b1b846648006f1de6b268980":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_258e02557a4d41289f92e29a4f2597a8","max":221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6f228ecdcba47e1b05a44db78aad18c","value":221}},"94fddfda49fe4380bb87817f5b4fe543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db88764414684e059d958c166132ebbf","placeholder":"​","style":"IPY_MODEL_4af3a4a926534355a0d4ca861c0b2597","value":" 221/221 [00:00&lt;00:00, 22.8kB/s]"}},"6f0abae8420c47509710cd149f946eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18a373c54cb94faeb37a20f0d5ee0892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7149bdc91e7847b29dd745f8f46be114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"258e02557a4d41289f92e29a4f2597a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6f228ecdcba47e1b05a44db78aad18c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db88764414684e059d958c166132ebbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af3a4a926534355a0d4ca861c0b2597":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98782,"status":"ok","timestamp":1754628660158,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"ldTlH2oL2jFc","outputId":"5b645c97-b0aa-49c6-f274-9d17430fcdc7","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-r0yz5zuz\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-r0yz5zuz\n","  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (10.7.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.0.2)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (4.67.1)\n","Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20250625) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (0.6.2)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20250625)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20250625) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20250625) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=d960787bd0952eb706dcabb4bee8492a98875e6dcf3379f670f2fe70361d4628\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-214baqa_/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625\n"]}],"source":["!pip install --break-system-packages git+https://github.com/openai/whisper.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17600,"status":"ok","timestamp":1754628677779,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"p5QqGdod2qeW","outputId":"c3c2c0a2-fefe-48f2-b228-c64cf28bb935","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyannote.audio\n","  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.34.3)\n","Collecting lightning>=2.0.1 (from pyannote.audio)\n","  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n","Collecting pyannote.core>=5.0.0 (from pyannote.audio)\n","  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting pyannote.database>=5.0.1 (from pyannote.audio)\n","  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pyannote.metrics>=3.2 (from pyannote.audio)\n","  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n","Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n","  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n","Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n","  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n","Collecting semver>=3.0.0 (from pyannote.audio)\n","  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n","Collecting speechbrain>=1.0.0 (from pyannote.audio)\n","  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tensorboardX>=2.6 (from pyannote.audio)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n","Collecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Collecting torchmetrics>=0.11.0 (from pyannote.audio)\n","  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->pyannote.audio) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote.audio)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Collecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n","Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n","Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.1)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n","Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n","Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n","Collecting optuna>=3.1 (from pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n","Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio)\n","  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.12.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n","Collecting alembic>=1.5.0 (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.42)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n","Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.8.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n","Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.5.2-py3-none-any.whl (821 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n","Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n","Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n","Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt, julius\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2dc422c0e21a491074f33e6fab469160bfb4297caef9e62a337849dcf67aa21e\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=252ca0e93e229af76f0be2f6c823a4882c76193a1e7696f1eea3f5422124d1a0\n","  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\n","Successfully built docopt julius\n","Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, colorlog, ruamel.yaml, pyannote.core, alembic, optuna, hyperpyyaml, torchmetrics, pytorch-metric-learning, pyannote.database, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch-audiomentations, lightning, pyannote.audio\n","Successfully installed alembic-1.16.4 asteroid-filterbanks-0.4.0 colorlog-6.9.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.2 lightning-utilities-0.15.2 optuna-4.4.0 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.2 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 torchmetrics-1.8.1\n"]}],"source":["!pip install --break-system-packages pyannote.audio torchaudio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ECsmgLC2rZI"},"outputs":[],"source":["import whisper\n","from pyannote.audio import Pipeline\n","import torch\n","import re\n","import os\n","import subprocessa\n","from glob import glob\n","from pathlib import Path\n","import json\n","import difflib\n","from difflib import SequenceMatcher"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhtT5qwn2u4c"},"outputs":[],"source":["# Configuration\n","HUGGING_FACE_ACCESS_TOKEN = \"hf_\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55543,"status":"ok","timestamp":1754628756132,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"},"user_tz":-330},"id":"mldJRQe92v7m","outputId":"8353bdad-4dfc-4d6a-f1ef-9afcdcdcc4ab","collapsed":true},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████████████████████████████████| 2.88G/2.88G [00:24<00:00, 125MiB/s]\n"]}],"source":["#model downloading\n","model = whisper.load_model(\"large-v3\")"]},{"cell_type":"code","source":["model"],"metadata":{"collapsed":true,"id":"5UbLg5SaBzQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754628756134,"user_tz":-330,"elapsed":106,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"7b8fc422-ff41-4121-9c1f-5f4506a4ebd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Whisper(\n","  (encoder): AudioEncoder(\n","    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n","    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (decoder): TextDecoder(\n","    (token_embedding): Embedding(51866, 1280)\n","    (blocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (cross_attn): MultiHeadAttention(\n","          (query): Linear(in_features=1280, out_features=1280, bias=True)\n","          (key): Linear(in_features=1280, out_features=1280, bias=False)\n","          (value): Linear(in_features=1280, out_features=1280, bias=True)\n","          (out): Linear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=1280, out_features=5120, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djnUi6al2zFp"},"outputs":[],"source":["# Known agents list (added from recent updates)\n","KNOWN_AGENTS = [\n","    \"Jaya Parkash\", \"Chandru\", \"Sneha\", \"Kowsalya\", \"Swathi\",\n","    \"Arogya Marry\", \"Delphina\", \"Aesu Marry\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2J1Jvqj297n"},"outputs":[],"source":["def get_audio_duration(audio_path):\n","    \"\"\"Get audio duration using ffprobe\"\"\"\n","    try:\n","        cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n","               \"-of\", \"default=noprint_wrappers=1:nokey=1\", audio_path]\n","        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n","        return float(result.stdout.strip())\n","    except Exception as e:\n","        print(f\"Could not get duration: {e}\")\n","        return 0"]},{"cell_type":"code","source":["def audio_preprocessing_v1(input_path, output_path):\n","    \"\"\"Advanced audio preprocessing with better parameters\"\"\"\n","    print(\"--- Trying Advanced Audio Preprocessing ---\")\n","\n","    # Improved ffmpeg command - less aggressive filtering to preserve speech\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",  # Mono\n","        \"-ar\", \"16000\",  # 16kHz sample rate\n","        \"-af\", \"loudnorm=I=-23:TP=-2:LRA=7,highpass=f=80,lowpass=f=8000,afftdn=nr=10\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Advanced preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Advanced preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v2(input_path, output_path):\n","    \"\"\"Simplified but effective preprocessing\"\"\"\n","    print(\"--- Trying Simplified Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm=I=-23:TP=-2,highpass=f=100\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Simplified preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Simplified preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v3(input_path, output_path):\n","    \"\"\"Basic but reliable preprocessing\"\"\"\n","    print(\"--- Trying Basic Audio Preprocessing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-af\", \"loudnorm\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Basic preprocessing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Basic preprocessing failed: {e.returncode}\")\n","        return False\n","\n","def audio_preprocessing_v4(input_path, output_path):\n","    \"\"\"Minimal processing - just format conversion\"\"\"\n","    print(\"--- Trying Minimal Audio Processing ---\")\n","\n","    ffmpeg_command = [\n","        \"ffmpeg\", \"-i\", input_path,\n","        \"-acodec\", \"pcm_s16le\",\n","        \"-ac\", \"1\",\n","        \"-ar\", \"16000\",\n","        \"-y\", output_path\n","    ]\n","\n","    try:\n","        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n","        print(\"Minimal processing successful\")\n","        return True\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Minimal processing failed: {e.returncode}\")\n","        return False\n","\n","def smart_audio_preprocessing(input_path, output_path):\n","    \"\"\"Try different preprocessing methods in order of preference\"\"\"\n","    original_duration = get_audio_duration(input_path)\n","    print(f\"Original audio duration: {original_duration:.2f} seconds\")\n","\n","    methods = [\n","        audio_preprocessing_v1,\n","        audio_preprocessing_v2,\n","        audio_preprocessing_v3,\n","        audio_preprocessing_v4\n","    ]\n","\n","    for i, method in enumerate(methods, 1):\n","        if method(input_path, output_path):\n","            if os.path.exists(output_path):\n","                processed_duration = get_audio_duration(output_path)\n","                print(f\"Processed audio duration: {processed_duration:.2f} seconds\")\n","\n","                if abs(original_duration - processed_duration) < 1.0:\n","                    print(f\"✅ Audio preprocessing successful with method {i}\")\n","                    return True\n","                else:\n","                    print(f\"⚠️  Duration mismatch with method {i}, trying next...\")\n","                    continue\n","\n","    print(\"❌ All preprocessing methods failed!\")\n","    return False"],"metadata":{"id":"wgLrlFKBtBS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XczmnOn3CxG"},"outputs":[],"source":["def post_process_text(text):\n","    \"\"\"Clean up transcribed text from Whisper output for call center insurance context.\"\"\"\n","    if not text:\n","        return \"\"\n","\n","    # === 1. Remove excessive immediate repetitions ===\n","    words = text.split()\n","    cleaned_words = []\n","    i = 0\n","    while i < len(words):\n","        current_word = words[i].lower()\n","        repetition_count = 1\n","        j = i + 1\n","        while j < len(words) and words[j].lower() == current_word:\n","            repetition_count += 1\n","            j += 1\n","\n","        keep_count = min(repetition_count, 2) if repetition_count <= 3 else 1\n","        for _ in range(keep_count):\n","            cleaned_words.append(words[i])\n","        i += repetition_count\n","\n","    text = ' '.join(cleaned_words)\n","\n","    # === 2. Remove filler sounds (non-verbal, repetitive) ===\n","    filler_sounds = [\"uh\", \"um\", \"mm\", \"hmm\", \"ah\", \"oh\", \"huh\", \"ha ha\"]\n","    soft_fillers = [\"okay okay\", \"yes yes\", \"yes yes yes\", \"i mean\", \"you know\", \"like like\", \"ok ok\"]\n","\n","    for filler in filler_sounds + soft_fillers:\n","        text = re.sub(rf'\\b{re.escape(filler)}\\b', '', text, flags=re.IGNORECASE)\n","\n","    # === 3. Insurance domain term normalization ===\n","    corrections = {\n","        'access max life': 'Axis Max Life',\n","        'axis max life': 'Axis Max Life',\n","        'g pay': 'GPay',\n","        'google pay': 'Google Pay',\n","        'phone pay': 'PhonePe',\n","        'phone pe': 'PhonePe',\n","        'pay tm': 'Paytm',\n","        'net banking': 'netbanking',\n","        'some assured': 'sum assured',\n","        'premium do': 'premium due',\n","        'do date': 'due date',\n","        'okay sir': 'Okay sir',\n","    }\n","\n","    text_lower = text.lower()\n","    for wrong, correct in corrections.items():\n","        text_lower = text_lower.replace(wrong, correct)\n","\n","    # === 3.5 Replace 'Rs.', 'Rs' → '₹' with optional space cleanup ===\n","    text_lower = re.sub(r'\\brs[.]?\\s*', '₹', text_lower)\n","\n","    # === 4. Punctuation cleanup ===\n","    text_lower = re.sub(r'\\s{2,}', ' ', text_lower)          # Extra spaces\n","    text_lower = re.sub(r'[,]{2,}', ',', text_lower)         # Repeated commas\n","    text_lower = re.sub(r'\\s+,', ',', text_lower)            # Space before comma\n","    text_lower = re.sub(r'\\s+\\.', '.', text_lower)           # Space before period\n","    text_lower = re.sub(r'\\s+[!?]', lambda m: m.group(0).strip(), text_lower)\n","\n","    # === 5. Capitalize sentences ===\n","    text_lower = re.sub(r'(^|[.!?]\\s+)([a-z])',\n","                        lambda m: m.group(1) + m.group(2).upper(),\n","                        text_lower)\n","\n","    return text_lower.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5TWJf3J3Ict"},"outputs":[],"source":["def calculate_repetition_score(segments):\n","    \"\"\"\n","    Calculate a repetition score for transcription segments\n","    Lower score = less repetition = better\n","    \"\"\"\n","    if not segments:\n","        return 0.0\n","\n","    total_repetition = 0\n","    total_words = 0\n","\n","    for segment in segments:\n","        text = segment.get('text', '').strip().lower()\n","        words = text.split()\n","\n","        if len(words) < 2:\n","            continue\n","\n","        total_words += len(words)\n","\n","        # Count immediate word repetitions\n","        for i in range(len(words) - 1):\n","            if words[i] == words[i + 1]:\n","                total_repetition += 1\n","\n","        # Count phrase repetitions within segment\n","        for phrase_len in range(2, min(len(words)//2 + 1, 6)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len])\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2])\n","                if phrase1 == phrase2:\n","                    total_repetition += phrase_len * 2  # Heavy penalty\n","\n","    return total_repetition / max(total_words, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGGfOjUF3KlB"},"outputs":[],"source":["def detect_and_remove_repetitions(segments, max_repetition_ratio=0.3):\n","    \"\"\"\n","    AGGRESSIVE post-processing function to detect and remove repetitive segments\n","    \"\"\"\n","    print(\"🔍 Starting aggressive repetition detection...\")\n","    cleaned_segments = []\n","\n","    for i, segment in enumerate(segments):\n","        text = segment['text'].strip()\n","        words = text.split()\n","\n","        # Skip very short segments\n","        if len(words) < 2:\n","            continue\n","\n","        # AGGRESSIVE: Check for excessive word repetition\n","        is_repetitive = False\n","\n","        # Count word frequencies\n","        word_counts = {}\n","        for word in words:\n","            word_lower = word.lower().strip('.,!?')\n","            word_counts[word_lower] = word_counts.get(word_lower, 0) + 1\n","\n","        # Check if any single word dominates the segment\n","        max_word_count = max(word_counts.values()) if word_counts else 0\n","        word_dominance = max_word_count / len(words) if words else 0\n","\n","        if word_dominance > 0.4:  # If any word is >40% of the segment\n","            print(f\"🚫 Rejecting word-dominated segment: {text[:50]}... (dominance: {word_dominance:.2f})\")\n","            continue\n","\n","        # Check for immediate repetitions (same word repeated consecutively)\n","        consecutive_repeats = 0\n","        max_consecutive = 0\n","\n","        for j in range(1, len(words)):\n","            if words[j].lower().strip('.,!?') == words[j-1].lower().strip('.,!?'):\n","                consecutive_repeats += 1\n","                max_consecutive = max(max_consecutive, consecutive_repeats + 1)\n","            else:\n","                consecutive_repeats = 0\n","\n","        if max_consecutive > 3:  # More than 3 consecutive identical words\n","            print(f\"🚫 Rejecting consecutive repeat segment: {text[:50]}... (max consecutive: {max_consecutive})\")\n","            continue\n","\n","        # Check for pattern repetitions within segment\n","        for phrase_len in range(2, min(len(words)//3 + 1, 8)):\n","            for start in range(len(words) - phrase_len * 2 + 1):\n","                phrase1 = ' '.join(words[start:start + phrase_len]).lower()\n","                phrase2 = ' '.join(words[start + phrase_len:start + phrase_len * 2]).lower()\n","\n","                if phrase1 == phrase2:\n","                    repetition_coverage = (phrase_len * 2) / len(words)\n","                    if repetition_coverage > max_repetition_ratio:\n","                        print(f\"🚫 Rejecting pattern repeat segment: {text[:50]}... (coverage: {repetition_coverage:.2f})\")\n","                        is_repetitive = True\n","                        break\n","            if is_repetitive:\n","                break\n","\n","        if is_repetitive:\n","            continue\n","\n","        # Check for similarity with recent segments (avoid near-duplicates)\n","        is_near_duplicate = False\n","        for prev_segment in cleaned_segments[-5:]:  # Check last 5 segments\n","            prev_words = prev_segment['text'].lower().split()\n","            current_words = [w.lower() for w in words]\n","\n","            if prev_words and current_words:\n","                # Calculate Jaccard similarity\n","                prev_set = set(prev_words)\n","                current_set = set(current_words)\n","                intersection = len(prev_set.intersection(current_set))\n","                union = len(prev_set.union(current_set))\n","\n","                similarity = intersection / union if union > 0 else 0\n","\n","                if similarity > 0.7 and abs(len(prev_words) - len(current_words)) < 5:\n","                    print(f\"🚫 Rejecting near-duplicate: {text[:30]}... (similarity: {similarity:.2f})\")\n","                    is_near_duplicate = True\n","                    break\n","\n","        if is_near_duplicate:\n","            continue\n","\n","        # If we reach here, the segment passed all checks\n","        cleaned_segments.append(segment)\n","\n","    removed_count = len(segments) - len(cleaned_segments)\n","    print(f\"📊 Aggressive cleaning: {len(segments)} → {len(cleaned_segments)} segments\")\n","    print(f\"🗑️  Removed {removed_count} repetitive/problematic segments\")\n","\n","    return cleaned_segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFkd2n9J3FzX"},"outputs":[],"source":["def enhanced_whisper_transcription(audio_path):\n","    \"\"\"\n","    Enhanced Whisper transcription with optimal anti-repetition parameters\n","    \"\"\"\n","    print(\"--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\")\n","    agents_str = \", \".join(KNOWN_AGENTS)\n","    prompt = (\n","        f\"This is a customer support call from Axis Maxlife Insurance in Tamil regarding insurance renewal call. \"\n","        f\"Agents are always one of these: {agents_str}. Use exact names like 'Jaya Parkash' or 'Swathi' \"\n","        f\"when they introduce themselves. Discuss policy numbers, due dates, fund values, sum assured, late fees, \"\n","        f\"and payment methods such as Google Pay, PhonePe, Paytm, netbanking.\"\n","    )\n","\n","    # Single optimal strategy - no need for multiple attempts\n","    result = model.transcribe(\n","        audio_path,\n","        language=\"ta\",\n","        task=\"translate\",\n","        temperature=0.0,\n","        beam_size=5,\n","        patience=1.2,\n","        condition_on_previous_text=False,\n","        no_speech_threshold=0.8,\n","        compression_ratio_threshold=2.0,\n","        logprob_threshold=-0.35,\n","        word_timestamps=False,\n","        initial_prompt=prompt,  # Updated with agent bias\n","        verbose=True,\n","    )\n","\n","    print(\"✅ Whisper transcription completed with optimal parameters\")\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9gcy8vB3R_r"},"outputs":[],"source":["def process_single_file(audio_file_path):\n","    print(f\"\\n📁 Processing file: {audio_file_path.name}\")\n","    clean_audio_path = OUTPUT_DIR / f\"{audio_file_path.stem}_clean.wav\"\n","    json_output_path = OUTPUT_DIR / f\"{audio_file_path.stem}_transcription.json\"\n","\n","    if not smart_audio_preprocessing(str(audio_file_path), str(clean_audio_path)):\n","        print(\"❌ Preprocessing failed, skipping file.\")\n","        return\n","\n","    try:\n","        whisper_result = enhanced_whisper_transcription(str(clean_audio_path))\n","    except Exception as e:\n","        print(f\"❌ Whisper transcription failed: {e}\")\n","        return\n","\n","    cleaned_segments = detect_and_remove_repetitions(whisper_result[\"segments\"])\n","\n","    processed_segments = []\n","    all_texts = [seg['text'] for seg in cleaned_segments]  # For global checks\n","    for segment in cleaned_segments:\n","        cleaned = post_process_text(segment['text'])\n","        if cleaned.strip() and len(cleaned.strip()) > 5:\n","            new_segment = segment.copy()\n","            new_segment['text'] = cleaned\n","            processed_segments.append(new_segment)\n","\n","    whisper_result[\"segments\"] = processed_segments\n","\n","    print(\"🔊 Performing Speaker Diarization...\")\n","    try:\n","        pipeline = Pipeline.from_pretrained(\n","            \"pyannote/speaker-diarization-3.1\",\n","            use_auth_token=HUGGING_FACE_ACCESS_TOKEN\n","        )\n","        if torch.cuda.is_available():\n","            pipeline.to(torch.device(\"cuda\"))\n","            print(\"✅ Using GPU\")\n","        diarization = pipeline(str(clean_audio_path))\n","    except Exception as e:\n","        print(f\"⚠️ Diarization failed: {e}\")\n","        diarization = None\n","\n","    def get_dominant_speaker(start, end, diarization_result, segment_text=\"\"):\n","        if not diarization_result:\n","            return \"Speaker_Unknown\"\n","        speakers = {}\n","        for seg, _, spk in diarization_result.itertracks(yield_label=True):\n","            overlap = max(0, min(end, seg.end) - max(start, seg.start))\n","            if overlap > 0:\n","                speakers[spk] = speakers.get(spk, 0) + overlap\n","        generic_speaker = max(speakers, key=speakers.get) if speakers else \"Speaker_Unknown\"\n","\n","        # Map to known agent if text matches\n","        words = segment_text.split()\n","        for i, word in enumerate(words):\n","            for agent in KNOWN_AGENTS:\n","                if difflib.SequenceMatcher(None, word.lower(), agent.lower()).ratio() > 0.8:\n","                    return agent  # e.g., \"Jaya Parkash\"\n","        return \"Customer\" if \"Customer\" in generic_speaker else generic_speaker  # Fallback\n","\n","    dialogue = []\n","    current_speaker, current_texts, current_start, current_end = None, [], 0, 0\n","    for seg in processed_segments:\n","        start, end, text = seg['start'], seg['end'], seg['text'].strip()\n","        speaker = get_dominant_speaker(start, end, diarization, text)\n","        if (speaker == current_speaker and current_speaker and (start - current_end) < 3.0):\n","            current_texts.append(text)\n","            current_end = end\n","        else:\n","            if current_speaker and current_texts:\n","                combined = ' '.join(current_texts)\n","                if len(combined.strip()) > 10:\n","                    dialogue.append({\n","                        'speaker': current_speaker,\n","                        'text': combined,\n","                        'start_time': current_start,\n","                        'end_time': current_end\n","                    })\n","            current_speaker, current_texts, current_start, current_end = speaker, [text], start, end\n","\n","    if current_speaker and current_texts:\n","        combined = ' '.join(current_texts)\n","        if len(combined.strip()) > 10:\n","            dialogue.append({\n","                'speaker': current_speaker,\n","                'text': combined,\n","                'start_time': current_start,\n","                'end_time': current_end\n","            })\n","\n","    # Save JSON per file\n","    output_data = {\n","        'metadata': {\n","            'audio_file': str(audio_file_path.name),\n","            'total_duration': whisper_result.get('duration', 0),\n","            'total_speakers': len(set(d['speaker'] for d in dialogue)),\n","            'total_segments': len(dialogue),\n","            'model_used': 'whisper-large',\n","            'processing_successful': True,\n","            'anti_repetition_applied': True\n","        },\n","        'dialogue': dialogue,\n","        'raw_transcription': whisper_result\n","    }\n","\n","    with open(json_output_path, 'w', encoding='utf-8') as f:\n","        json.dump(output_data, f, indent=2, ensure_ascii=False)\n","    print(f\"✅ Output saved: {json_output_path.name}\")\n","\n","    # Generate final conversation string\n","    full_text = \"\\n\".join([f\"{d['speaker']}: {d['text']}\" for d in dialogue])\n","\n","    # Add to manifest entry\n","    manifest_entries.append({\n","        \"audio_filepath\": str(clean_audio_path),\n","        \"text\": full_text,\n","        \"language\": \"ta\",\n","        \"task\": \"translate\"\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGS8D2wM3Nzz"},"outputs":[],"source":["# Directory paths\n","INPUT_DIR = Path(\"training_data\")\n","OUTPUT_DIR = Path(\"processed_output\")\n","OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n","# Manifest for Whisper fine-tuning\n","manifest_path = OUTPUT_DIR / \"training_manifest.jsonl\"\n","manifest_entries = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a48d9b0339044980ab44a84e7d00e5e5","df0a4c47f03c40dc8f648e9fb6584933","5a5a20c7b54743798d6c4617dc737f0c","7714afba286d4838990cd41c8294b420","e8a687f83b5344bfa10bfc0f4eb3ad3c","410f088932e245bc9479608be0771f5d","7c6909c0a6484c37931071a13a5a8138","9557613c0df141be824bccc5d9a3a1fb","debe118acd2c49c598133b407711ce11","73272477371e4e938c011affdcafb17c","98bc375ff00c4c0fb1948b7294a938d0","b7af769568c844dea6b5c487c24fd606","8b7c65261c874d709c5d134bb48cdf31","c9d16fa45f5645b2805b9c75869a4217","80504c343fe8409586b95271ff19f42e","065f2d6831b94600b7059e5dbd154358","6b552f3418b2487cb47b7a1b995d5557","0eceb1e1ac524585a3aa147a6ec1cdd1","afd8605231ad4c78bb3e7fb0948deff8","483884bdbe354eafadb85c1c28b9c07f","ecc34a7c811f4f74b4652ec7b77700f9","e2ab3119cee0412fa0a8769820358fa8","831c1548686d4da2bfbcb0076e036f36","72d59f9ecf174e4eb1a77cc9c9663655","9f9acaa3e891474ba600dd639da848c4","c1f2f23a44cf4a07ad84bca6f83ddc20","86d79d16c635453bb5c92483b12e77e2","7c7aa1ce26e644f28e48c53f22fdd7c3","3936b75980ce4c779ad708de435ad0f8","4f2d0268392d469d9263a168ae615c0d","35174264800f4149be64812592bbdc09","e98cedfdda2546c29f9a66aeda58cc8b","752eab6faa1740fc96699995b925218e","5c9ad88d905946ac94d7cc71063b5111","e6b0d866a53a4d2eabe2a73cda6404c2","544168548d40482ab01588ff530ca605","578563bfa3094440a5128f774258a52e","f93dd9879e2141a4a91fb09268bb052a","41a57034b3cb4d1ba9e3a95666e601a2","0176d2710eed4b72be67ec3e5dbfb9e1","3877d3585fe443e1aa5b0f89453db612","190d8804c7eb4cb7a0d4cbd89586b337","040e90a6e0d84c95a5413f61e218c974","c9548f078450485fb46e43821a7db691","f046467850024ec88981726cc6be438c","2aabd79e8d4f45d187aed4172e23c2ac","885800a1b1b846648006f1de6b268980","94fddfda49fe4380bb87817f5b4fe543","6f0abae8420c47509710cd149f946eb2","18a373c54cb94faeb37a20f0d5ee0892","7149bdc91e7847b29dd745f8f46be114","258e02557a4d41289f92e29a4f2597a8","e6f228ecdcba47e1b05a44db78aad18c","db88764414684e059d958c166132ebbf","4af3a4a926534355a0d4ca861c0b2597"]},"id":"yRWE9j_F3UZd","executionInfo":{"status":"ok","timestamp":1754629562042,"user_tz":-330,"elapsed":210119,"user":{"displayName":"VBS Analytics","userId":"12942825704271409009"}},"outputId":"2293dd3a-55a0-4fc2-901a-be6291971f01"},"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Found 2 files to process...\n","\n","📁 Processing file: call2.wav\n","Original audio duration: 190.76 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 190.74 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n","[00:00.000 --> 00:10.000]  Hello. Good morning. My name is Swathi. We are calling from Licensure. This is an email call. You have taken a policy from Axis Maxlife Insurance. Can you speak for 2 minutes?\n","[00:10.000 --> 00:18.000]  Yes, Madam. Quick call. I am unable to receive the call. If the policy is cancelled, will it be refunded?\n","[00:18.000 --> 00:23.000]  Okay. I will inform you about the details of your policy in a short time.\n","[00:23.000 --> 00:27.000]  Okay.\n","[00:27.000 --> 00:29.000]  Okay. Can you speak in Himalayan?\n","[00:29.000 --> 00:31.000]  Yes.\n","[00:31.000 --> 00:33.000]  Okay. Can you tell me the reason why you did not commit the crime?\n","[00:33.000 --> 00:35.000]  I did not have a job.\n","[00:35.000 --> 00:37.000]  I did not have a job.\n","[00:37.000 --> 00:39.000]  Okay.\n","[00:39.000 --> 00:41.000]  Okay. I understand the situation.\n","[00:41.000 --> 00:43.000]  You have already made a payment for a year.\n","[00:43.000 --> 00:45.000]  When you surrender after making the first payment,\n","[00:45.000 --> 00:47.000]  do you have any attempts?\n","[00:47.000 --> 00:49.000]  Because for the surrender value to be generated,\n","[00:49.000 --> 00:51.000]  you should have made the payment for at least 3 years.\n","[00:51.000 --> 00:53.000]  Usually, if you surrender after making the payment for at least 3 years,\n","[00:53.000 --> 00:55.000]  usually, if you surrender for at least 3 years,\n","[00:55.000 --> 00:57.000]  They will give you a surrender value of some amount.\n","[00:57.000 --> 01:01.000]  But when you surrender after first year payment, there won't be any attempts.\n","[01:01.000 --> 01:04.000]  You can at least take some time to continue.\n","[01:04.000 --> 01:08.000]  How much will it cost if I do this now?\n","[01:08.000 --> 01:14.000]  That's it. When you surrender after first year payment, there won't be any attempts.\n","[01:14.000 --> 01:17.000]  You are saying that the amount will go like that.\n","[01:17.000 --> 01:24.000]  Yes. Because if you pay for at least 3 years and surrender, then only the cash surrender value will be generated.\n","[01:24.000 --> 01:28.000]  So, when you surrender, they will give you the surrender amount accordingly.\n","[01:28.000 --> 01:32.000]  When you surrender with the first payment, will there be any other benefits?\n","[01:32.000 --> 01:37.000]  Can you please continue with the timer for a while?\n","[01:37.000 --> 01:39.000]  Yes, I will continue.\n","[01:39.000 --> 01:42.000]  Okay, please tell me what you are thinking.\n","[01:42.000 --> 01:45.000]  There are lapses in the policy status and there are holes in the summaries.\n","[01:45.000 --> 01:47.000]  There will be no benefit in the lapses.\n","[01:47.000 --> 01:51.000]  In case, there is a sudden policy change, there will be no benefit for us.\n","[01:51.000 --> 01:59.000]  If the delay is too high according to the lay payment charges, you will have to pay the payment soon.\n","[01:59.000 --> 02:04.000]  Your policy number is 149607129.\n","[02:04.000 --> 02:08.000]  Next is Smart Wealth Advantage Growth Per Price Insta Income Fixed Return Policy.\n","[02:08.000 --> 02:13.000]  You have to pay Rs.66,537 to cross Avanadu 102024.\n","[02:43.000 --> 02:45.000]  Okay, I will call you back.\n","[02:45.000 --> 02:50.000]  Okay ma'am. In case you need any special services, you can contact the branch.\n","[02:50.000 --> 02:55.000]  I will submit the health declaration form. I will arrange for a call back next week.\n","[02:55.000 --> 02:58.000]  Just inform me what you need. Okay?\n","[02:58.000 --> 02:59.000]  Okay ma'am.\n","[02:59.000 --> 03:01.000]  Any other questions?\n","[03:01.000 --> 03:02.000]  No, ma'am.\n","[03:02.000 --> 03:05.000]  Do you want to update anything via alternate mobile?\n","[03:05.000 --> 03:06.000]  No, ma'am.\n","[03:06.000 --> 03:10.000]  Okay ma'am. I will try to get access to you. Thank you.\n","✅ Whisper transcription completed with optimal parameters\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting near-duplicate: I did not have a job.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: That's it. When you surrender ... (similarity: 0.80)\n","🚫 Rejecting word-dominated segment: Okay ma'am.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: No, ma'am.... (dominance: 0.50)\n","🚫 Rejecting word-dominated segment: No, ma'am.... (dominance: 0.50)\n","📊 Aggressive cleaning: 47 → 39 segments\n","🗑️  Removed 8 repetitive/problematic segments\n","🔊 Performing Speaker Diarization...\n"]},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48d9b0339044980ab44a84e7d00e5e5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7af769568c844dea6b5c487c24fd606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"831c1548686d4da2bfbcb0076e036f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9ad88d905946ac94d7cc71063b5111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f046467850024ec88981726cc6be438c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n","It can be re-enabled by calling\n","   >>> import torch\n","   >>> torch.backends.cuda.matmul.allow_tf32 = True\n","   >>> torch.backends.cudnn.allow_tf32 = True\n","See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n","\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Output saved: call2_transcription.json\n","\n","📁 Processing file: call1.wav\n","Original audio duration: 106.92 seconds\n","--- Trying Advanced Audio Preprocessing ---\n","Advanced preprocessing successful\n","Processed audio duration: 106.89 seconds\n","✅ Audio preprocessing successful with method 1\n","--- Enhanced Whisper Transcription (Optimal Single Strategy) ---\n","[00:00.000 --> 00:07.000]  Hello, I am Jaya Parkash from Axis Maxlife Insurance. I am here to speak to Mr. Govind Harad.\n","[00:07.000 --> 00:09.000]  I am here to speak to Mr. Govind Harad.\n","[00:09.000 --> 00:11.000]  I am here to speak to Mr. Govind Harad.\n","[00:11.000 --> 00:13.000]  I am here to speak to Mr. Govind Harad.\n","[00:13.000 --> 00:15.000]  I am here to speak to Mr. Govind Harad.\n","[00:15.000 --> 00:17.000]  I am here to speak to Mr. Govind Harad.\n","[00:17.000 --> 00:19.000]  I am here to speak to Mr. Govind Harad.\n","[00:19.000 --> 00:21.000]  I am here to speak to Mr. Govind Harad.\n","[00:21.000 --> 00:23.000]  I am here to speak to Mr. Govind Harad.\n","[00:23.000 --> 00:25.000]  I am here to speak to Mr. Govind Harad.\n","[00:25.000 --> 00:27.000]  I am here to speak to Mr. Govind Harad.\n","[00:27.000 --> 00:29.000]  I am here to speak to Mr. Govind Harad.\n","[00:29.000 --> 00:31.000]  Pending for 2 years\n","[00:31.000 --> 00:35.000]  Pending amount is Rs.1,14,713\n","[00:35.000 --> 00:37.000]  Pending amount is Rs.47,000\n","[00:37.000 --> 00:41.000]  Pending amount is Rs.1,14,713\n","[00:41.000 --> 00:43.000]  Pending amount is Rs.47,000\n","[00:43.000 --> 00:47.000]  Pending policy is paid up\n","[00:47.000 --> 00:51.000]  Rs.1,14,713\n","[00:51.000 --> 00:53.000]  Pending amount is Rs.47,000\n","[00:53.000 --> 00:57.000]  Pending policy is paid up\n","[00:57.000 --> 01:27.000]  There are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there are 8 pitchers and there\n","[01:27.000 --> 01:29.000]  If you have any questions, please feel free to ask.\n","[01:29.000 --> 01:31.000]  Thank you very much.\n","[01:31.000 --> 01:33.000]  If you have any questions, please feel free to ask.\n","[01:33.000 --> 01:35.000]  Thank you very much.\n","[01:35.000 --> 01:37.000]  If you have any questions, please feel free to ask.\n","[01:37.000 --> 01:39.000]  Thank you very much.\n","[01:39.000 --> 01:41.000]  If you have any questions, please feel free to ask.\n","[01:41.000 --> 01:43.000]  Thank you very much.\n","[01:43.000 --> 01:45.000]  Thank you very much.\n","[01:45.000 --> 01:47.000]  Thank you.\n","✅ Whisper transcription completed with optimal parameters\n","🔍 Starting aggressive repetition detection...\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: I am here to speak to Mr. Govi... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Pending amount is Rs.1,14,713... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Pending amount is Rs.47,000... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Pending amount is Rs.47,000... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Pending policy is paid up... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: If you have any questions, ple... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you very much.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: If you have any questions, ple... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you very much.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: If you have any questions, ple... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you very much.... (similarity: 1.00)\n","🚫 Rejecting near-duplicate: Thank you very much.... (similarity: 1.00)\n","🚫 Rejecting word-dominated segment: Thank you.... (dominance: 0.50)\n","📊 Aggressive cleaning: 32 → 9 segments\n","🗑️  Removed 23 repetitive/problematic segments\n","🔊 Performing Speaker Diarization...\n","✅ Using GPU\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n","  std = sequences.std(dim=-1, correction=1)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Output saved: call1_transcription.json\n","\n","📄 Training manifest saved to: processed_output/training_manifest.jsonl\n","✅ All files processed.\n"]}],"source":["def main():\n","    audio_files = list(INPUT_DIR.glob(\"*.wav\"))\n","    if not audio_files:\n","        print(\"❌ No .wav files found in 'training_data/' folder.\")\n","        return\n","\n","    print(f\"🚀 Found {len(audio_files)} files to process...\")\n","    for audio_file in audio_files:\n","        process_single_file(audio_file)\n","\n","    # Save consolidated JSONL manifest\n","    with open(manifest_path, 'w', encoding='utf-8') as f:\n","        for entry in manifest_entries:\n","            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n","\n","    print(f\"\\n📄 Training manifest saved to: {manifest_path}\")\n","    print(\"✅ All files processed.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}